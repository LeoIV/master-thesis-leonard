This thesis discusses whether \acp{VAE} are a good model of the visual system.
The following sections, therefore, first introduce the human visual system and, subsequently, work related to \acp{VAE} as a model of this.

\subsection{Visual Cognition}\label{subsec:human-brain-structure}

The visual system allows humans and other developed animals to make sense of visual stimuli.
The following sections describe how the visual system's hierarchical structure enables the perception of gradually increasing complex visual features.

\subsubsection{Cells}

The brain, together with the spinal cord, constitutes the \ac{CNS}~\citep[p. 340]{mack2013principles}.
The \ac{CNS} comprises two types of cells: \textit{neurons} and \textit{glial cells}~\citep[p. 71]{mack2013principles}.

Even though glial cells occur two to ten times more often in the nervous system, neurons are the basic units allowing signal exchange in the nervous system~\citep[p. 24]{mack2013principles}.
In contrast, glial cells surround neurons and play a supportive role but are not directly involved in this signal exchange~\citep[p. 26]{mack2013principles}.

Neurons take different forms in different brain regions.
However, regardless of their specific configuration, they have four defined regions~\citep[p. 22]{mack2013principles}.
The (1) cell body (\textit{soma}) performs metabolism and contains genetic cell information.
(2) \textit{Dendrites} are attached to the cell body.
By branching out, they receive signals from other neurons.
Through their (3) \textit{presynaptic terminals}, (4) \textit{axons} transmit signals, the \textit{action potentials}, to other cells~\citep[pp. 22, 23]{mack2013principles}.

\setcounter{figure}{1}
\begin{wrapfigure}[28]{R}{0.3\textwidth}
    \begin{center}
        \includegraphics[width=0.28\textwidth]{images/neuron.jpeg}
    \end{center}
    \caption[Neuron structure]{The morphological structure of a neuron, taken from \citet[p. 22]{mack2013principles} (revised)}
    \label{fig:neuron_structure}
\end{wrapfigure}

\subsubsection{Human Brain Structure}

The human brain is subdivided into six regions of different forms and functions: \textit{medulla}, \textit{pons}, \textit{midbrain}, \textit{cerebellum}, \textit{diencephalon}, and the \textit{cerebral hemispheres}~\citep[p. 340]{mack2013principles} (see Figure~\ref{fig:human_brain_divisions}).

Medulla, pons, and midbrain constitute the \textit{brain stem}.
The brain stem receives input from some senses (but not vision) and plays a role in motor control~\citep[p. 341]{mack2013principles}.
% Furthermore, it \say{contains [\ldots] pathways that carry [\ldots] information to other divisions of the \ac{CNS}}~\citep[p. 341]{mack2013principles}.

The \textit{cerebellum} is mostly responsible for motor skills but is also involved in cognition~\citep[p. 341]{mack2013principles}.
It contains more neurons than other brain divisions but only a few different types and is well explored~\citep[p. 341]{mack2013principles}.

The \textit{diencephalon} contains the \textit{thalamus} and \textit{hypothalamus}.
The thalamus acts as a filter, deciding which information is forwarded to the \textit{neocortex}~\citep[p. 341]{mack2013principles}.
The hypothalamus plays a vital role in controlling body functions such as eating or drinking and initiating behaviors~\citep[p. 341]{mack2013principles}.

The \textit{cerebral hemispheres}~\citep[p. 341]{mack2013principles} is the brain region the most relevant for vision.
It can be further subdivided into the \textit{cerebral cortex}, \textit{white matter}, \textit{basal ganglia}, \textit{amygdala}, and \textit{hippocampus}~\citep[p. 341]{mack2013principles}.
The latter three are \say{concerned with the expression of emotion (amygdala), [\ldots] memory formation (hippocampus), and \ldots control of movement and aspects of motor learning (basal ganglia)}~\citep[p. 342]{mack2013principles}.
Underlaid by the \textit{white matter}, the neocortex is the brain region that enables cognition~\citep[pp. 341, 392]{mack2013principles}.
It is the region of the cerebral cortex closest to the brain surface~\citep[p. 345]{mack2013principles}.

The neocortex is structured into six layers and columns~\citep[p. 345]{mack2013principles}.
Neurons within a column are assumed to form a \textit{local processing network} and are understood as \say{the fundamental computational modules of the neocortex}~\citep[p. 348]{mack2013principles}.
Depending on the layer, neurons show different kinds of connectivity.
For example, Layer I mainly contains dendrites of cells in deeper layers, whereas Layers II and III contain \textit{pyramidal neurons} whose axons project onto other neurons~\citep[p. 346]{mack2013principles}.

Furthermore, the neocortex is structured \textit{topographically}, i.e., cells within a sensory area (e.g., the skin or retina) map onto the neocortex such that neighboring cells in the sensory area ultimately map onto neighboring regions in the neocortex~\citep[p. 343]{mack2013principles}.

\setcounter{figure}{0}
\begin{figure}
    \centering
    \includegraphics[width=.4\textwidth]{images/brain_regions.jpeg}
    \caption[Divisions of the human brain]{Five divisions of the human brain in the \ac{CNS}, the cerebral cortex as part of the cerebral hemispheres, and the spinal cord. Taken from \citet[p. 340]{mack2013principles}.}
    \label{fig:human_brain_divisions}
\end{figure}
\setcounter{figure}{2}

\subsubsection{Visual Cortex}\label{subsubsec:visual-cortex}

% TODO: How many regions does the visual cortex have? Introduce that first

Through the retina, signals of visual stimuli travel through the \textit{primary visual pathway} that includes the \ac{LGN}~\citep[p. 559]{mack2013principles}~(see Figure~\ref{fig:ventral_dorsal_pathway}).
The primary visual pathway transports the signals to \acf{V1}, the first region of the visual cortex~\citep[p. 559]{mack2013principles}.

The \ac{LGN} contains \textit{on-center} and \textit{off-center} cells that respond strongly to stimuli having either a bright center and a dark surrounding or a dark center and a bright surrounding in their receptive field~\citep[pp. 564-566]{mack2013principles}.
In the context of vision, the receptive field of a cell is the area of the visual field,\ the neuron is responsive to~\citep[p. 564]{mack2013principles}.
Although signals travel primarily from \ac{LGN} to \ac{V1}, the \ac{LGN} receives strong feedback from \ac{V1}.
However, the function of these feedback connections, outnumbering the number of feedforward neurons from the \ac{LGN} by factor ten, is mostly unknown~\citep[p. 573]{mack2013principles}.

\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth]{images/ventral_dorsal.jpeg}
    \caption[Ventral and dorsal pathways]{The ventral and dorsal pathways, carrying information from \ac{V1} to other brain regions. Taken from \citet[p. 571]{mack2013principles}}
    \label{fig:ventral_dorsal_pathway}
\end{figure}

From \ac{V1}, information is propagated to other brain regions via the \textit{ventral} and \textit{dorsal} pathways~\citep[pp. 563, 563]{mack2013principles}.
The dorsal pathway is responsible for the pass of information regarding the direction of movements, whereas the ventral pathway is concerned with object recognition~\citep[p. 564]{mack2013principles}.
Figure~\ref{fig:ventral_dorsal_pathway} shows the two pathways and the flow of information.
From V4, the ventral pathway has feedback and feed-forward connections from and to the \ac{TEO}, and from the \ac{TEO} it has feedback and feed-forward connections from and to \acf{IT}, which, in turn, has feedback connections to \ac{V1}~\citep[p. 563]{mack2013principles}.

The visual cortex is mainly structured in a feed-forward manner.
Lower regions in the visual cortex detect lower level features.
In contrast, higher regions detect higher-level features~\citep{eickenberg2017seeing,desimone1995neural}:
The primary visual cortex (\ac{V1}) detects edges~\citep{hubel1962receptive, eickenberg2017seeing}, while the \ac{V2} does not respond to such basic shapes~\citep{freeman2013functional}.
Instead, it responds to naturalistic textures that are combinations of features \ac{V1} is sensitive to~\citep{freeman2013functional}.
The function of V4 is manifold - it \say{respond[s] to more complex geometric shapes, color, and a large number of other
stimulus characteristics}~\citep{eickenberg2017seeing} and it is assumed to perform foreground and background segmentation~\citep{roe2012toward}.

Furthermore, the \ac{V4} is assumed to play a role in \textit{visual attention}.
Attentional mechanisms are assumed to be enabled by top-down connections.
They allow the suppression of irrelevant stimuli and focus on relevant ones in the current context~\citep{desimone1995neural,roe2012toward}.
\ac{V4} receives input from top-down connections and is assumed to enable attentional mechanisms~\citep{roe2012toward}.
The \ac{IT}, finally, responds to high-level features such as faces and complex objects~\citep{logothetis1995shape, eickenberg2017seeing}.

The two-stream hypothesis~\citep{goodale1992separate} states that the two pathways are concerned with the \textit{where} (dorsal stream) and \textit{what} (ventral stream) in a visual scene~\citep[p. 520]{mack2013principles}.
However, \citet[p. 564]{mack2013principles} note that the two pathways can exchange information.
Nevertheless, they still encode two different qualities of a stimulus: the identity and the location.

\subsubsection{Visual Object Perception}\label{subsec:visual-object-perception}

\begin{wrapfigure}[16]{R}{0.3\textwidth}
    \begin{center}
        \includegraphics[width=0.28\textwidth]{images/rubens_sketches.jpg}
    \end{center}
    \caption[Copies of line drawings]{\say{Copies of line drawings.} taken from \citet{rubens1971associative}}
    \label{fig:copies_line_drawings}
\end{wrapfigure}
% TODO Ich benutze hier perceive, see und recognize. Definiere die Begriffe und benutze trennschärfer
Recognizing an object's identity is different from the ability to see an object or make a copy of it.
\citet{rubens1971associative} report the case of a 47-year-old man who showed an inability to recognize objects, and in cases where he was unable to recognize an object, he also could not describe its use.
When given the category of an object, \say{identification improved very slightly}:
He recognized the item after being told the name.
When shown sketches of items, he was generally unable to recognize the items.
However, he was able to name geometric shapes such as circles or squares present in the sketch.
Even though the man did not recognize the objects, he could make copies of them (see Figure~\ref{fig:copies_line_drawings}).
\citet{rubens1971associative} report that the patient \say{was unable to identify any [items] before copying.}
% TODO meine ich hier retain oder recognize (contain eher nicht)
However, he was able to contain some of the categories of the objects after copying them.

This example shows that the ability to see and reproduce an object is different from the ability to \textit{recognize} it.

For monkeys, the \ac{IT} is assumed to be the brain region being crucial for object recognition~\citep[pp. 1070, 1071]{squire2012fundamental}.
Bilateral lesions of the \ac{IT} in monkeys affect their ability to \say{distinguish between different visual patterns or objects, and in retaining previously acquired visual discriminations}~\citep[p. 1070]{squire2012fundamental}.
They are no longer able to generalize from tasks learned in one half of the visual space to the other half, presumably because the invariance of representations is lost~\citep[p. 1070]{squire2012fundamental}.
\citet[p. 1071]{squire2012fundamental} explicitly point out the importance of the \ac{IT} during object recognition.

\subsubsection{Sparse Representations}\label{subsubsec:sparse_representations}

When shown natural images, between 1.8\% and 3.0\% (with $p < 0.01$) of mice \ac{V1} neurons are active across planes~\citep{yoshida2020natural}.
The overlap of responsive neurons for different images is quite small.
Between 4.8\% and 6.0\% of neurons overlap for different natural images.
The brain uses sparseness to represent information~(\citep{yoshida2020natural},~\citep[pp. 356, ff.]{trappenberg2010fundamentals}).

Sparse representations (or sparse codes) are a trade-off between \textit{local} and \textit{dense} codes~\citep{foldiak2003sparse}.
Assume a region in the brain has $N$ neurons that can either be \textit{on} or \textit{off}, depending on the input.
For a local code, one and only one neuron is active for a particular input.
This coding scheme allows to encode only $N$ distinct inputs but representations of different inputs can be superposed.
Dense codes, in contrast, activate about one half of the neurons for an input.
One example is binary coding that can encode $2^N$ different inputs.
However, dense codes do not allow to superpose representations of different inputs~\citep{foldiak2003sparse}.

Sparse codes combine advantages of dense and local codes while avoiding their drawbacks~\citep{foldiak2003sparse}.
Unsupervised models trained to represent natural images from a sparse set of basis functions have shown to learn Gabor-like basis function~\citep{Olshausen1996}.

\subsection{Types of Learning}\label{subsec:types-of-learning}

In machine learning, one often distinguishes between the three fundamentally different types of learning that are roughly described in the following.

\paragraph{Supervised Learning}
Algorithms employing supervised learning learn to predict the label for samples representative for the training set.
Datasets for this type of learning usually consist of sets of \textit{samples} $\bm{x}_i$ and \textit{labels} $y_i$, where the label describes the desired outcome of the algorithm on the sample.
An example of supervised learning is \textit{object detection}.
Here, one is interested in predicting what is present in an image or a sequence of images, e.g.,~images showing a dog or a cat.

However, it is not always the case that a pre-defined and discrete set of samples and labels exist.
In \textit{active learning}, the learning algorithm has no beforehand-dataset but has to query labels for self-chosen data points in the sample space.
In this setting, the learning algorithm chooses itself what samples to use for learning.

In the example of a classifier distinguishing cats and dogs, the dataset of samples and labels is available beforehand.
In such settings, the learning algorithm learns the mapping function and does not update it once the learning is finished.
Contrarily, in \textit{online learning}, the mapping function is updated during the lifetime of a system, e.g.,~in recommender systems for streaming services.
Users get recommendations while using the service but also provide new labels for the algorithm as they rate watched movies.

\textit{Self-supervised learning} is a special case of supervised learning, where the learning algorithm creates the labels itself.
An example of this type of learning is the prediction of the next frame in a video~\citep{walker2016uncertain}.
The dataset has no labels.
However, given a sequence $\bm{x}_{i,1},\bm{x}_{i,2},\dots,\bm{x}_{i,n}$ of images in a video, the sample at time $t$ can be considered as the supervised training set $((\bm{x}_{i,1},\bm{x}_{i,2},\dots,\bm{x}_{i,t}),\bm{x}_{i,t+1}), \quad 1\leq t\leq n-1$.
Self-supervised learning is also a subtype of unsupervised learning.
However, algorithms employing this approach usually use classical supervised learning techniques.
Another example of self-supervised learning is word embeddings~\citep{mikolov2013efficient}.

\paragraph{Unsupervised Learning}
\begin{wrapfigure}[15]{R}{0.5\textwidth}
    \centering
    \includegraphics[width=.48\textwidth]{images/clustering_example.png}
    \caption[Clusters of data points]{Data points colored by their cluster correspondence.}
    \label{fig:clustering_example}
\end{wrapfigure}
Like in self-supervised learning, unsupervised learning works with unlabeled data, i.e.,~the dataset only consists of samples $\bm{x}_i$.
A typical example of unsupervised learning is clustering, an approach that can be used if the data points can be compared in terms of a distance measure.
Consider Figure~\ref{fig:clustering_example}.
A clustering algorithm assigns the points labels as indicated by the color.

Cluster algorithms are often employed in cases where the class identity of data points is unknown, i.e., they aim to find the labeling function by identifying clusters of data points.
Figure~\ref{fig:clustering_example} shows an example where this can be feasible because adjacent data points tend to have the same class.

\paragraph{Reinforcement Learning}
Reinforcement learning is a very different paradigm compared to supervised and unsupervised learning.
Here, the aim is to train an \textit{agent} to show achieve a goal.

An example is rats in a maze.
A piece of cheese is located in one corner of the maze.
The different positions in the maze are the possible \textit{states}, the set of all states is the \textit{state space}.
The rat has multiple possible actions, going forward, backward, left, or right, the set of all actions is the \textit{action space}.
In this setting, the rat is the agent, trying to maximize its reward.
It is rewarded for each action in a state (the reward can also be zero).
By rewarding good sequences of actions, the rat eventually shows a reasonable behavior.

Reinforcement learning algorithms resemble such settings by modeling state space, action space, rewards, and reward function~\citep[pp. 1, ff.]{sutton2018reinforcement}.

\subsection{Semantic Representations}\label{subsec:semantic-representations}

Section~\ref{subsec:visual-object-perception} described a subject who was able to copy drawings of objects but not capture their \textit{meaning}.
\citet[pp. 1069, 1070]{squire2012fundamental} hypothesize that the subject's \ac{IT} was impaired, leading to an \say{inability to generate a high-level representation of the object.}

The term \textit{Semantic Representation} refers to these high-level representations that allow to capturing the \textit{meaning} of a stimulus.

Albeit, a universally accepted definition for this term yet has to be found.
\textit{Semantics} is defined as~\citep{lexico}
\begin{displayquote}
    The branch of linguistics and logic concerned with meaning. There are a number of branches and subbranches of semantics, including formal semantics, which studies the logical aspects of meaning, such as sense, reference, implication, and logical form, lexical semantics, which studies word meanings and word relations, and conceptual semantics, which studies the cognitive structure of meaning.
\end{displayquote}
This thesis is less concerned with linguistics but with \textit{meanings} and \textit{relations}, not of language but \textit{images}.
A general definition of \textit{Semantic Representations}, however, should not be restricted to words or images.
It should incorporate all kinds of concepts a human can form of perception.

In this thesis, \textit{Semantic Representations} are somewhat related to \textit{Word Embeddings} in \ac{NLP}.
Word embeddings~\citep{mikolov2013efficient} are a vectorized representation of words in a vector space.
The training procedure learns a mapping from word into the latent space so that the word's position itself has meaning.
One prominent example is that the vector space allows \say{simple algebraic operations[...]. [I]t was shown for example that \textit{vector}(\say{King}) - \textit{vector}(\say{Man}) + \textit{vector}(\say{Woman}) results in a vector that is closest to the vector representation of the word \textit{Queen} [...]}~\citep{mikolov2013efficient}.
It has been shown that for word representations, they correlate with brain activity fMRI data~\citep{ruan2016exploring, anderson2013words}.

Just like the brain maps similar visual stimuli to neurons close to one another, a semantic representation should encode concepts such that encodings of similar concepts are close to one another according to some distance metric.
Unfortunately, a dense vector (as in word embeddings) as a semantic representation is somewhat unrelated to active neurons’ subpopulation.

Another consideration is the \textit{mode} of perception.
Section~\ref{subsec:visual-object-perception} discusses that the \ac{IT} seems to play a crucial role in building and accessing semantic representations for visual input.
Different brain areas may be activated by different modes of presentation (e.g., \textit{visual} vs. \textit{auditory}).
However, only the brain regions that are active regardless of the mode of presentation are candidate regions for amodal representations~\citep{fairhall2013brain}.
\citet{fairhall2013brain} attempt to find these brain regions by analyzing which brain regions are active for textual and visual stimuli (both presented visually).

All in all, the question of how to obtain and assess good semantic representations remains.
One minimum requirement, albeit, is that those (amodal) semantic representations should behave like biological examples.

This thesis focuses on visual-modal semantic representations.

For visual input, it has been shown that higher-layer activations of supervised \acp{CNN}, described in the following section, explain \ac{IT} cortical representation~\citep{khaligh2014deep,cadieu2014deep}.
For unsupervised models operating on image data, in contrast, there is some evidence that these models do not explain \ac{IT} cortical representation (or by far worse than supervised models)~\citep{khaligh2014deep} - even though not explicitly for \acp{CNN}.

Therefore, \acp{CNN} seem to be a promising candidate for visual-modal semantic representations.

\subsection{Models of the Visual System}\label{subsec:models-of-the-visual-system}

The following section discusses established models of the visual system.

\subsubsection{Simple and Complex Cells in the Primary Visual Cortex}\label{subsubseq:simple_complex_cells}

\citet{hubel1962receptive} distinguish two main types of cells in the \ac{V1}: simple and complex cells.
Both cell types are orientation sensitive, i.e.,~different cells are tuned towards different orientations of stripes in a stimulus.
Unlike simple cells, complex cells are more invariant towards the stripe's location (but not its orientation) in the receptive field.
Complex cells are assumed to receive input from simple cells~\citep{hubel1962receptive}.
Based on this assumption, complex cells can be modeled by receiving input from many simple cells, tuned towards a specific orientation but not location.
If any of the afferent simple cells fire, the complex cells fires, leading to a translation-invariant behavior~\citep{hubel1962receptive}.

% TODO: Was ist Gabor wavelet
It can be shown that a two-dimensional Gabor wavelet is an excellent stimulus for a simple cell in \ac{V1} in terms of the neuronal action potential~\citep{jones1987evaluation}.

\subsubsection{Neocognitron}

The Neocognitron~\citep{fukushima1980neocognitron} is a model of object perception based on the findings of \citet{hubel1962receptive}.
It is a neural network consisting of models of simple (\say{S-cells}) and complex (\say{C-cells}) cells, alternatingly arranged in multiple layers.
It can be trained in an unsupervised manner by reinforcing connections leading to high cell activations in the next layer and has shown to be effective in pattern recognition~\citep{fukushima1980neocognitron}.

\subsubsection{Convolutional Neural Networks}\label{subsubsec:cnn_model_visual_system}
\acp{CNN}~\citep{lecun1989backpropagation} can be understood as a successor of the Neocognitron~\citep{lindsay2020convolutional} and is a network type commonly used on image data~\citep[p. 326]{Goodfellow-et-al-2016}.
The convolutional layers of a \ac{CNN} act as a pattern detector, similar to the S-cells in the Neocognitron~\citep{lindsay2020convolutional}.
After an activation layer, \acp{CNN} usually perform an operation called (max-)pooling~\citep[pp. 326, 339]{Goodfellow-et-al-2016}.
The pooling operation introduces invariance to the network, similar to C-cells~\citep{lindsay2020convolutional}.

\paragraph{Convolution}

Assume $I$ is a two-dimensional grayscale image, and $K$ is a convolutional kernel.
The convolution operation then is~\citep[p. 327]{Goodfellow-et-al-2016}:
\begin{align}
    S(i, j)=(I * K)(i, j)=\sum_{m=-\infty}^{\infty} \sum_{n=-\infty}^{\infty} I(m, n) K(i-m, j-n) \label{eq:conv}
\end{align}

The result of the convolution is $S$, a \textit{feature map}.

The kernel is usually implemented as a two-dimensional array~\citep[p. 327]{Goodfellow-et-al-2016}.
% TODO: Den folgenden Satz verständlicher, auch mit weniger assumes drin
Values outside of this array, as they are assumed in the summation in Equation~\ref{eq:conv}, are assumed to be zero.
These values then do not contribute to the sum, and the convolution effectively only has to use a finite number of summations.
In case of a $3\times 3$ kernel, Equation~\ref{eq:conv} simplifies to:
\begin{align}
    S(i, j)=(I * K)(i, j)=\sum_{m=i-1}^{i+1} \sum_{n=j-1}^{j+1} I(m, n) K(i-m, j-n) \label{eq:conv_boiled_down}
\end{align}
Equation~\ref{eq:conv} holds if indexing starts at zero for the image and at minus one for the kernel.

The output of a convolution is maximum in image areas where the image contains a flipped version of the kernel.
% The sums in Equation~\ref{eq:conv_boiled_down} go from low to high indices for the image but from high to low indices for the kernel.
Unlike the cross-correlation\footnote{The cross-correlation is defined as $S(i, j)=(I * K)(i, j)=\sum_{m=-\infty}^{\infty} \sum_{n=-\infty}^{\infty} I(i+m, j+n) K(m, n)$~\citep[p. 329]{Goodfellow-et-al-2016}.}, the convolution is commutative.
However, this property is not relevant in practice, and \say{many neural network libraries implement [...] the cross-correlation [instead]}~\citep[p. 329]{Goodfellow-et-al-2016}.
% TODO: Umschreiben, den Satz vorher

\subparagraph{Padding}
The convolution operation applies the kernel at all image locations.
At border pixels, however, this operation is problematic because the sum in Equation~\ref{eq:conv_boiled_down} also considers image pixels with an index smaller than $i$ or $j$.
There are multiple options to deal with this problem.

One option is not to pad the image at all and to start and end the convolution at indices such that the sum in Equation~\ref{eq:conv_boiled_down} is never applied to invalid image indices~\citep[p. 350]{Goodfellow-et-al-2016}.
However, this leads to feature maps smaller than the input image because the border pixels are not considered.
This can be problematic for deep networks as this happens on all convolutional layers.
Also, the network does not consider border pixels equally often compared to non-border pixels.

Another option is \textit{zero-padding}~\citep[p. 350]{Goodfellow-et-al-2016}, i.e.,~assuming values outside the image to be zero.
Here, the feature map is of the same size as the input.
However, the network still handles border pixels differently from inner pixels.
The feature maps values for border pixels tend to be smaller due to the multiplication with zero in Equation~\ref{eq:conv_boiled_down}.

Other padding techniques mirror the image at the borders or assume that all pixel values outside the image are the same as at the borders~\footnote{\href{https://www.tensorflow.org/api\_docs/python/tf/pad}{https://www.tensorflow.org/api\_docs/python/tf/pad}, last access: 2020/06/18}.

\paragraph{Max-Pooling}
Like the convolution, pooling operations use a sliding window over the feature maps, often with an offset (\textit{stride}) such that the sliding window sees every pixel only once.
A stridden max-pooling has multiple effects.
First, it leads to the downsampling of the image.
Downsampling is a useful property as it allows us to use more convolutional kernels in higher layers without exceeding a system's memory\footnote{Of course, the convolutional kernels itself are almost certainly unproblematic as they are very small. Each convolution, however, produces another memory-consuming feature map.}.
Not less importantly, max-pooling makes the network more invariant to small translations~\citep[p. 342]{Goodfellow-et-al-2016}.
In cases where translation invariance is not desired\footnote{This is often the case for generative models.}, but the image should be down-sampled, the max-pooling layer can be omitted, and a stridden convolution can be used instead~\citep[p. 337]{Goodfellow-et-al-2016}.

\paragraph{Stridden Convolutions}
The \textit{stride} of a convolution operation refers to the number of pixels being skipped as the sliding window moves over the feature map~\citep[p. 348]{Goodfellow-et-al-2016}.
For example, a stride of three with a $3\times 3$ kernel leads to each pixel of the convolution input being evaluated only \textit{once}.
A \textit{stride} in the convolution usually replaces the max-pooling operation.
Just like max-pooling, it leads to a down-sampling of the image because it is evaluated fewer times.
Replacing max-pooling by strides seems to have no significant effect on network classification performance~\citep{springenberg2015striving}, but it requires fewer evaluations than a full convolution with a subsequent max-pooling step.

\paragraph{Transposed Convolutions}
Suppose we have a $4\times 4$ input image which we are convolving with a $3\times 3$ kernel without padding and no stride\footnote{No stride is equivalent to a stride of one.}.
The output of this is a $2\times 2$ feature map.
In this example, the \textit{transposed convolution} is the operation that transforms a $2\times 2$ \textit{input} into a $4\times 4$ \textit{output}~\citep{dumoulin2016guide}.
This can be achieved by padding the $2\times 2$ input with two pixels of padding on the borders.
Convolving this padded input with a $3\times 3$ kernel results in a $4\times 4$ output.
Therefore, a transposed convolution performs an upsampling of the image.
Usually, it is used to inverse the convolution operation.
Even though it could be implemented as described above, this implementation would be rather slow.
Convolutions are mostly implemented as matrix multiplications~\citep{georganas2018anatomy}.
The transposed convolution is then implemented multiplying with an (eponymous) transposed matrix~~\citep{dumoulin2016guide}.

\paragraph{Convolutional Neural Networks as Models of the Visual System}
By design, \acp{CNN} share some features with the visual system, namely the convolution as a model of S-cells and max-pooling as a C-cell model.
However, \acp{CNN} show further unintentional parallels to the visual system:

AlexNet~\citep{krizhevsky2012imagenet} is a deep \ac{CNN} trained on image classification.
\citet{krizhevsky2012imagenet} have shown that this network learns Gabor wavelets in the first layer's kernels.
A Gabor wavelet is the optimal stimulus for a Gabor-like kernel (see Section~\ref{subsubsec:cnn_model_visual_system}), resulting in the strongest excitement.
It is also the optimal stimulus for cells in \ac{V1} (see Section~\ref{subsubseq:simple_complex_cells}, and \citet{jones1987evaluation}).

Furthermore, \citet{khaligh2014deep} have shown that \acp{CNN} trained in a supervised manner show similar \acp{RDM} like the human \ac{IT}.
An \ac{RDM} is a matrix encoding the correlation of activity patterns for a given model.
Rows and columns refer to different input stimuli.
A cell in an \ac{RDM} then contains the correlation of the activity patterns for the given input stimuli and the given model.
% \acp{RDM}, therefore, are symmetric.

\citet{eickenberg2017seeing} explicitly discuss \acp{CNN} as models of the visual system.
% TODO: Zitat hier rausnehmen und ummodeln
Their findings show that \acp{CNN} reproduce the hierarchy of semantic representations in the visual cortex~\citep{eickenberg2017seeing, wen2018neural}.
The similarity of the \acp{RDM} is evidence that supervised \acp{CNN} and the human \ac{IT} have a similar high-level encoding of information.

% TODO: Der und der nächste Absatz nimmt VAEs vorweg, verschieben?
\citet{khaligh2014deep} also show that this is not true for a variety of unsupervised models.
The unsupervised models under investigation, however, are only loosely related to \acfp{VAE}.
The question of whether \acp{VAE} show similar \acp{RDM} as the human \ac{IT}, therefore, remains.

It has been shown that \acp{VAE}, trained in a self-supervised manner, partially learn Gabor wavelets when trained to predict the next image in a sequence of images~\citep{palm2012prediction}.
Also, some unsupervised models learn Gabor wavelets~\citep{Olshausen1996,berkes2005slow} but whether \acp{VAE} learn Gabor wavelets in lower layers remains unanswered.

\subsection{Generative Methods}\label{subsec:generative-methods}

Section~\ref{subsec:types-of-learning} used the example of a \say{cat-and-dog-classifier} to introduce supervised learning.
Such a classifier is also an example of a \textit{discriminative model} because it \say{model[s] the posterior probabilities directly}~\citep[p. 43]{bishop2006pattern}.
Therefore, a discriminative model, for example, does not allow to predict the probability density of a data point in the feature space~\citep[pp. 43,44]{bishop2006pattern}.
\textit{Generative methods} model both the input and the output probabilities.
Access to the input probability allows sampling from the input space to generate new data points~\citep[p. 43]{bishop2006pattern}.
The following sections give an overview of existing generative methods.

\subsubsection{Generative Adversarial Networks}\label{subsubsec:gans}

The idea behind \acp{GAN}~\citep{goodfellow2014gans} is to use two networks, one \textit{generator network} $G$ to approximate the data distribution, and another \textit{discriminator network} $D \mapsto [0, 1]$ to discriminate between true data points and data points generated by the generator network.
This interplay can be formalized as~\citep{goodfellow2014gans}:
\begin{align}
    V(D, G)=\mathbb{E}_{\bm{x} \sim p_{\text{data}}(\bm{x})}[\log D(\bm{x})]+\mathbb{E}_{\bm{z} \sim p_{\bm{z}}(\bm{z})}[\log (1-D(G(\bm{z})))]
\end{align}
where $\bm{z}$ is random noise\footnote{Usually Gaussian white noise.}.

\acp{GAN} are commonly used to generate new data points.
Therefore, one is usually interested in the generator network.
It can be found by~\citep{goodfellow2014gans}:
\begin{align}
    G^* = \argmin_G \max_{D} V(D, G) \label{eq:gan_objective}
\end{align}

The discriminator $D$ in Equation~\ref{eq:gan_objective} is trained towards maximizing $V(D, G)$.
A perfect discriminator achieves this by labeling all true samples $\bm{x} \sim p_{\text{data}}$ with 1 and all generated samples $G(\bm{z}), \bm{z}\sim p_{\bm{z}}$ with 0.
The correct labeling of all samples results in a value $V(D, G) = 0$, which is the maximum.

The generator $G$ is trained towards minimizing $V(D, G)$.
As the discriminator output is the only variable term in $V(D, G)$, $G$ can only minimize $V(D, G)$ by misleading the discriminator to make a more significant error.
If the generator reproduces the data distribution $p_{\text{data}}$, the discriminator would no longer distinguish the generated from the real samples as they come from the same distribution.
Reproducing the data distribution is what is desired in generative methods.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/gan_samples.png}
    \caption[GAN-generated samples]{GAN-generated samples, taken from \citet{brock2018large}.}
    \label{fig:gan_samples}
\end{figure}

Recent improvements on \acp{GAN} have led to syntheses of highly natural images, as shown in Figure~\ref{fig:gan_samples}.

The main disadvantage of \acp{GAN} is training stability.
When training \acp{GAN}, one commonly encounters \textit{mode collapse} or \textit{loss oscillation}.
For mode collapse, the generator produces only a few or even just one sample~\citep{che2016mode}.
Mode collapse indicates a failure to match the posterior distribution, which is the \ac{GAN} training objective.
Loss oscillation occurs when the generator and the discriminator alternatingly become stronger and weaker~\citep{ham2020unbalanced}, leading to a non-convergence of the model.

\subsubsection{Variational Autoencoders}\label{subsec:variational-autoencoders}

Autoencoders are models transforming input into a lower dimensional representation~\citep[p. 146]{Goodfellow-et-al-2016}.
Unlike the regular Autoencoder, \acp{VAE} furthermore require the lower dimensional representation to follow a defined distribution.
Since \acfp{VAE} are a specialization of the autoencoder, the traditional autoencoder is introduced first.

\paragraph{Autoencoders}

Autoencoders are neural networks trained to reconstruct their input~\citep[p. 499]{Goodfellow-et-al-2016}.
They consist of an \textit{encoder} and a \textit{decoder}.
The encoder $f: \mathbb{R}^n \mapsto \mathbb{R}^m$ transforms an input $\bm{x}$ to a hidden representation $\bm{r} = f(\bm{x})$.
Usually, the encoder transforms the input into a lower-dimensional representation ($m < n$).
This mapping can be used for dimensionality reduction or feature learning~\citep[p. 499]{Goodfellow-et-al-2016}.
The decoder $g: \mathbb{R}^m \mapsto \mathbb{R}^n$ transforms the hidden representation back into the original feature space.
Usually, one wants the reconstruction $\tilde{\bm{x}}$ to be close to the original feature $\bm{x}$ ($\tilde{\bm{x}} \approx \bm{x}$).
In order to achieve this, the autoencoder is usually trained by minimizing the loss $\mathcal{L}(\bm{x}, g(f(\bm{x})))$ with
\begin{align}
    \mathcal{L}: \mathbb{R}^n \times \mathbb{R}^n \mapsto \mathbb{R}
\end{align}
One common choice for $\mathcal{L}$ is the \ac{MSE} which is defined as~\citep[p. 106]{Goodfellow-et-al-2016}:
\begin{align}
    \mathcal{L}(\bm{x}, \bm{y}) = \frac{1}{n}\sum (\bm{x}_i - \bm{y}_i)^2 \label{eq:mse}
\end{align}
Note that for linear activations in the autoencoder, the subspace spanned by the first $m$ principal components is the optimal solution for equation~\ref{eq:mse}~\citep{chicco2014deep}.

\paragraph{Variational Autoencoders}

% TODO: hier schreibe ich manchmal we blablabla, das muss raus
Autoencoders transform an input samples $\bm{x}$ to a \textit{hidden representation} $f(\bm{x})$.
However, the distribution over $f(\bm{x})$ is generally unknown.
Let $\bm{z} = f(\bm{x})$.
Assume $\bm{\tilde{z}} = \bm{z} + \bm{\epsilon}$ is a slightly perturbed version of $\bm{z}$, created by adding a small amount of noise $\bm{\epsilon}$.
Even though $\bm{\tilde{z}} \approx \bm{z}$, in terms of equation~\ref{eq:mse}, the result after decoding can be very different, i.e., $g(\bm{\tilde{z}}) \not\approx g(\bm{z})$.
This reconstruction-mismatch can occur because the distribution $p(\bm{z})$ can take any arbitrary form.
Values of $\bm{z}$ that are close to another\footnote{For example in terms of Euclidean distance} can be likely for very different values of $\bm{x}$.

To \textit{generate} new images, it would be advantageous to enforce $p(\bm{z})$ to follow a particular distribution.
A common choice is an independent, multivariate normal distribution~\citep[pp. 24, 25]{kingma2019introduction}.
\acp{VAE} enforce a specific (usually Gaussian) distribution over $\bm{z}$.

\acp{VAE} are trained to generate samples that are likely to occur in the training set.
Therefore, they aim to maximize $\log p(\bm{x})$\footnote{Note that $\max \log  p(\bm{x}) \equiv \max  p(\bm{x})$.}~\citep[p. 18]{kingma2019introduction}.
Assuming a latent \textit{prior} distribution $p(\bm{z})$ over $\bm{z}$, $p(\bm{x})$ can be written as the marginal distribution
\begin{align}
    p(\bm{x}) &= \int p(\bm{x}, \bm{z})d\bm{z} \label{eq:vae_x1}\\
    &= \int p(\bm{x}|\bm{z})\,p(\bm{z})d\bm{z}. \label{eq:vae_x2}
\end{align}
Unfortunately, due to the integral w.r.t. $\bm{z}$ in equations~\ref{eq:vae_x1} and~\ref{eq:vae_x2}, computing $p(\bm{x})$ is intractable~\citep[p. 13]{kingma2019introduction}.
However, if the posterior $p(\bm{z}|\bm{x})$ was given, $p(\bm{x})$ could be obtained by
\begin{align}
    p(\bm{x}) = \frac{p(\bm{x}, \bm{z})}{p(\bm{z}|\bm{x})}
\end{align}
since computing $p(\bm{x}, \bm{z})$ is tractable~\citep[p. 14]{kingma2019introduction}.

\acp{VAE} approximate the \textit{posterior} by an \textit{inference model} $q_\phi(\bm{z}|\bm{x}) \approx p_\theta(\bm{z}|\bm{x})$, also called the \textit{encoder}~\citep[p. 15]{kingma2019introduction}.

To minimize the difference between $q$ and $p$, usually, the \ac{KL-divergence} plus the reconstruction error is minimized.
The reason for this is as follows.
The starting point is that we cannot compute the posterior $p_\theta(\bm{z}|\bm{x})$.
Therefore, we approximate it by another distribution $q_\phi$.
The \ac{KL-divergence} is used o minimize the difference between $p_{\theta}$ and $p_{\phi}$: $\kldiv{q_\phi(\bm{z}|\bm{x})}{p_\theta(\bm{z}|\bm{x})}$.

\begin{align}
    \kldiv{q_\phi(\bm{z}|\bm{x})}{p_\theta(\bm{z}|\bm{x})} &= -\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{z}|\bm{x})}{q_\phi(\bm{z}|\bm{x})} \\
    &= -\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{\frac{p_\theta(\bm{x},\bm{z})}{p_\theta(\bm{x})}}{\frac{q_\phi(\bm{z}|\bm{x})}{1}}\\
    &= -\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{x},\bm{z})}{q_\phi(\bm{z}|\bm{x})}\frac{1}{p_\theta(\bm{x})}\\
    &= -\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \left[ \log \frac{p_\theta(\bm{x},\bm{z})}{q_\phi(\bm{z}|\bm{x})} - \log p_\theta(\bm{x}) \right]\\
    &= -\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{x},\bm{z})}{q_\phi(\bm{z}|\bm{x})} + \sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log p_\theta(\bm{x}) \\
    &= -\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{x},\bm{z})}{q_\phi(\bm{z}|\bm{x})} + \log p_\theta(\bm{x}) \underbrace{\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x})}_{=1} \\
    &= -\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{x},\bm{z})}{q_\phi(\bm{z}|\bm{x})} + \log p_\theta(\bm{x})
\end{align}
Then
\begin{align}
    \underbrace{\vphantom{\sum_{\bm{z}} x}\log p_\theta(\bm{x})}_{\text{constant}} =  \underbrace{\vphantom{\sum_{\bm{z}} x}\kldiv{q_\phi(\bm{z}|\bm{x})}{p_\theta(\bm{z}|\bm{x})}}_{\downarrow}  + \underbrace{\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{x},\bm{z})}{q_\phi(\bm{z}|\bm{x})}}_{\uparrow}
\end{align}
Because $\log p_\theta(\bm{x})$ is assumed to be fixed, instead of minimizing $\kldiv{q_\phi(\bm{z}|\bm{x})}{p_\theta(\bm{z}|\bm{x})}$, we can maximize $\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{x},\bm{z})}{q_\phi(\bm{z}|\bm{x})}$, as both terms always sum up to a constant.
$\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{x},\bm{z})}{q_\phi(\bm{z}|\bm{x})}$ is called \textit{variational lower bound} or \acfi{ELBO}~\citep[p. 18]{kingma2019introduction}.

The variational lower bound can be written as
\begin{align}
    \sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{x},\bm{z})}{q_\phi(\bm{z}|\bm{x})} &= \sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{z}|\bm{x})p_\theta(\bm{x})}{q_\phi(\bm{z}|\bm{x})}\\
    &= \sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \left[ \log p_\theta(\bm{x}) + \log \frac{p_\theta(\bm{z}|\bm{x})}{q_\phi(\bm{z}|\bm{x})} \right]\\
    &= \sum_{\bm{z}} q_\phi(\bm{z}|\bm{x})\log p_\theta(\bm{x}) + \sum_{\bm{z}} q_\phi(\bm{z}|\bm{x})\log \frac{p_\theta(\bm{z}|\bm{x})}{q_\phi(\bm{z}|\bm{x})}\\
    &= \underbrace{\mathbb{E}_{q_\phi(\bm{z}|\bm{x})} \left[ \log p_\theta(\bm{x}) \right]}_{=\log p_\theta(\bm{x})\, \text{, independent of $\bm{z}$}} + \underbrace{\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x})\log \frac{p_\theta(\bm{z}|\bm{x})}{q_\phi(\bm{z}|\bm{x})}}_{=\kldiv{q_\phi(\bm{z}|\bm{x})}{p_\theta(\bm{z}|\bm{x})}} \label{eq:elbo_lower_bound}
\end{align}
but also as
\begin{align}
    \sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{x},\bm{z})}{q_\phi(\bm{z}|\bm{x})} &= \sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{x}|\bm{z})p_\theta(\bm{z})}{q_\phi(\bm{z}|\bm{x})}\\
    &= \sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \left[ \log p_\theta(\bm{x}|\bm{z}) + \log \frac{p_\theta(\bm{z})}{q_\phi(\bm{z}|\bm{x})} \right]\\
    &= \underbrace{\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x})\log p_\theta(\bm{x}|\bm{z})}_{=\mathbb{E}_{q_\phi(\bm{z}|\bm{x})}\left[ \log p_\theta(\bm{x}|\bm{z}) \right]} + \underbrace{\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x})\log \frac{p_\theta(\bm{z})}{q_\phi(\bm{z}|\bm{x})}}_{=\kldiv{q_\phi(\bm{z}|\bm{x})}{p_\theta(\bm{z})}} \label{eq:elbo_error_term}
\end{align}

Equations~\ref{eq:elbo_lower_bound} and~\ref{eq:elbo_error_term} allow two different interpretations of the \ac{ELBO}.
Equation~\ref{eq:elbo_lower_bound} justifies the name \textit{\acl{ELBO}}.
If the \ac{KL-divergence} nears zero, it approaches the likelihood of the data~\citep[p. 18]{kingma2019introduction}.
However, $p_\theta(\bm{z}|\bm{x})$ cannot be computed with a \ac{VAE}.

Equation~\ref{eq:elbo_error_term}, however, is more useful for implementing the error function of a \ac{VAE}.
Consider the term inside the expectation of the first term.
Usually, $p_\theta(\bm{x}|\bm{z})$ is chosen to be a normal distribution with \ac{PDF}
\begin{align}
    f_{\sigma,\mu}(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2}
\end{align}
Then
\begin{align}
    \log f_{\sigma,\mu}(x) &= \log \left( \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2} \right)\\
    &= - \log \sqrt{2\pi}\sigma -\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2 \label{eq:log_normal_distr}
\end{align}
Since $\mu$, in this case, is a function of $\bm{z}$ (parametrized by $\theta$), we can consider this as the reconstruction $\hat{\bm{x}} = f_\theta(\bm{z})$.
Then, except for subtracting a constant and scaling by a constant, equation~\ref{eq:log_normal_distr} is the \ac{MSE}.
The sum on the left of Equation~\ref{eq:elbo_error_term} is subsequently called \textit{reconstruction term}, the sum on the \textit{\ac{KL}-term}.

Since $p(\bm{x}|\bm{z})$ is a Gaussian, maximizing $\mathbb{E}_{q(z)} \left[ p(x|z) \right]$ is equivalent to minimizing the mean squared error between $x$ and the \textit{reconstruction} $\hat{x}$.
$\hat{x}$ can be written instead of $z$ because $p(z) = \hat{x}$ and minimizing does not change the relation.

When implementing a \ac{VAE}, $q_\phi$ and $p_\theta$ are realized by neural networks.
The encoder predicts the mean and variances for an independent, multivariate normal distribution, whereas the decoder reconstructs the input.
The usage of neural networks poses a challenge for computing the first term in equation~\ref{eq:elbo_error_term}.
Usually, one would draw a sufficient number of samples from $q_\phi(\bm{z}|\bm{x})$ to approximate the expectation.
However, sampling is impossible if one wants to compute the gradient to train the model by backpropagation.

Backpropagation for the encoder would be intractable if $\bm{z}$ were realized as a random variable, i.e., $\bm{z} \sim q_\phi(\bm{z}|\bm{x})$.
This is because the gradients w.r.t.~$\phi$ cannot merely be calculated for one $\bm{z} \sim q_\phi(\bm{z}|\bm{x})$ as this would be biased towards this $\bm{z}$~\citep{kingma2019introduction}.
Instead, one would have to sample many $\bm{z}$s to approximate the expectation w.r.t.~$q_\phi(\bm{z}|\bm{x})$.
This is not true for the decoder as its gradients w.r.t. $\theta$ are independent of this expectation.
To solve this problem, implementations of \acp{VAE} do not sample from the posterior $q_\phi(\bm{z}|\bm{x})$.
Instead, they use the \textit{resampling trick}~\citep{kingma2019introduction}.
Under this regime, $\bm{z}$ is the value of a function $g(\phi, \bm{x}, \epsilon)$ where $\epsilon \sim \mathcal{N}(\bm{0}, \bm{I})$ is a random variable~\citep{kingma2019introduction}.
In an intermediate step, the encoder predicts values $\bm{\mu}$ and $\log \bm{\sigma}$ and $\bm{z}$, eventually, is realized as $\bm{z} = \bm{\mu} + \bm{\sigma} \cdot \epsilon$~\citep{kingma2019introduction}.
Using this trick, the encoder gradients are w.r.t.~the expectation of $\epsilon$.
Backpropagation on a minibatch of data, therefore, is unbiased.

Unlike regular autoencoders, \acp{VAE} produce an interpretable latent space.
Enforcing randomness in the positioning of points in the latent space guides the model to predict an area in the latent space that is likely for a given input $\bm{x}$~\citep[p. 701]{Goodfellow-et-al-2016}.
Eventually, this allows to sample from the random space to generate new images, employing the decoder.
Furthermore, it allows simple arithmetic in the latent space (see Section~\ref{subsubsec:representation_learning}).

Even though the latent space is forced to follow a prior distribution, it remains unclear how (lower-level) attributes of an image are encoded in this latent space.

\paragraph{Disadvantages}

% TODO: Zitat weg
One problem arising when training \acp{VAE} is \textit{posterior collapse}, meaning that one or more of the latent dimensions resemble the prior (\citep[p. 694]{Goodfellow-et-al-2016}, \citep{lucas2019understanding}), i.e.~\citep{lucas2019understanding}:
\begin{align}
    \exists i: \forall \mathbf{x} q_{\phi}\left(z_{i} | \mathbf{x}\right) \approx p\left(z_{i}\right) .
\end{align}
In the most extreme case, this happens for \textit{all} latent dimensions.
In that case, the generator cannot make use of $\bm{x}$ at all, because generated images are unrelated to the input image.
The KL-term is probably the reason for the posterior collapse as it is the term forcing the posterior to match the prior distribution~\citep{lucas2019understanding}.
One technique to avoid posterior collapse is to lower the weight of the KL-term compared to the reconstruction term~\citep{lucas2019understanding}.

Another disadvantage of \acp{VAE} are \say{blurry} and \say{fuzzy} reconstructions~\citep{zhao2017towards}.
According to \citet{zhao2017towards}, these reconstructions occur for $\bm{z}$'s that are encodings of a variety of different $\bm{x}$'s.
They propose that blurry reconstructions are caused by the pixel-wise loss and a too simple prior, e.g.,~a standard multivariate normal distribution~\citep{zhao2017towards}.

\subsubsection{Representation Learning}\label{subsubsec:representation_learning}
\textit{Representation learning} originally addresses the learning of representations of input data that makes models operating on this data more efficient~\citep{bengio2013representation}.
In the context of \ac{NLP}, for example, sentences are often represented by \textit{Bag-of-Words} vectors.
In a Bag-of-Words vector, each cell stands for one word in the vocabulary.
The representation for one specific sentence is obtained by assigning each cell the number of occurrences of its word in the sentence.

However, representation learning can also be considered in semantic representations (see Section~\ref{subsec:semantic-representations}).
As discussed earlier (see Section~\ref{subsubsec:visual-cortex}), different regions of the visual cortex learn representations of the input in different regions.
Earlier regions learn representations of simple features, whereas later regions respond to sophisticated features of the input.
RTherefore, representations of inputs are learned in a \textit{hierarchy} of simple to complex shapes~\citep{rodriguez2015hierarchical}.

The following paragraphs introduce different hierarchical representation learning approaches.

\paragraph{InfoGAN}
InfoGANs~\cite{chen2016infogan} address feature learning in \acp{GAN} by \say{decomposing the input noise vector into two parts}~\citep{chen2016infogan}.
Instead of only the noise $z$, the generator $G$ receives an additional input of \say{structure latent variables} $\bm{c} = (\bm{c_1}, \bm{c_2}, \dots, \bm{c_L})$.
The additional variables are assumed to be independent of each other.
To force $G$ not just to ignorce $c$,~\citet{chen2016infogan} modify the training objective such that \say{there should be high mutual information between latent codes $c$ and the generator distribution $G(z,c)$}~\citep{chen2016infogan}.
The new loss function takes the form
\begin{align}
    \min _{G} \max _{D} V_I(D, G)=V(D, G) - \lambda I(\bm{c}; G(\bm{z}, \bm{c}))
\end{align}
where $V(D, G)$ is the training objective defined in Equation~\ref{eq:gan_objective}, $\lambda$ is a trade-off hyperparameter, and $I(\bm{x}; \bm{y})$ is the mutual information between $\bm{x}$ and $\bm{y}$.

\begin{figure}
    \centering
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=.8\textwidth]{images/infogan_c1.png}
        \caption{Varying $\bm{c}_1 \in [0, 10]$}
    \end{subfigure}%
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=.8\textwidth]{images/infogan_c2.png}
        \caption{Varying $\bm{c}_2 \in [-2, 2]$}
    \end{subfigure}
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=.8\textwidth]{images/infogan_c3.png}
        \caption{Varying $\bm{c}_3 \in [-2, 2]$}
    \end{subfigure}
    \caption[InfoGAN latent space traversal]{Exploration of $\bm{c}$ for InfoGAN on \textsc{Mnist} by traversing either, $\bm{c}_1$, $\bm{c}_2$, or $\bm{c}_3$. Rows correspond to different noise values $\bm{z}$ but are equal for the three figures, columns correspond to different values for $\bm{c}_i$. $\bm{c}_1$ takes discrete values, $\bm{c}_2$ and $\bm{c}_3$ take continuous values. Taken from \citet{chen2016infogan}.}
    \label{fig:infogan}
\end{figure}

\say{To disentagle digit shape styles on \textsc{Mnist},} \citet{chen2016infogan} choose $\bm{c}$ as three-element set $\bm{c} = (\bm{c}_1, \bm{c}_2, \bm{c}_3)$ with $\bm{c}_1$ drawn from a categorical distribution with ten categories (equal to the number of classes in MNIST) and event probability $p_i=0.1 \, \forall i \in [0, 10]$.
$\bm{c}_2$ and $\bm{c}_3$ are drawn from a uniform distribution over $[-1, 1]$.

Figure~\ref{fig:infogan} shows the latent space separability by varying the $\bm{c}_i$-values.
In conclusion, InfoGAN seems to generalize as $\bm{c}_2$, and $\bm{c}_3$ vary between $[-2, 2]$, even though these values were in $[-1, 1]$ at training time.
Generalization is a critical property for a model of the visual context.
Also, InfoGAN addresses the problem of hierarchical learning.
As discussed in Section~\ref{subsubsec:visual-cortex}, the different regions in the visual cortex learn increasingly complex representations.
Therefore, InfoGAN, as a model employing hierarchical representations, addresses one more important property of models of the visual system.

\paragraph{StyleGAN}
StyleGAN~\citep{karras2019style} is a type of \ac{GAN} that explicitly addresses \textit{latent space separability} (see Section~\ref{subsec:feature-separability}).
The term \textit{style} refers to latent space separability, i.e.,~the model is can learn a disentanglement of images into different styles.
Another term for style is factor of variation.
For human faces, for example, one style or factor of variation is the hair color.

The styles of an image are learned at different levels in a self-supervised manner.
Therefore, it is not possible to explicitly force the model to learn pre-defined aspects of an image.
A posterior analysis allows us to identify which aspects were learned on which level.

\citet{karras2019style} train their model on a dataset of human faces.
In this context, coarse styles correspond to gender, age, or glasses.
Middle styles correspond to skin color, face form, or mouth open/closed.
Fine styles correspond mainly to hair color and lightning.

\acp{GAN} generate new images using random noise $\bm{z}\in \mathcal{Z}$ as input (see Section~\ref{subsubsec:gans}).
Besides, StyleGAN uses a mapping network $f: \mathcal{Z}\mapsto \mathcal{W}$ to map the random noise to a vector $\bm{w}\in \mathcal{W}$.
With an affine transformation, the vector $w$ is then mapped to a vector $\bm{y}_i = (\bm{y}_{s,i}, \bm{y}_{b,i})$.
Different vectors $\bm{y}_i$ are fed into different layers $i$ of the generator network to control the style on this layer.

The generator network $g$ has a constant input $\bm{x}_0$ that is lower-dimensional than the final output image.
$\bm{x}_0$ is of size $4\times 4\times 512$~\citep{karras2019style}.
On each layer, a noise vector $\bm{\epsilon}_i$ is added, followed by an \textit{\acf{AdaIN}} operation.
The \ac{AdaIN} operation of SyleGAN is defined as~\citep{karras2019style}:
\begin{align}
    \text{\ac{AdaIN}}(\bm{x}_i, \bm{y}) = \bm{y}_{s, i} \frac{\bm{x}_{i}-\mu\left(\bm{x}_{i}\right)}{\sigma\left(\bm{x}_{i}\right)}+\bm{y}_{b, i},
\end{align}
where $\mu(\cdot)$ gives the mean and $\sigma(\cdot)$ the (empirical) standard deviation.
Importantly, this operation is applied separately for each feature map of $\bm{x}_i$.

On each resolution, $g$ first applies noise addition, \ac{AdaIN}, a $3\times 3$ convolution, noise addition, and one last \ac{AdaIN} operation.
Then, the image is upsampled to double size.
This process is repeated until the image has the desired output size.
\citet{karras2019style} use nine of these blocks, resulting in an output size of $1024\times 1024$.

In StyleGAN, latent space separability is achieved by a technique called \textit{mixing regularization}.
Here, for a subset of the training images, two input vectors $\bm{z}_1$ and $\bm{z}_2$ are drawn.
During the training, a point (or layer) of $g$ up to which layer $\bm{z}_1$ controls the style is chosen at random.
After this point, the style information from $\bm{z}_2$ is fed into the network.
Randomly choosing this point \say{prevents the network from assuming that adjacent styles are correlated}~\citep{karras2019style}.

The training procedure allows for using two sources for the generation.
During inference time, the crossover point can be chosen arbitrarily, i.e.,~it can be controlled what styles are taken from the first and what styles are taken from the second source.

$\bm{y}_i$s fed into the network at a low resolution (i.e., $i$ is small) control the coarse styles, whereas $\bm{y}_i$ at higher layers control finer styles.

Finally, \citet{karras2019style} analyze how disentangled generations from $\mathcal{W}$-space are compared to $\mathcal{Z}$-space employing \textit{perceptual path length} (see Section~\ref{subsec:feature-disentanglement}).
They show that the perceptual path length is lower for interpolating in $\mathcal{W}$ than for interpolation in $\mathcal{Z}$~\citep{karras2019style}.
Therefore, they conclude that $\mathcal{Z}$ is more entangled than $\mathcal{W}$ in their setup.

Disentanglement in the latent space has important ramifications in the context of semantic representations (see Section~\ref{subsec:semantic-representations}).
It has to be considered if the interpolation between two points in the latent space requires a curved instead of linear path.

\paragraph{\acl{LVAE}}

% TODO: Schreib hier um die zs rum, dass das die Hidden representation is
\begin{figure}
    \centering
    \begin{tikzpicture}
        % Nodes
        \node[draw,circle,minimum size=1.3cm] (x1) {$\bm{x}$};
        \node[draw,diamond,above=of x1,minimum size=1.3cm] (d1) {$\bm{d}_1$};
        \node[draw,diamond,above=of d1,minimum size=1.3cm] (d2) {$\bm{d}_2$};
        \node[above=of d2,minimum size=1.3cm] (ddots) {\vdots};
        \node[draw,diamond,above=of ddots,minimum size=1.3cm] (dL) {$\bm{d}_L$};
        \node[draw,circle,right=of dL,minimum size=1.3cm] (zL1) {$\bm{z}_L$};
        \node[below=of zL1,minimum size=1.3cm] (zdots1) {\vdots};
        \node[draw,circle,right=of d2,minimum size=1.3cm] (z21) {$\bm{z}_2$};
        \node[draw,circle,right=of d1,minimum size=1.3cm] (z11) {$\bm{z}_1$};

        \node[draw,circle,right=of zL1,minimum size=1.3cm] (zL2) {$\bm{z}_L$};
        \node[below=of zL2,minimum size=1.3cm] (zdots2) {\vdots};
        \node[draw,circle,right=of z21,minimum size=1.3cm] (z22) {$\bm{z}_2$};
        \node[draw,circle,right=of z11,minimum size=1.3cm] (z12) {$\bm{z}_1$};
        \node[draw,circle,below=of z12,minimum size=1.3cm] (x2) {$\bm{x}$};

        % Arrows
        \draw[->] (x1.north) -- (d1.south);
        \draw[->] (d1.north) -- (d2.south);
        \draw[->] (d2.north) -- (ddots.south);
        \draw[->] (ddots.north) -- (dL.south);

        \draw[->] (d1.east) -- (z11.west);
        \draw[->] (d2.east) -- (z21.west);
        \draw[->] (dL.east) -- (zL1.west);

        \draw[->] (zL1.south) -- (zdots1.north);
        \draw[->] (zdots1.south) -- (z21.north);
        \draw[->] (z21.south) -- (z11.north);
        \draw[->] (z22.south) -- (z12.north);
        \draw[->] (z12.south) -- (x2.north);
        \draw[->] (zL2.south) -- (zdots2.north);
        \draw[->] (zdots2.south) -- (z22.north);

        \draw[<-,dashed] (z21.east) -- (z22.west);
        \draw[<-,dashed] (z11.east) -- (z12.west);
        \draw[<-,dashed] (zL1.east) -- (zL2.west);

        \draw [decorate,decoration={brace,amplitude=5pt,raise=0.7cm},rotate=90] (dL.west) -- (zL1.east) node [black,midway,yshift=1.5cm] {Encoder};
        \draw [decorate,decoration={brace,amplitude=5pt,raise=0.7cm},rotate=90] (zL2.west) -- (zL2.east) node [black,midway,yshift=1.5cm] {Decoder};
    \end{tikzpicture}
    \caption[LVAE structure]{Ladder Variational Autoencoder structure, adapted from \citet{sonderby2016ladder}. Solid arrows indicate feed-forward connections, dashed arrows indicate weight sharing. Diamonds indicate stochastic variables, circles deterministic variables. $\bm{z}$s are latent spaces, $\bm{x}$s are model inputs/outputs.}
    \label{fig:lvae}
\end{figure}

\aclp{LVAE}~\citep{sonderby2016ladder} address the problem of hierarchical learning in \acp{VAE}.
Classical \acp{VAE}, in contrast, learn one latent representation in one layer.

Hierarchical \acp{VAE}, also, only use the first few layers to learn meaningful semantics of an input.
In the case of \citet{sonderby2016ladder}, the first two layers are sufficient.
\citet{zhao2017learning} state that, given a sufficiently large encoder network in the first layer, the first layer learns all the semantics.
While this is not always true, they \say{demonstrate that this phenomen occurs in practice [\ldots]}.

\ac{LVAE} is a model designed to use all embedding layers to learn a representation instead of just the first few layers.
They do this by passing information top-down in the inference network.
\begin{figure}
    \centering
    \includegraphics[width=0.2\textwidth]{images/gerben.png}
    \caption[Context-Dependent Semantic Ambiguity]{Illustration of context-dependent semantic ambiguity. Taken from \citet[p. 61]{van2016auto}}
    \label{fig:gerben_ambiguity}
\end{figure}
Figure~\ref{fig:gerben_ambiguity} gives an intuition of why passing information top-down can be advantageous.
Here, the two rows show two different alphanumeric strings.

Apparently, the first row shows a sequence of alphabetic characters (\say{GERBEN}), whereas the second row shows a sequence of numeric characters (\say{10 11 12 13 14 15}).
However, \say{R} and \say{B} in the first row are the same symbols as \say{12} and \say{13} in the second one.
A simple feed-forward system would be unable to discriminate between the low-level features \textit{R} and \textit{13}.
Incorporating high-level features like \textit{alphabetic string} helps to improve the posterior low-level feature representation.
Furthermore, the top-down pass is more biologically plausible than a pure feed-forward network and enables the model to focus on relevant parts of the input (see Section~\ref{subsubsec:visual-cortex}).


Unlike hierarchical \acp{VAE}, \acp{LVAE} do not use lower stochastic embedding layers as input for higher layers.
Instead, they use a deterministic multi-layer network in the encoder (see Figure~\ref{fig:lvae}) and pass information from intermediate layers to the stochastic embedding layers.
\begin{align}
    \bm{d}_i &= \text{NN}(\bm{d}_{i-1})\\
    \hat{\bm{\mu}}_{q,i}&=\text{NN}(\bm{d}_i)\\
    \hat{\bm{\sigma}}^2_{q,i}&=\text{NN}(\bm{d}_i)
\end{align}
The output from higher stochastic embedding layers is passed to lower layers via the decoder $p$:
\begin{align}
    \bm{\sigma}_{q,i}&=\frac{1}{\hat{\bm{\sigma}}^{-2}_{q,i}+\hat{\bm{\sigma}}^{-2}_{p,i}}\\
    \bm{\mu}_{q,i}&=\frac{\hat{\bm{\mu}}_{q,i}\hat{\bm{\sigma}}^{-2}_{q,i}+\hat{\bm{\mu}}_{p,i}\hat{\bm{\sigma}}^{-2}_{p,i}}{\hat{\bm{\sigma}}^{-2}_{q,i}+\hat{\bm{\sigma}}^{-2}_{p,i}}
\end{align}
The latent variables then are sampled conditionally:
\begin{align}
    q(\bm{z}_i|\cdot)=\mathcal{N}(\bm{z}_i|\bm{\mu}_{q,i},\bm{\sigma}^2_{q,i})
\end{align}

\citet{sonderby2016ladder} show that gradually increasing the weight of the KL-term in the \ac{VAE} training criterion leads to better usage of the latent spaces in terms of active units.
Without this \textit{warm-up} phase, \acp{VAE} show many inactive units.
\citet{sonderby2016ladder} discuss that \acp{VAE} show this behavior because it allows them to minimize the KL-term in the \ac{VAE} loss function quickly.

Using batch-normalization and warm-up alone already leads to more meaningful representations in the higher layers of a hierarchical VAE~\citep{sonderby2016ladder}.
Using all these techniques on the \ac{LVAE} model allows meaningful representations in even higher layers than a hierarchical VAE\footnote{In \citet{sonderby2016ladder}, a hierarchical \ac{VAE} with batch-normalization and warm-up learns meaningful representations up to the fourth layer where as 5-layer \ac{LVAE} also learns meaningful representations in the fifth layers. Deeper models were not investigated.}.

\paragraph{\acl{VLAE}}

\begin{figure}
    \centering
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[height=.8\textheight]{images/vlae_encoder.png}
        \caption{Varying $\bm{c}_1 \in [0, 10]$}
    \end{subfigure}%
    \begin{subfigure}{.45\textwidth}
        \centering
        \includegraphics[height=.8\textheight]{images/vlae_decoder.png}
        \caption{Varying $\bm{c}_2 \in [-2, 2]$}
    \end{subfigure}
    \caption[VLAE structure]{Structure of the VLAE model}
    \label{fig:vlae_structure}
\end{figure}

Unlike the \ac{LVAE}, the \ac{VLAE}~\citep{zhao2017learning} is a pure feed-forward network.
Its design is driven by the idea that \say{[i]f $\bm{z}_i$ is more abstract than $\bm{z}_j$, then the inference mapping $q(\bm{z}_i|\bm{x})$ and generative mapping when other layers are fixed $p(\bm{x}|\bm{z}_i, \bm{z}_{\neg i}=z^0_{\neg i})$ requires a more expressive network to capture}~\citep{zhao2017learning}.
Figure~\ref{fig:vlae_structure} shows an exemplary network structure of a \ac{VLAE}.
Early embedding layers like $\bm{z}_1$ are equipped with a less powerful network that, according to \citet{zhao2017learning}, should not learn abstract features\footnote{In case of \textsc{Mnist}, more abstract features are for example the digit identity.}.

This design allows \acp{VLAE} to encode lower-level factors of variation (for example \textit{stroke thickness} in the context of a dataset of written digits) in lower layers and higher-level factors of variation (e.g., \textit{digit identity}) in higher layers.
\citet{zhao2017learning} claim that the \ac{VLAE}, for such a dataset, learns different factors of variation independently in different layers.

The hierarchical structure of embedding layers in the \ac{VLAE} make it another candidate model for the visual cortex.
However, the proposition that different layers learn different factors of variation independently seems to be mostly based on an analysis on model-generated samples.

\paragraph{$\beta$-VAE}

Consider Equation~\ref{eq:elbo_error_term}, describing the loss function in a \ac{VAE}.
\citet{higgins2017beta} discovered that weighting both terms, the KL-divergence and the reconstruction loss, equally leads to suboptimal results in terms of \textit{feature disentanglement}.
They, therefore, formulated the $\beta$-VAE learning objective that adds a hyperparameter $\beta$ to the loss function, controlling the relative weights of the two terms:
\begin{align}
    \mathbb{E}_{q_{\phi}(\bm{z} | \bm{x})}\left[\log p_{\theta}(\bm{x} | \bm{z})\right]-\beta \kldiv{q_{\phi}(\bm{z} | \bm{x})}{p(\bm{z})}. \label{eq:beta-vae}
\end{align}

A value $\beta > 1$ puts more emphasis on the KL-term than the original \ac{VAE} loss function ($\beta = 1$).
\citet{higgins2017beta} state that this could lead to better feature disentanglement because the KL-term \say{encourages conditional independence in $q_\phi(\bm{z}|\bm{x})$}~\citep{higgins2017beta}.
A proper disentanglement requires that some factors of the data are conditionally independent, \citet{higgins2017beta}, however, show empirically that this often is the case.

Setting $\beta > 1$ also lowers the influence of the reconstruction error in the \ac{VAE} training objective.
A larger $\beta$ leads to worse reconstructions.
Hence, the right parameter choice is context-dependent.

\citet{burgess2018understanding} show that disentangled representations and a good reconstruction quality can be achieved by giving the model \say{control of the encoding capacity.}
Controlling encoding capacity is done by introducing a parameter $C$ to Equation~\ref{eq:beta-vae}~\citep{burgess2018understanding}:
\begin{align}
    \mathbb{E}_{q_{\phi}(\mathbf{z} \mid \bm{x})}\left[\log p_{\theta}(\bm{x} \mid \bm{z})\right]-\gamma\left|D_{K L}\left(q_{\phi}(\bm{z} \mid \bm{x}) \| p(\bm{z})\right)-C\right|
\end{align}
During training, $C$ gradually increases from zero to a higher value (25 for dSprites with $\gamma=1000$ \citep{burgess2018understanding}).

\paragraph{\ac{VAE}-\ac{GAN}}
\ac{VAE}-\acp{GAN}~\citep{larsen2015autoencoding} are a variation of \acp{VAE} specifically designed to replace the pixel-wise reconstruction loss.
Even on highly curated datasets such as \textsc{Mnist} (see Section~\ref{subsubsec:mnist}), \acp{VAE} tend to generate blurry images.
The design of \ac{VAE}-\acp{GAN} is based on the assumption that the pixel-wise loss in the \ac{VAE} training objective (see Equation~\ref{eq:log_normal_distr}) is the reason for blurry output images.
\citet{larsen2015autoencoding}~motivate this by stating that even small translations result in a high pixel-wise loss\footnote{This depends on the image's frequency as well as the variance of the pixel intensities.}.

Instead of measuring the \ac{MSE} between the true and the generated image, \ac{VAE}-\acp{GAN} train a discriminator network to distinguish between generated and real images, i.e.,~to maximize
\begin{align}
    \mathcal{L}_{\mathrm{GAN}}=\log (D(\bm{x}))+\log (1-D(G(\bm{z}))), \label{eq:vae_gan_l_gan}
\end{align}
where $D$ and $G$ are the discriminator/generator networks, $\bm{x}$ an input image and $\bm{z}\sim \mathcal{N}(\bm{0},\bm{I})$ a random Gaussian sample.
For \ac{VAE}-\acp{GAN}, the generator network is nothing but the decoder network, whereas the discriminator network is a new network that does not exist for \acp{VAE}.

Equation~\ref{eq:vae_gan_l_gan} is sufficient to train the discriminator network.
However, it does not force $p(\bm{x}) \approx \mathcal{N}(\bm{0},\bm{I})$, nor does it force reconstructions to resemble the input image.

The latter is achieved by introducing an additional loss term
\begin{align}
    \mathcal{L}_{\text {llike}}^{\text {D}}=-\mathbb{E}_{q(\bm{z} | \bm{x})}\left[\log p\left(\operatorname{D}_{l}(\bm{x}) | \bm{z}\right)\right]
\end{align}
penalizing differences of intermediate discriminator layer activations between $\bm{x}$ and the reconstruction $\tilde{\bm{x}}$ of $\bm{x}$.
The former, forcing $p(\bm{x}) \approx \mathcal{N}(\bm{0},\bm{I})$, is achieved by minimizing the \ac{KL}-divergence as in Equation~\ref{eq:elbo_error_term}:
\begin{align}
    \mathcal{L}_{\text {prior }}=\kldiv{q(\boldsymbol{z} | \boldsymbol{x})}{p(\boldsymbol{z})}
\end{align}

The encoder minimizes $\mathcal{L}_{\text {prior }} +  \mathcal{L}_{\text {llike}}^{\text {D}}$, the decoder minimizes $\gamma \mathcal{L}_{\text {llike}}^{\text {D}} - \mathcal{L}_\mathrm{GAN}$, and the discriminator minimizes $-\mathcal{L}_\mathrm{GAN}$.
Choosing $\gamma = 0.75$ led to training convergence for the experiments performed for this thesis.

\citet{larsen2015autoencoding}~show that this training procedure leads to less blurry reconstructions.
Adapting the \ac{VAE}-\ac{GAN} training objective to the \ac{VLAE}-model could lead to improvements for that model as well.

\paragraph{Adversarial Latent Autoencoder}
\begin{figure}
    \begin{tikzpicture}
        % Nodes
        \node[draw,circle] (z) {$\bm{z}$};
        \node[draw,rectangle,right=of z] (F) {$\mathcal{F}$};
        \node[draw,circle,right=of F] (w) {$\bm{w}$};
        \node[draw,rectangle,right=of w] (G) {$\mathcal{G}$};
        \node[draw,circle,right=of G] (xt) {$\tilde{\bm{x}}$};
        \node[draw,rectangle,right=of xt] (E1) {$\mathcal{E}$};
        \node[draw,circle,right=of E1] (zt1) {$\tilde{\bm{z}}_1$};
        \node[draw,rectangle,right=of zt1] (D1) {$\mathcal{D}$};
        \node[right= of D1] (realfake1) {real / fake};
        \node[below=of G] (eta) {$\eta$};

        \node[draw,circle,below=of xt] (x) {$\bm{x}$};
        \node[draw,rectangle,right=of x] (E2) {$\mathcal{E}$};
        \node[draw,circle,right=of E2] (zt2) {$\tilde{\bm{z}}_2$};
        \node[draw,rectangle,right=of zt2] (D2) {$\mathcal{D}$};
        \node[right= of D2] (realfake2) {real / fake};
        % Arrows
        \draw[->] (z.east) -- (F.west);
        \draw[->] (F.east) -- (w.west);
        \draw[->] (w.east) -- (G.west);
        \draw[->] (G.east) -- (xt.west);
        \draw[->] (xt.east) -- (E1.west);
        \draw[->] (E1.east) -- (zt1.west);
        \draw[->] (zt1.east) -- (D1.west);
        \draw[->] (D1.east) -- (realfake1.west);
        \draw[->] (eta.north) -- (G.south);

        \draw[->] (x.east) -- (E2.west);
        \draw[->] (E2.east) -- (zt2.west);
        \draw[->] (zt2.east) -- (D2.west);
        \draw[->] (D2.east) -- (realfake2.west);

        \draw [decorate,decoration={brace,amplitude=10pt,raise=0.7cm},rotate=90] (F.west) -- (G.east) node [black,midway,yshift=1.5cm] {$G$};
        \draw [decorate,decoration={brace,amplitude=10pt,raise=0.7cm},rotate=90] (E1.west) -- (D1.east) node [black,midway,yshift=1.5cm] {$D$};
    \end{tikzpicture}
    \caption[ALAE training]{Training of ALAE, adapted from \citet{pidhorskyi2020adversarial}}
    \label{fig:alae_flow}
\end{figure}
Similar to \ac{VAE}-\acp{GAN}, \Acfp{ALAE}~\citep{pidhorskyi2020adversarial} are a hybrid model between \acp{VAE} and \acp{GAN}.
\acp{ALAE} are not based on variational inference but \acp{GAN}.

The starting point of the \ac{ALAE} model is to decompose both the generator $G$ and discriminator $D$ into two parts, i.e., $G=\mathcal{G}\circ \mathcal{F}$ and $D=\mathcal{D}\circ \mathcal{E}$ (see Figure~\ref{fig:alae_flow}).
$\mathcal{G}$ uses noise $\eta$ as an additional input.
$\mathcal{F}$ maps the Gaussian random noise $\bm{z} \sim \mathcal{N}(\bm{0}, \bm{I})$ to another latent representation $\bm{w}$.
The loss function requires the outputs of $\mathcal{F}$ and $\mathcal{E}$ to be similar in terms of the $l^2$-norm.
Apart from this, the $\bm{w}$-space is unregularized.
In addition to the $l^2$-loss, the model is optimized towards minimizing the discriminator and the generator loss.

Because $p_{\mathcal{E}}(w) \approx p_{\mathcal{F}}(w)$, \ac{ALAE} can generate reconstructions by
\begin{align}
    \bm{x}\approx (\mathcal{E}\circ \mathcal{G}_\eta)(\bm{x}).
\end{align}
$\mathcal{G}_\eta$ here indicates that $\mathcal{G}$ uses $\eta$ as an additional input.

Furthermore, just like for the \ac{VAE}, the Gaussian latent space $Z$ can be traversed to generate new samples by
\begin{align}
    \bm{x}_{\text{new}} = (\mathcal{F}\circ \mathcal{G}_\eta)(\bm{z}).
\end{align}

\citet{pidhorskyi2020adversarial} show that (linearly) interpolating in $\bm{w}$-space yields smoother transitions than interpolation between two corresponding points in $\bm{z}$-space.
This indicates that the $\bm{w}$-space is less entangled than the $\bm{z}$-space~\citep{shao2018riemannian,arvanitidis2017latent}.
However, traversing the $\bm{w}$-space directly is not easily possible as its structure is unknown.
Compared to \acp{VAE}, where a linear interpolation between points in the latent $\bm{z}$-space leads to an almost geodesic path in $\bm{x}$-space\footnote{Therefore, points that are close in $\bm{z}$-space of an \ac{VAE} lead to images that are close in the feature space.}, the mapping function $\mathcal{F}$ seems to be non-smooth.
Therefore, traversing $\bm{z}$ for an \ac{ALAE} seems to lead to a non-smooth path in the feature space.

Compared to the \ac{VAE} that uses a pixel-wise loss on the reconstructions, \ac{ALAE} penalizes the $l^2$-norm of $\bm{w}$ and $\tilde{\bm{w}}$.
Except for this loss, \ac{ALAE} by no means forces the input to be similar to the reconstruction.
This mitigates the problem of the pixel-wise loss in \acp{VAE} that is assumed to be the reason for blurry reconstructions.

\citet{tschannen2018recent} further discuss \say{recent advances in autoencoder-based representation learning.}

\paragraph{Feature Consistency}

One important property of the \ac{VAE} latent space is that it captures image semantics up to a certain degree.
For a model trained on CelebA\footnote{See Section~\ref{subsubsec:celeba_dataset}.} it is possible to add sunglasses to a randomly generated face by adding the mean latent vector of images containing sunglasses~\citep{larsen2015autoencoding,radford2016deep,hou2017deep}.
\citet{hou2017deep} train a model particularly tailored to have this property by using the \textit{perceptual loss} (the between hidden layer activations of a feature extraction network).

Feature consistency as well as an interpretable latent space qualify \acp{VAE} as models of the visual system.

\subsection{Latent Space Disentanglement}\label{subsec:feature-disentanglement}
\textit{Latent Space Disentanglement} is a concept not to be confused with \textit{Latent Space Separability} (see Section~\ref{subsec:feature-separability}).
Assume we have a dataset of images.
Each image consists of a shape (for example \textit{Square}, \textit{Heart}, and \textit{Ellipse}) at different $x$- and $y$-positions.
Then, \textit{shape}, $x$\textit{-position}, and $y$\textit{-position} are called \say{factors of variation}~\citep{Shu2020Weakly,higgins2018towards}, say $S_1$, $S_2$, and $S_3$ with $S_i\in \mathcal{S}^n$~\citep{Shu2020Weakly}.
Each image $\bm{x}\in \mathcal{X}^m$ can be generated from the factors of variation by a data-generating process $g: \mathcal{S}^n\times \mathcal{S}^n\times \mathcal{S}^n\mapsto \mathcal{X}^m$.

In the context of \acp{VAE} where the generator maps $\bm{x}$ to a vector $\bm{z}$ in the latent space, a disentangled representation has the following property.
As one factor of variation (e.g., the shape) is changed, one and only one subspace in the latent space is affected~\citep{higgins2018towards}.

\begin{figure}
    \centering
    \begin{subfigure}{.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/latent_space_disentanglement_a.png}
        \caption{\say{Distribution of features in the training set}~\citep{karras2019style}}
        \label{fig:feature_space_disentanglement_a}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/latent_space_disentanglement_b.png}
        \caption{Mapping from latent space to feature space}
        \label{fig:feature_space_disentanglement_b}
    \end{subfigure}
    \caption[Latent Space Entanglement]{Unbalanced regions of probability density as one problem of latent space entanglement (taken from \citet{karras2019style})}
    \label{fig:feature_space_disentanglement}
\end{figure}
A stricter definition requires these subspaces to be linear subspaces of the latent space~\citep{higgins2018towards}.
Furthermore, it is often desired that the sampling from the latent space leads to the same data distribution~\citep{karras2019style}.

Latent space disentanglement is supposed to have two properties: \textit{Consistency} and \textit{Restrictiveness}, as defined in \citet{Shu2020Weakly}.
Consistency means that if only one factor of variation is varied (e.g., the factor that attributes to an object's size) and the others are fixed, generated items only change for this very factor but not to other factors (i.e., only the object's size is changing but not its shape).
Restrictiveness means that if again, only one factor $i$ of variation is varied, the choice of the other factors should not affect the model's measurement of the $i$th factor.
For example, if factor $i$ again encodes an object's size, the same value of factor $i$ should always encode the same size, irrespective of the choice if the other factors~\citep{Shu2020Weakly}.

Consider Figure~\ref{fig:feature_space_disentanglement_a} and assume a dataset of males containing only two factors of variation, e.g., age ($x$-axis from older to younger) and voice level ($y$-axis, bottom-up from lower to higher).
The missing square in the upper left is the combination \say{old and high voice level} that is missing in the dataset.
Now, consider Figure~\ref{fig:feature_space_disentanglement_b} showing a mapping from a latent space to the feature space.
The upper left square in Figure~\ref{fig:feature_space_disentanglement_a} is not present in this latent space.
Therefore, there are regions of higher (or lower) probability density in the latent space than in the data space.

Different methods have been proposed to measure the degree of latent space entanglement.
One approach is to measure the degree by which a generated image changes as the latent space is entangled (\textit{Perceptual Path Length})~\citep{karras2019style}.
\ac{PPL} measures by much reconstructions change for small variation in the latent space.
It is calculated by moving by a small value $\epsilon$ on a path obtained by interpolating between two random $\bm{z}_1$, $\bm{z}_2$ (or $\bm{w}_1$, $\bm{w}_2$)\footnote{Interpolation is spherical for $\bm{z}$ and linear for $\bm{w}$.}, and taking the perceptual loss~\citep{johnson2016perceptual} between the image before moving by $\epsilon$ and after.
Finally, the mean of this value is obtained by applying the procedure for sufficiently many samples.
If the latent space is entangled, this value on average should be higher compared to a disentangled space.

Another approach is \textit{Linear Separability}~\citep{karras2019style}, measuring how well a linear hyperplane can separate sets of points.

\citet{kim2018disentangling} propose to generate data by fixing one factor of variation (for example, one dimension in the latent space) and randomly sampling from others.
If the latent space is disentangled, there should be one or multiple almost invariant dimensions in the generated data attributed to the fixed factor of variation.

\subsection{Latent Space Separability}\label{subsec:feature-separability}

\textit{Latent Space Separability} is related to Latent Space Disentanglement, but it is used in a stricter sense in the course of this thesis.
Whereas Latent Space Disentanglement was mainly motivated by the disentanglement of one latent space, Latent Space Separability is concerned with splitting up the latent space representation of features into multiple levels of latent spaces.
If successful, Latent Space Separability allows to directly control the factors of variation without the need to find the subspace in one latent space controlling this factor.
Besides this, latent space separability has the same properties as latent space disentanglement.
If there are less layers than factors of variation, the latent spaces in one layer also have to be disentangled.

For example, \textit{InfoGAN} (see Section~\ref{subsubsec:representation_learning}) and \acp{VLAE} are based on this principle.