\subsection{Human Cognition}\label{subsec:human-brain-structure}

\subsubsection{Cells}

\begin{wrapfigure}[18]{r}{0.3\textwidth}
    \begin{center}
        \includegraphics[width=0.28\textwidth]{images/neuron.jpeg}
    \end{center}
    \caption[Neuron]{The morphological structure of a neuron, taken from \citet[p. 22]{mack2013principles}}
    \label{fig:neuron_structure}
\end{wrapfigure}

The human nervous systems has two types of cells, \textit{neurons} and \textit{glial cells} or \textit{glia}~\citep[p. 71]{mack2013principles}.
Even though glia occur two to ten times more often in the nervous system than neurons~\citep[p. 24]{mack2013principles}, they - unlike neurons - \say{are not directly involved in electrical signaling}~\citep[p. 26]{mack2013principles} and therefore not further described in the course of this work.

Neurons in different brain regions can take different forms.
However, regardless of their specific configuration, they have four defined regions: \say{(1) the cell body, (2) dendrites, (3) axon, and (4) presynaptic terminals}~\citep[p. 22]{mack2013principles} (see Figure~\ref{fig:neuron_structure}).
The dendrites receive input from other neurons while axons convey electrical signals to other neuron's dendrites.
\textbf{Hier k√∂nnte noch deutlich mehr kommen.}

\subsubsection{Human Brain Structure}

The human brain is subdivided into six regions of different forms and functions: the medulla, pons, midbrain, cerebellum, diencephalon, and the cerebral hemispheres~\citep[p. 340]{mack2013principles} (see Figure~\ref{fig:human_brain_divisions}).
Together with the spinal cord, it constitutes the \ac{CNS}~\citep[p. 340]{mack2013principles}.

\begin{figure}
    \centering
    \includegraphics[width=.4\textwidth]{images/brain_regions.jpeg}
    \caption{Five divisions of the human brain, the cerebral cortex as part of the cerebral hemispheres, and the spinal cord. Taken from \citet[p. 340]{mack2013principles}.}
    \label{fig:human_brain_divisions}
\end{figure}

Medulla, pons, and midbrain constitute the \textit{brain stem}.
The brain stem recieves input from \say{several specialized senses, such as hearing, balance and taste} and \say{mediates sensation and motor control of head, neck, and face}~\citep[p. 341]{mack2013principles}.
Furthermore, it \say{contains [\ldots] pathways that carry [\ldots] information to other divisions of the \ac{CNS}}~\citep[p. 341]{mack2013principles}.

The \textit{cerebellum} is mostly responsible for motor skills but is also involved in \say{language and other cognitive function}~\citep[p. 341]{mack2013principles}.
Even though containing more neurons than other divisions of the brain, it \say{is well understood because relatively few types of neurons are involved}~\citep[p. 341]{mack2013principles}.

The \textit{diencephalon} contains the \textit{thalamus} and \textit{hypothalamus}.
The thalamus acts as a filter, deciding which information is forwarded to the neocortex~\citep[p. 341]{mack2013principles}.
The hypothalamus plays an important role in controlling body functions such as eating or drinking, but also in initiating behaviors~\citep[p. 341]{mack2013principles}.

The \textit{cerebral hemispheres}~\citep[p. 341]{mack2013principles}, finally, is the brain region most relevant for this thesis.
It can be further subdivided into \textit{cerebral cortex}, \textit{white matter}, \textit{basal ganglia}, \textit{amygdala}, and \textit{hippocampus}~\citep[p. 341]{mack2013principles}.
The latter three are \say{concerned with the expression of emotion [amygdala], [\ldots] memory formation [hippocampus], and [\ldots] control of movement and aspects of motor learning [basal ganglia]}~\citep[p. 342]{mack2013principles}.
The \textit{cerebral cortex}, underlayed by the \textit{white matter}, is the structure of the brain closest to the surface~\citet[p. 341]{mack2013principles}.

The \textit{neocortex} is \say{the region of cerebral cortex nearest the surface of the brain} \citet[p. 345]{mack2013principles}.
It is structured into six layers and columns~\citep[p. 345]{mack2013principles}.
Neurons within a column are assumed to from a \say{local processing network} and are understood as \say{the fundamental computational modules of the neocortex}~\citep[p. 348]{mack2013principles}.
Neurons within the different layers show different kinds of connectivity.
For example, Layer I mainly contains dendrites of cells in deeper layers where as Layers II and III contain \textit{pyramidal neurons} whose axons project onto other neuros~\citep[p. 346]{mack2013principles}.

Furthermore, the neocortex is structured \textit{topographically}, i.e.~neurons within a sensory area (e.g. the skin or retina) are mapped onto the neocortex such that neighboring neurons in the sensory area ultimately map onto neighboring regions in the neocortex~\citep[p. 343]{mack2013principles}.

\subsubsection{Visual Cortex}\label{subsubsec:visual-cortex}

The first cortical region receiving retinal signals is the \acf{V1}~\citep[p. 559]{mack2013principles}.
The \ac{V1} does not receive input directly from the retina, but the signal travels through the \textit{primary visual pathway} that includes the \ac{LGN}~\citep[p. 559]{mack2013principles}.
Two other pathways transport signals from the retina to other brain regions but are not concerned with object recognition but controlling movements and pupillary reflexes~\citep[p. 559]{mack2013principles}.

The \ac{LGN} contains so-called \textit{on-center} and \textit{off-center} cells that respond strongly to stimuli having either a bright center and a dark surrounding or a dark center and a bright surrounding~\citep[pp. 564-566]{mack2013principles}.
Noteworthy, the \ac{LGN} receives strong feedback from \ac{V1}.
However, the function of these feedback connections, outnumbering the number of feedforward neurons from the \ac{LGN} by the factor ten, \say{is largely unknown}~\citep[p. 573]{mack2013principles}.

\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth]{images/ventral_dorsal.jpeg}
    \caption{The ventral and dorsal pathways, carrying information from \ac{V1} to other brain regions. Taken from \citet[p. 571]{mack2013principles}}
    \label{fig:ventral_dorsal_pathway}
\end{figure}

From \ac{V1}, information is propagated to other brain regions via the \textit{ventral} and \textit{dorsal} pathways~\citep[pp. 563, 563]{mack2013principles}.
The dorsal pathway is responsible for the pass of information regarding the direction of movements whereas the ventral pathway is more concerned with object recognition~\citep[p. 564]{mack2013principles}.
Figure~\ref{fig:ventral_dorsal_pathway} shows the two pathways and the flow of information.
Importantly, Figure~\ref{fig:ventral_dorsal_pathway} does not show the complete pathways.
From V4, the Ventral pathways has feedback and feedforward connections from and to \ac{TEO}, and from \ac{TEO} it has feedback and feedforward connections from and to \ac{IT}, which, in turn, has feedback connections to \ac{V1} \citep[p. 563]{mack2013principles}.

According to the two-stream hypothesis~\citep{goodale1992separate}, the two pathways are assumed to encode the \textit{what} (dorsal) and \textit{where} (ventral) in a visual scene~\citep[p. 520]{mack2013principles}.
Even though the two pathways can exchange information \citep[p. 564]{mack2013principles}, they encode two different qualities of a stimulus: the identity and the location.

\subsection{Visual Object Perception}\label{subsec:visual-object-perception}

\begin{wrapfigure}[12]{r}{0.3\textwidth}
    \begin{center}
        \includegraphics[width=0.28\textwidth]{images/rubens_sketches.jpg}
    \end{center}
    \caption[Copies of line drawings]{\say{Copies of line drawings.} taken from \citet{rubens1971associative}}
    \label{fig:copies_line_drawings}
\end{wrapfigure}
Recognizing an object as what it is is different from the ability of seeing an object or making a copy of it.
\citet{rubens1971associative} report the case of a 47-year old man who, on March 5--1969, \say{was found unconscious with vomitus on his face and bathrobe}.
Only after \say{his breathing became irregularly}, he was taken to a hospital where a low blood pressure was diagnosed.
The man showed an inability to recognize objects and in cases where he was unable to recognize an object, he also could not describe its use.
When given the category of an object, \say{identification improved very slightly}.
He claimed to recognize the item after being told the name.
In such cases, he was able to \say{point out various parts of the previously unrecognized item}.
When shown sketches of items, he was generally unable to recognize the items.
However, he was able to name geometric shapes such as circles or squares present in the sketch.
Even though the man did not recognize the objects, he was able to make copies of them (see Figure~\ref{fig:copies_line_drawings}).
\citet{rubens1971associative} report, that the Patient \say{was unable to identify any [items] before copying}.
However, he was able to contain some of the objects categories after copying them.

The example presented above shows that the ability to reproduce an object is different from the ability to \textit{perceive} it.

For monkeys, the \ac{IT} is assumed to be the brain region being crucial for object perception~\citep[pp. 1070, 1071]{squire2012fundamental}.
Bilateral lesions of the \ac{IT} in monkeys affect their ability to \say{distinguish between different visual patterns or objects, and in retaining previously acquired visual discriminations}~\citep[p. 1070]{squire2012fundamental}.
They are no longer able to generalize from tasks learned in one half of the visual space to the other half, presumably because the invariance of representations is lost~\citep[p. 1070]{squire2012fundamental}.
\citet[p. 1071]{squire2012fundamental} explicitly point out \say{the crucial role of the inferior temporal cortex during object perception and recognition}.

\subsection{Types of Learning}\label{subsec:types-of-learning}

In machine learning, one often distinguishes between the three fundamentally different types of learning that are roughly described in the following.

\paragraph{Supervised Learning} An example for supervised learning is \textit{object detection} where one is interested in predicting what is present in an image or a sequence of images.
Datasets for this type of learning usually consist of sets of images (or \textit{samples}) $\bm{x}_i$ and \textit{labels} $y_i$ where the label describes what is present in the image, e.g. \say{dog} or \say{cat}.
Algorithms employing supervised learning learn to predict the label for images representative for the training set.

In the former example, it was assumed that the complete dataset of samples and labels was present beforehand.
However, there are setting where this is not the case.

In \textit{active learning} the learning algorithm has no beforehand-dataset but has to query labels for self-chosen datapoints in the sample space.
In this setting, the learning algorithm chooses itself what points to use for learning.
An example for this setting are \textit{Gaussian Processes} \textbf{REF}.

In the introductory example of this paragraph, the dataset was present beforehand.
In such settings, the learning algorithm learns the mapping function and does not update it once the learning is finished.
Contrary to this is \textit{online learning} where the mapping function is updated during the lifetime of a system.
An example for this are recommender systems, e.g.~for streaming services.
Users get recommendations while they are using the service but also provide new labels for the algorithm as they rate watched movies.

\textit{Self-supervised learning} can be seen as a special case of supervised learning where the learning algorithm creates the labels itself.
An example for this type of learning is the prediction of the next frame in a video \textbf{REF}.
The dataset has no labels as in the introductory example of this paragraph.
However, given a sequence $\bm{x}_{i,1},\bm{x}_{i,2},\dots,\bm{x}_{i,n}$ of images in a video, the sample at time $t$ can be considered as the supervised training set $((\bm{x}_{i,1},\bm{x}_{i,2},\dots,\bm{x}_{i,t}),\bm{x}_{i,t+1}), \quad 1\leq t\leq n-1$.
Self-supervised learning can also be seen as a subtype of unsupervised learning, however algorithms employing this approach usually use classical supervised learning techniques.
Another example for self-supervised learning are word embeddings \textbf{REF}.

\paragraph{Unsupervised Learning}
\begin{wrapfigure}[11]{r}{0.5\textwidth}
    \centering
    \includegraphics[width=.48\textwidth]{images/vlae_embeddings/embeddings_mu_3_6.png}
    \caption{Clustering of hand-written digits in a latent space.}
    \label{fig:clustering_example}
\end{wrapfigure}
Just like in self-supervised learning, unsupervised learning works with unlabeled data, i.e.~the dataset only consists of samples $\bm{x}_i$.
A typical example for unsupervised learning is clustering, an approach that can be used if the datapoints can be compared in terms of a distance measure.
Consider Figure~\ref{fig:clustering_example}.
It shows hand-written digits from the \textsc{Mnist} dataset (see Section~\ref{subsubsec:mnist}) mapped into a latent space.
The mapping is performed by a \ac{VLAE}, a model discussed in the later course of this thesis (see Section~\ref{subsubsec:representation_learning})
The dots are colord by digit identity.
In this case, the \ac{VLAE} learns a clustering of \textsc{Mnist} digits by digit identity.

\paragraph{Reinforcement Learning}
Reinforcement learning is a very different paradigm compared to supervised and unsupervised learning.
Here, the aim to is train an \textit{agent} to show some meaningful behavior, e.g.~to play a computer game.
Reinforcement learning algorithms achieve this by letting the agent explore the action space\footnote{In the case of a computer game, this would mean to carry out different moves.}.
If the outcome of an action is good, the agent is getting rewarded.
The agent, in turn, aims at maximizing its reward.

\subsection{Generative Methods}\label{subsec:generative-methods}

\subsubsection{\acfp{GAN}}\label{subsubsec:gans}

The idea behind \acp{GAN}~\citep{goodfellow2014gans} is to use two networks, one \textit{generator network} $G$ to approximate the data distribution, and one \textit{discriminator network} $D$ to discriminate between true data points and data points generated by the generator network.
This can be formalized as~\citep{goodfellow2014gans}:
\begin{align}
    \min _{G} \max _{D} V(D, G)=\mathbb{E}_{\bm{x} \sim p_{\text {data }}(\bm{x})}[\log D(\bm{x})]+\mathbb{E}_{\bm{z} \sim p_{\bm{z}}(\bm{z})}[\log (1-D(G(\bm{z})))] \label{eq:gan_objective}
\end{align}
where $\bm{z}$ is random noise.

The discriminator $D$ is trained towards predicting the correct label (\say{true datapoint}) for training samples $\bm{x}$ as well as for generated samples $G(\bm{z})$ (\say{fake samples}).
The generator $G$ is trained towards minimizing the probability that $D$ is capable of discriminating between true and fake samples.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/gan_samples.png}
    \caption{\ac{GAN}-generated samples, taken from \citet{brock2018large}.}
    \label{fig:gan_samples}
\end{figure}

Recent improvements on \acp{GAN} have led to synthesis of highly natural images as shown in Figure~\ref{fig:gan_samples}.

\subsubsection{Variational Autoencoders}\label{subsec:variational-autoencoders}

Since \acfp{VAE} are a specialization of the autoencoder, the traditional autoencoder is introduced first.

\paragraph{Autoencoders}

Autoencoders are neural networks trained to reconstruct their input~\citep[p. 499]{Goodfellow-et-al-2016}.
For autoencoders, it is common to speak of an \textit{encoder}-part and a \textit{decoder}-part.
The encoder $f: \mathbb{R}^n \mapsto \mathbb{R}^m$ transforms an input $\bm{x}$ to a hidden representation $\bm{r} = f(\bm{x})$.
Usually $m < n$, i.e.\ the encoder transforms the input to a lower-dimensional representation.
This can be beneficial for dimensionality reduction or feature learning~\citep[p. 499]{Goodfellow-et-al-2016}.
The decoder $g: \mathbb{R}^m \mapsto \mathbb{R}^n$ transforms the hidden representation back into the original feature space.
Usually, one wants the reconstruction $\tilde{x}$ to be close to the original feature $x$ ($\tilde{x} \approx x$).
In order to achieve this, the autoencoder is usually trained by minimizing $\mathcal{L}(\bm{x}, g(f(\bm{x})))$ with
\begin{align}
    \mathcal{L}: \mathbb{R}^n \times \mathbb{R}^n \mapsto \mathbb{R}
\end{align}
One common choice for $\mathcal{L}$ is the \ac{MSE} which is defined as
\begin{align}
    \mathcal{L}(\bm{x}, \bm{y}) = \frac{1}{n}\sum (\bm{x}_i - \bm{y}_i)^2 \label{eq:mse}
\end{align}~\citep[p. 106]{Goodfellow-et-al-2016}.
Note that for linear activations in the autoencoder, the space spanned by the first $m$ principal components is the optimal solution for equation~\ref{eq:mse}~\citep{chicco2014deep}.

\paragraph{Variational Autoencoders}

Autoencoders transform an input image to a \textit{hidden representation} $f(\bm{x})$.
However, the distribution over $f(\bm{x})$ is generally unknown.
Let $\bm{z} = f(\bm{x})$.
Assume $\bm{\tilde{z}} = \bm{z} + \bm{\epsilon}$ is a slightly perturbed version of $\bm{z}$, created by adding a small amount of noise $\bm{\epsilon}$.
Even though $\bm{\tilde{z}} \approx \bm{z}$, for example in terms of equation~\ref{eq:mse}, the result after decoding can be very different, i.e. $g(\bm{\tilde{z}}) \not\approx g(\bm{z})$ (again, in terms of equation~\ref{eq:mse}).
This is because the distribution $P(\bm{z})$ can take any arbitrary form.
Values of $\bm{z}$ that are one close to another\footnote{For example in terms of Euclidean distance} can be likely for very different values of $\bm{x}$.

To \textit{generate} new images it would be advantageous to enforce $P(\bm{z})$ to follow a certain distribution.
A common choice is a multivariate normal distribution without covariances~\citep[pp. 24, 25]{kingma2019introduction}.
\acp{VAE} enforce a certain (usually Gaussian) distribution over $\bm{z}$.

\acp{VAE} are trained to generate samples that are likely to occur in the training set.
Therefore, they aim to maximize $\log p(\bm{x})$\footnote{Please not that $\max \log  p(\bm{x}) \equiv \max  p(\bm{x})$.}~\citep[p. 18]{kingma2019introduction}.
Assuming a latent distribution $p(\bm{z})$ over $\bm{z}$, we can write $p(\bm{x})$ as the marginal distribution
\begin{align}
    p(\bm{x}) &= \int p(\bm{x}, \bm{z})d\bm{z} \label{eq:vae_x1}\\
    &= \int p(\bm{x}|\bm{z})\,p(\bm{z})d\bm{z} \label{eq:vae_x2}
\end{align}
Unfortunately, due to the integral w.r.t. $\bm{z}$ in equations~\ref{eq:vae_x1} and~\ref{eq:vae_x2}, computing $p(\bm{x})$ is intractable~\citep[p. 13]{kingma2019introduction}.
However, if the posterior $p(\bm{z}|\bm{x})$ was given, $p(\bm{x})$ could be obtained by
\begin{align}
    p(\bm{x}) = \frac{p(\bm{x}, \bm{z})}{p(\bm{z}|\bm{x})}
\end{align}
since computing $p(\bm{x}, \bm{z})$ is tractable~\citep[p. 14]{kingma2019introduction}.

\acp{VAE} approximate the posterior by an \textit{inference model} $q_\phi(\bm{z}|\bm{x}) \approx p_\theta(\bm{z}|\bm{x})$, also called the \textit{encoder}~\citep[p. 15]{kingma2019introduction}.

To make $q$ to match $p$ closely, usually the \ac{KL-divergence} plus the reconstruction error is minimized.
The reason for this is explained in the following.
The starting point is that we cannot compute the posterior $p_\theta(\bm{z}|\bm{x})$.
Therefore, we approximate it by another distribution $q_\phi$.
To make these two as similar as possible we use the \ac{KL-divergence} $\kldiv{q_\phi(\bm{z}|\bm{x})}{p_\theta(\bm{z}|\bm{x})}$

\begin{align}
    \kldiv{q_\phi(\bm{z}|\bm{x})}{p_\theta(\bm{z}|\bm{x})} &= -\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{z}|\bm{x})}{q_\phi(\bm{z}|\bm{x})} \\
    &= -\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{\frac{p_\theta(\bm{x},\bm{z})}{p_\theta(\bm{x})}}{\frac{q_\phi(\bm{z}|\bm{x})}{1}}\\
    &= -\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{x},\bm{z})}{q_\phi(\bm{z}|\bm{x})}\frac{1}{p_\theta(\bm{x})}\\
    &= -\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \left[ \log \frac{p_\theta(\bm{x},\bm{z})}{q_\phi(\bm{z}|\bm{x})} - \log p_\theta(\bm{x}) \right]\\
    &= -\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{x},\bm{z})}{q_\phi(\bm{z}|\bm{x})} + \sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log p_\theta(\bm{x}) \\
    &= -\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{x},\bm{z})}{q_\phi(\bm{z}|\bm{x})} + \log p_\theta(\bm{x}) \underbrace{\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x})}_{=1} \\
    &= -\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{x},\bm{z})}{q_\phi(\bm{z}|\bm{x})} + \log p_\theta(\bm{x})
\end{align}
Then
\begin{align}
    \underbrace{\vphantom{\sum_{\bm{z}} x}\log p_\theta(\bm{x})}_{\text{constant}} =  \underbrace{\vphantom{\sum_{\bm{z}} x}\kldiv{q_\phi(\bm{z}|\bm{x})}{p_\theta(\bm{z}|\bm{x})}}_{\downarrow}  + \underbrace{\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{x},\bm{z})}{q_\phi(\bm{z}|\bm{x})}}_{\uparrow}
\end{align}
Now, because $\log p_\theta(\bm{x})$ is assumed to be fixed, instead of minimizing $\kldiv{q_\phi(\bm{z}|\bm{x})}{p_\theta(\bm{z}|\bm{x})}$, we can maximize $\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{x},\bm{z})}{q_\phi(\bm{z}|\bm{x})}$, as these two always sum to a constant.
$\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{x},\bm{z})}{q_\phi(\bm{z}|\bm{x})}$ is called \textit{variational lower bound} or \acfi{ELBO}~\citep[p. 18]{kingma2019introduction}.

The variational lower bound can be written as
\begin{align}
    \sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{x},\bm{z})}{q_\phi(\bm{z}|\bm{x})} &= \sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{z}|\bm{x})p_\theta(\bm{x})}{q_\phi(\bm{z}|\bm{x})}\\
    &= \sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \left[ \log p_\theta(\bm{x}) + \log \frac{p_\theta(\bm{z}|\bm{x})}{q_\phi(\bm{z}|\bm{x})} \right]\\
    &= \sum_{\bm{z}} q_\phi(\bm{z}|\bm{x})\log p_\theta(\bm{x}) + \sum_{\bm{z}} q_\phi(\bm{z}|\bm{x})\log \frac{p_\theta(\bm{z}|\bm{x})}{q_\phi(\bm{z}|\bm{x})}\\
    &= \underbrace{\mathbb{E}_{q_\phi(\bm{z}|\bm{x})} \left[ \log p_\theta(\bm{x}) \right]}_{=\log p_\theta(\bm{x})\, \text{, independent of $\bm{z}$}} + \underbrace{\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x})\log \frac{p_\theta(\bm{z}|\bm{x})}{q_\phi(\bm{z}|\bm{x})}}_{=\kldiv{q_\phi(\bm{z}|\bm{x})}{p_\theta(\bm{z}|\bm{x})}} \label{eq:elbo_lower_bound}
\end{align}
but also as
\begin{align}
    \sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{x},\bm{z})}{q_\phi(\bm{z}|\bm{x})} &= \sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \log \frac{p_\theta(\bm{x}|\bm{z})p_\theta(\bm{z})}{q_\phi(\bm{z}|\bm{x})}\\
    &= \sum_{\bm{z}} q_\phi(\bm{z}|\bm{x}) \left[ \log p_\theta(\bm{x}|\bm{z}) + \log \frac{p_\theta(\bm{z})}{q_\phi(\bm{z}|\bm{x})} \right]\\
    &= \underbrace{\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x})\log p_\theta(\bm{x}|\bm{z})}_{=\mathbb{E}_{q_\phi(\bm{z}|\bm{x})}\left[ \log p_\theta(\bm{x}|\bm{z}) \right]} + \underbrace{\sum_{\bm{z}} q_\phi(\bm{z}|\bm{x})\log \frac{p_\theta(\bm{z})}{q_\phi(\bm{z}|\bm{x})}}_{=\kldiv{q_\phi(\bm{z}|\bm{x})}{p_\theta(\bm{z})}} \label{eq:elbo_error_term}
\end{align}

Equations~\ref{eq:elbo_lower_bound} and~\ref{eq:elbo_error_term} allow two different interpretations of the \ac{ELBO}.
Equation~\ref{eq:elbo_lower_bound} justifies the name \textit{\acl{ELBO}}.
If the \ac{KL-divergence} goes to zero, it approaches the likelihood of the data~\citep[p. 18]{kingma2019introduction}.
In particular, we cannot even compute $p_\theta(\bm{z}|\bm{x})$ with our \ac{VAE} setup.

\textbf{I am happy with the VAE part above but the following needs some polishing.
In particular, I want to work on the difference between propabilities and neural networks.
The part with the MSE needs clarification and I also have to check the notation.
Also, I should add more references.}

Equation~\ref{eq:elbo_error_term}, however, is more useful for implementing the error function of an \ac{VAE}.
Consider the term inside the expectation of the first term.
$p_\theta(\bm{x}|\bm{z})$ usually is chosen to be a normal distribution with \ac{PDF}
\begin{align}
    f_{\sigma,\mu}(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2}
\end{align}
Then
\begin{align}
    \log f_{\sigma,\mu}(x) &= \log \left( \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2} \right)\\
    &= - \log \sqrt{2\pi}\sigma -\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2 \label{eq:log_normal_distr}
\end{align}
Since $\mu$ in this case is a function of $\bm{z}$ (parametrized by $\theta$), we can consider this as the reconstruction $\hat{\bm{x}} = f_\theta(\bm{z})$.
Then, except for subtracting a constant and scaling by a constant, equation~\ref{eq:log_normal_distr} is the \ac{MSE}.

Since $p(\bm{x}|\bm{z})$ is a Gaussian, maximizing $\mathbb{E}_{q(z)} \left[ p(x|z) \right]$ is equivalent to minimizing the mean squared error between $x$ and the \textit{reconstruction} $\hat{x}$.
We can write $\hat{x}$ instead of $z$ because $p(z) = \hat{x}$ and minimizing does not change anything.

Now when actually implementing a \ac{VAE}, $q_\phi$ and $p_\theta$ are realized by neural networks.
The encoder predicts the mean and variances for a factorized Gaussian distribution whereas the decoder reconstructs an image.
The usage of neural networks poses a challenge for computing the first term in equation~\ref{eq:elbo_error_term}.
Usually, one would draw a sufficient number of samples from $q_\phi(\bm{z}|\bm{x})$ to approximate the expectation.
However, sampling is not possible if one wants to compute the gradient to train the model by backpropagation.

\textbf{TODO: Posterior Collapse}

\paragraph{Disadvantages}
Another disadvantage of \acp{VAE} is that they \say{use only a small subset of the dimensions of $\bm{z}$}~\citep[p. 694]{Goodfellow-et-al-2016}.

\subsubsection{Representation Learning}\label{subsubsec:representation_learning}

Representation learning can be understood as \say{learning representations of the data that make it easier to extract useful information when building classifiers or other predictors}~\citep{bengio2013representation}.
While this is a rather broad definition, this thesis focuses on \textit{hierarchical representations}, since this is what can be observed in the visual cortex (\textbf{QUELLE}).

The following paragraphs introduce different (hierarchical) representation learning approaches relevant to the techniques used in the course of this thesis.

\paragraph{The Latent Space}
\citet{shao2018riemannian,arvanitidis2017latent} discuss if the latent space is a Riemann space.
Elaborate on this and the consequences of this assumption in terms of latent space traversal and smoothness of the latent / semantic space.

\paragraph{InfoGAN}
InfoGANs~\cite{chen2016infogan} address feature learning in \acp{GAN} by \say{decomposing the input noise vector into two parts}~\citep{chen2016infogan}.
Instead of only the noise $z$, the generator $G$ receives an additional input of \say{structure latent variables} $\bm{c} = (\bm{c_1}, \bm{c_2}, \dots, \bm{c_L})$.
The additional variables are assumed to be independent one to another.
To force $G$ to not simply ignore $c$,~\citet{chen2016infogan} modify the training objective such that \say{there should be high mutual information between latent codes $c$ and the generator distribution $G(z,c)$}~\citep{chen2016infogan}.
The new loss function takes the form
\begin{align}
    \min _{G} \max _{D} V_I(D, G)=V(D, G) - \lambda I(\bm{c}; G(\bm{z}, \bm{c}))
\end{align}
where $V(D, G)$ is the training objective defined in Equation~\ref{eq:gan_objective}, $\lambda$ is a trade-off hyperparameter, and $I(\bm{x}; \bm{y})$ is the mutual information between $\bm{x}$ and $\bm{y}$.

\begin{figure}
    \centering
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=.8\textwidth]{images/infogan_c1.png}
        \caption{Varying $\bm{c}_1 \in [0, 10]$}
    \end{subfigure}%
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=.8\textwidth]{images/infogan_c2.png}
        \caption{Varying $\bm{c}_2 \in [-2, 2]$}
    \end{subfigure}
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=.8\textwidth]{images/infogan_c3.png}
        \caption{Varying $\bm{c}_3 \in [-2, 2]$}
    \end{subfigure}
    \caption{Exploration of $\bm{c}$ on MNIST by systematically exploring either, $\bm{c}_1$, $\bm{c}_2$, or $\bm{c}_3$. Rows correspond to different noise values $\bm{z}$ but are equal for the three figures, columns correspond to different values for $\bm{c}_i$. $\bm{c}_1$ takes discrete values, $\bm{c}_2$ and $\bm{c}_3$ take continuous values. Taken from \citet{chen2016infogan}.}
    \label{fig:infogan}
\end{figure}

\say{To disentagle digit shape styles on MNIST}, \citet{chen2016infogan} choose $\bm{c}$ as three-element set $\bm{c} = (\bm{c}_1, \bm{c}_2, \bm{c}_3)$ with $\bm{c}_1$ drawn from a categorical distribution with ten categories (equal to the number of classes in MNIST) and event probability $p_i=0.1 \, \forall i \in [0, 10]$.
$\bm{c}_2$ and $\bm{c}_3$ are drawn from a uniform distribution over $[-1, 1]$.

Figure~\ref{fig:infogan} shows the representation disentaglement by varying the $\bm{c}_i$-values.
Noteworthy, InfoGAN seems to generalize as $\bm{c}_2$ and $\bm{c}_3$ varied between $[-2, 2]$ whereas these values took values in $[-1, 1]$ at training time.

\paragraph{StyleGAN}
StyleGAN~\citep{karras2019style} is a type of \ac{GAN} explicitly addressing \textit{feature disentanglement}.
The term \textit{style} refers to feature disentanglement, i.e.~the model is capable of learning a disentanglement of images into different styles.
The styles of an image are learned at different levels.
Importantly, styles are learned in a self-supervised manner.
Therefore it is not possible to explicitly force the model to learn certain aspects of an image.
However, due to the training procedure, different styles are learned at different levels and a posterior analysis allows to identify which aspects were mainly learned on which level.
Coarse styles correspond to gender, age, or glasses.
Middle styles correspond to skin color, face form, or mouth open/closed.
Fine styles correspond mainly to hair color and lightning.

The model introduces uses some new ideas that are explained in the following.

\acp{GAN} generate new images by using some random noise $\bm{z}\in \mathcal{Z}$ as input (see Section~\ref{subsubsec:gans}).
StyleGAN uses a mapping network $f: \mathcal{Z}\mapsto \mathcal{W}$ to map the random noise to a vector $\bm{w}\in \mathcal{W}$.
With an affine transformation, the vector $w$ is then mapped to a vector $\bm{y}_i = (\bm{y}_{s,i}, \bm{y}_{b,i})$.
Different vectors $\bm{y}_i$ are fed into different layers $i$ in the generator network and control the style on this layer.

The generator network $g$ has a constant input $\bm{x}_0$ that is lower-dimensional than the final output image.
In~\citet{karras2019style}, $\bm{x}_0$ is of size $4\times 4\times 512$.
On each layer, a noise vector $\bm{\epsilon}_i$ is added, followed by an \textit{\acf{AdaIN}} operation.
The \ac{AdaIN} operation is defined as
\begin{align}
    \text{\ac{AdaIN}}(\bm{x}_i, \bm{y}) = \bm{y}_{s, i} \frac{\bm{x}_{i}-\mu\left(\bm{x}_{i}\right)}{\sigma\left(\bm{x}_{i}\right)}+\bm{y}_{b, i},
\end{align}
\citep{karras2019style} where $\mu(\cdot)$ gives the mean and $\sigma(\cdot)$ the (empirical) standard deviation.
Importantly, this operation is applied separately for each feature map of $\bm{x}_i$.

On each resolution, $g$ first applies noise addition, \ac{AdaIN}, $3\times 3$ convolution, noise addition, and one last \ac{AdaIN} operation.
Then, the image is upsampled to double size.
This process is repeated until the image has the desired output size.
\citet{karras2019style} use nine of these blocks, resulting in an output size of $1024\times 1024$.

Feature disentanglement is achieved by a technique called \textit{mixing regularization}.
Here, for a subset of the training images, two input vectors $\bm{z}_1$ and $\bm{z}_2$ are drawn.
During the training, a point (or layer) of $g$ is chosen by random up to which layer $\bm{z}_1$ controls the style.
After this point, the style information from $\bm{z}_2$ is fed to the network.
Randomly choosing this point \say{prevents the network from assuming that adjacent styles are correlated}~\citep{karras2019style}.

The training procedure allows using two sources for the generation.
During inference time, the crossover point can be chosen arbitrarily, i.e.~it can be controlled what styles are taken from the first and what styles are taken from the second source.

Now, $\bm{y}_i$ that are fed into the network at a low resolution (i.e. $i$ is small) control the coarse styles referred to in the beginning of the section, whereas $\bm{y}_i$ at higher layers control finer styles.

Finally, \citet{karras2019style} analyze how disentangled generations from $\mathcal{W}$-space are compared to $\mathcal{Z}$-space by means of \textit{perceptual path length}.
The perceptual path length is a measure of how much reconstructions change for small variation in the latent space.
It is calculated by moving by a small value $\epsilon$ on a path, obtained by interpolating between two random $\bm{z}_1$, $\bm{z}_2$ (or $\bm{w}_1$, $\bm{w}_2$)\footnote{Interpolation is spherical for $\bm{z}$ and interpolation for $\bm{w}$.} and taking the perceptual loss~\citep{johnson2016perceptual} between the image before moving by $\epsilon$ and after.
Finally, the mean of this value is obtained by applying the procedure for sufficiently many samples.

Using the previously described procedure, \citet{karras2019style} show that the perceptual path length is lower for interpolating in $\mathcal{W}$ compared to interpolating in $\mathcal{Z}$.
Therefore, they assumed that $\mathcal{Z}$ is more entangled than $\mathcal{W}$.

\footnote{Part on linear separability still missing}

\paragraph{\acl{LVAE}}

\aclp{LVAE}~\citep{sonderby2016ladder} address the problem of hierarchical learning in \acp{VAE}.
Hierarchical \acp{VAE} only use the first few layers to learn meaningful semantics of an input.
In case of \citet{sonderby2016ladder}, the first two layers are sufficient.
\citet{zhao2017learning} state that, given a sufficiently large encoder network in the first layer, the first layer learns all the semantics.
While this is not always true, they \say{demonstrate that this phenomen occurs in practive [\ldots]}.

\acp{LVAE} is a model designed to, instead of using just the first few layers, use all embedding layers to learn a representation.
They do this by passing information top-down in the inference network.
\begin{figure}
    \centering
    \includegraphics[width=0.2\textwidth]{images/gerben.png}
    \caption{Illustration of context-dependent semantic ambiguity. Taken from \citet[p. 61]{van2016auto}}
    \label{fig:gerben_ambiguity}
\end{figure}
Figure~\ref{fig:gerben_ambiguity} gives an intuition why passing information top-down can be advantegous.
Here, the two rows show two different alphanumeric strings.
Apparently, the first row shows a sequence of alphabetic characters (\say{GERBEN}) whereas the second row shows a sequence of numeric characters (\say{10 11 12 13 14 15}).
However, the \say{R} and the \say{B} in the first row are the exact same symbols as the \say{12} and the \say{13} in the second one.
A pure feed-forward systems would be unable to discriminate between the low-level features \textit{R} and \textit{13}.
Incorporating high-level features like \textit{alphabetic string} helps to improve the posterior low-level feature representation.
Furthermore, the additional top-down pass is more biologically plausible compared to a pure feedforward network (\textbf{REFERENCE}).

In fact, \acp{LVAE} model this top down pass by \textbf{BY WHAT? CONTINUE}
\textbf{DRAFT!}
\begin{itemize}
    \item Explain LVAE detailed
    \item Dont forget the warm-up phase
    \item Also mention that batch normalization and warm-up alone help a lot in disentangling the feature learning
\end{itemize}

\paragraph{\acl{VLAE}}

\paragraph{$\beta$-VAE}
\citet{higgins2017beta}

\paragraph{\ac{VAE}-\ac{GAN}}
\citet{larsen2015autoencoding}

\paragraph{Adversarial Latent Autoencoder}
\begin{figure}
    \begin{tikzpicture}
        % Nodes
        \node[draw,circle] (z) {$\bm{z}$};
        \node[draw,rectangle,right=of z] (F) {$\mathcal{F}$};
        \node[draw,circle,right=of F] (w) {$\bm{w}$};
        \node[draw,rectangle,right=of w] (G) {$\mathcal{G}$};
        \node[draw,circle,right=of G] (xt) {$\tilde{\bm{x}}$};
        \node[draw,rectangle,right=of xt] (E1) {$\mathcal{E}$};
        \node[draw,circle,right=of E1] (zt1) {$\tilde{\bm{z}}_1$};
        \node[draw,rectangle,right=of zt1] (D1) {$\mathcal{D}$};
        \node[right= of D1] (realfake1) {real / fake};
        \node[below=of G] (eta) {$\eta$};

        \node[draw,circle,below=of xt] (x) {$\bm{x}$};
        \node[draw,rectangle,right=of x] (E2) {$\mathcal{E}$};
        \node[draw,circle,right=of E2] (zt2) {$\tilde{\bm{z}}_2$};
        \node[draw,rectangle,right=of zt2] (D2) {$\mathcal{D}$};
        \node[right= of D2] (realfake2) {real / fake};
        % Arrows
        \draw[->] (z.east) -- (F.west);
        \draw[->] (F.east) -- (w.west);
        \draw[->] (w.east) -- (G.west);
        \draw[->] (G.east) -- (xt.west);
        \draw[->] (xt.east) -- (E1.west);
        \draw[->] (E1.east) -- (zt1.west);
        \draw[->] (zt1.east) -- (D1.west);
        \draw[->] (D1.east) -- (realfake1.west);
        \draw[->] (eta.north) -- (G.south);

        \draw[->] (x.east) -- (E2.west);
        \draw[->] (E2.east) -- (zt2.west);
        \draw[->] (zt2.east) -- (D2.west);
        \draw[->] (D2.east) -- (realfake2.west);

        \draw [decorate,decoration={brace,amplitude=10pt,raise=0.7cm},rotate=90] (F.west) -- (G.east) node [black,midway,yshift=1.5cm] {$G$};
        \draw [decorate,decoration={brace,amplitude=10pt,raise=0.7cm},rotate=90] (E1.west) -- (D1.east) node [black,midway,yshift=1.5cm] {$D$};
    \end{tikzpicture}
    \caption{Training of \ac{ALAE}, adapted from \citet{pidhorskyi2020adversarial}}
    \label{fig:alae_flow}
\end{figure}
\acfp{ALAE}~\citep{pidhorskyi2020adversarial} are a hybrid model between \acp{VAE} and \acp{GAN}.
\acp{ALAE} are not based on variational inference but on \acp{GAN}.

The starting point of the \ac{ALAE} model is to decompose both, the generator $G$ and discriminator $D$, into two parts, i.e.~$G=\mathcal{G}\circ \mathcal{F}$ and $D=\mathcal{D}\circ \mathcal{E}$ (see Figure~\ref{fig:alae_flow}).
$\mathcal{G}$ uses noise $\eta$ as an additional input.
$\mathcal{F}$ maps the Gaussian random noise $\bm{z} \sim \mathcal{N}(\bm{0}, \bm{I})$ to another latent representation $\bm{w}$.
The loss function requires the outputs of $\mathcal{F}$ and $\mathcal{E}$ to be similar in terms of the $l^2$-norm.
Apart from this, the $\bm{w}$-space is unregularized.
In addition to the $l^2$-loss, the model is optimized towards minimizing the discriminator and the generator loss.

Because $p_{\mathcal{E}}(w) \approx p_{\mathcal{F}}(w)$, \ac{ALAE} can generate reconstructions by
\begin{align}
    \bm{x}\approx (\mathcal{E}\circ \mathcal{G}_\eta)(\bm{x})
\end{align}
$\mathcal{G}_\eta$ here indicates that $\mathcal{G}$ uses $\eta$ as an additional input.

Furthermore, just like for the \ac{VAE}, the Gaussian latent space $Z$ can be traversed to generate new samples by
\begin{align}
    \bm{x}_{\text{new}} = (\mathcal{F}\circ \mathcal{G}_\eta)(\bm{z})
\end{align}

\citet{pidhorskyi2020adversarial} show that (linearly) interpolating in $\bm{w}$-space yields smoother transitions than interpolation between two corresponding points in $\bm{z}$-space.
This indicates that the $\bm{w}$-space is less entangled than the $\bm{z}$-space~\citep{shao2018riemannian,arvanitidis2017latent}.
However, traversing the $\bm{w}$-space is not easily possible as its structure is unknown.
Compared to \acp{VAE} where a linear interpolation between points in the latent $\bm{z}$-space leads to an almost geodesic path in $\bm{x}$-space\footnote{Therefore, points that are close in $\bm{z}$-space of an \ac{VAE} lead to images that are close in the feature space.}, the mapping function $\mathcal{F}$ seems to be non-smooth.
Therefore, traversing $\bm{z}$ for an \ac{ALAE} seems to lead to a non-smooth path in the feature space.

Compared to the \ac{VAE} that uses a pixelwise loss on the reconstructions, \ac{ALAE} penalizes the $l^2$-norm of $\bm{w}$ and $\tilde{\bm{w}}$.
Except for this loss, \ac{ALAE} by no means forces the input to be similar to the reconstruction.
This mitigates the problem of the pixelwise loss in \acp{VAE} that is assumed to be the reason for blurry reconstructions.

\paragraph{Other stuff}
\textbf{DRAFT!}
\begin{itemize}
    \item \citet{tschannen2018recent} give an overview over \say{recent advances in autoencoder-based representation learning}
    \item Discuss VAEs on sequences of images.
    \item Check \citet{pidhorskyi2020adversarial} for more related work.
\end{itemize}

\subsection{Visual Features in Neural Networks}\label{subsec:visual_features_in_neural_networks}
\textbf{DRAFT!}
\begin{itemize}
    \item~\citet{krizhevsky2012imagenet} report Gabor wavelets in \acp{CNN} trained on image classification
    \item~\citet{palm2012prediction} found Gabor wavelets in vanilla autoencoders, trained on predicting the next image in a sequence, \citet{walker2016uncertain} did something similar for VAEs
    \item~\citet{eickenberg2017seeing} and \citet{wen2018neural} compare response of certain brain areas with layers in CNNs (also many more, a good list is in \citet{wen2018neural})
\end{itemize}

\subsection{Feature Disentanglement}
\textbf{DRAFT!}
\begin{itemize}
    \item \citet{karras2019style} describe two means to measure feature entanglement: \textit{Perceptual Path Length} and \textit{Linear Separability}
    \item \citet{kim2018disentangling} give an overview of feature disentanglement for \acp{VAE}. They propose a new metric to measure feature disentanglement and a new VAE model tackling this problem.
\end{itemize}

\subsection{Semantic Representations}\label{subsec:semantic-representations}
\textbf{DRAFT!}

\subsubsection{Supervised Models}
\begin{itemize}
    \item \citet{khaligh2014deep} found evidence for that supervised models may explain \ac{IT} cortical representation
    \item \citet{cadieu2014deep} report more or less the same
\end{itemize}

\subsubsection{Unsupervised Models}
\textbf{DRAFT!}
\begin{itemize}
    \item \citet{han2019variational} found no evidence for or against Gabor wavelets in \acp{VAE} due to too small kernel size, however they were able to predict fmri activity from the latent representation in a VAE
    \item \citet{khaligh2014deep} found evidence against the assumption that unsupervised models might explain \ac{IT} cortical representation, however not explicitly for \acp{VAE}
\end{itemize}