In recent years, research in artificial neural networks has risen in due to their success in a large variety of tasks.
The increase in research has led to progressively better network architectures with greater performance.
However, the improvement of network architectures is mainly driven by the question of whether a new architecture will lead to better results \citep{lindsay2020convolutional}.

Biological plausibility, an essential consideration in computational neuroscience, is often of no interest to researchers aiming at solving complex problems.
Initially, neural networks were inspired by biology.
Modern neural networks operating on images, mainly \acp{CNN}, indeed share many features with the visual system.
However, recently, neural network research has become more disconnected from the biological example.

Even though neural networks are trained differently than how the brain learns and builds memories, it has been shown that image classification networks are related to the visual system.
This relatedness shows not only in areas where it has been intentionally built into the model.

Both the biological foundation of \acp{CNN} and the recent insights on their indirect relatedness to the visual system qualify them as a potentially useful model of the visual system.

Unfortunately, the neural networks where the relatedness has been discovered are trained in a supervised manner, requiring lots of labeled data.
This kind of learning is disconnected from human perception, where often one single example is sufficient when it comes to grouping objects into classes.

The \ac{VAE} is one important representative of \textit{generative models}.
This class of models allows a different training procedure.
They still require many samples, yet no labels.
Furthermore, \acp{VAE} build generalizable latent representations of inputs, which might be similar to the abstract representation found in the brain when perceiving the world.

\acp{VAE}, therefore, could be an important step towards a more realistic model of the visual system.
An important consideration in the context of computational neuroscience is the representation of visual input.
By design, \acp{VAE} learn an input representation that generalizes to some degree.
Furthermore, it has been shown that the latent space learns some semantic relationship.
Nonetheless, the structure of the latent space is mainly a black box.

This thesis investigates whether \acp{VAE} could be a good model of the visual system, focussing on the representation of data in the latent space.

The remainder of this work is structured as follows.

Section~\ref{sec:theoretical-background} introduces the theoretical background and related work.
Starting points for further studies and open questions are stated as well.
Section~\ref{sec:methods} describes the methods and means of work.
Section~\ref{sec:results} describes and discusses studies based on the open questions raised in Section~\ref{sec:theoretical-background}.