@article{han2019variational,
    title = "Variational autoencoder: An unsupervised model for encoding and decoding fMRI activity in visual cortex",
    journal = "NeuroImage",
    volume = "198",
    pages = "125 - 136",
    year = "2019",
    issn = "1053-8119",
    doi = "https://doi.org/10.1016/j.neuroimage.2019.05.039",
    url = "http://www.sciencedirect.com/science/article/pii/S1053811919304318",
    author = "Kuan Han and Haiguang Wen and Junxing Shi and Kun-Han Lu and Yizhen Zhang and Di Fu and Zhongming Liu",
    keywords = "Neural coding, Variational autoencoder, Bayesian brain, Visual reconstruction",
    abstract = "Goal-driven and feedforward-only convolutional neural networks (CNN) have been shown to be able to predict and decode cortical responses to natural images or videos. Here, we explored an alternative deep neural network, variational auto-encoder (VAE), as a computational model of the visual cortex. We trained a VAE with a five-layer encoder and a five-layer decoder to learn visual representations from a diverse set of unlabeled images. Using the trained VAE, we predicted and decoded cortical activity observed with functional magnetic resonance imaging (fMRI) from three human subjects passively watching natural videos. Compared to CNN, VAE could predict the video-evoked cortical responses with comparable accuracy in early visual areas, but relatively lower accuracy in higher-order visual areas. The distinction between CNN and VAE in terms of encoding performance was primarily attributed to their different learning objectives, rather than their different model architecture or number of parameters. Despite lower encoding accuracies, VAE offered a more convenient strategy for decoding the fMRI activity to reconstruct the video input, by first converting the fMRI activity to the VAE's latent variables, and then converting the latent variables to the reconstructed video frames through the VAE's decoder. This strategy was more advantageous than alternative decoding methods, e.g. partial least squares regression, for being able to reconstruct both the spatial structure and color of the visual input. Such findings highlight VAE as an unsupervised model for learning visual representation, as well as its potential and limitations for explaining cortical responses and reconstructing naturalistic and diverse visual experiences."
}

@incollection{krizhevsky2012imagenet,
    title = {ImageNet Classification with Deep Convolutional Neural Networks},
    author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
    booktitle = {Advances in Neural Information Processing Systems 25},
    editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
    pages = {1097--1105},
    year = {2012},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@article{khaligh2014deep,
    author = {Khaligh-Razavi, Seyed-Mahdi AND Kriegeskorte, Nikolaus},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation},
    year = {2014},
    month = {11},
    volume = {10},
    url = {https://doi.org/10.1371/journal.pcbi.1003915},
    pages = {1-29},
    abstract = {Author Summary Computers cannot yet recognize objects as well as humans can. Computer vision might learn from biological vision. However, neuroscience has yet to explain how brains recognize objects and must draw from computer vision for initial computational models. To make progress with this chicken-and-egg problem, we compared 37 computational model representations to representations in biological brains. The more similar a model representation was to the high-level visual brain representation, the better the model performed at object categorization. Most models did not come close to explaining the brain representation, because they missed categorical distinctions between animates and inanimates and between faces and other objects, which are prominent in primate brains. A deep neural network model that was trained by supervision with over a million category-labeled images and represents the state of the art in computer vision came closest to explaining the brain representation. Our brains appear to impose upon the visual input certain categorical divisions that are important for successful behavior. Brains might learn these divisions through evolution and individual experience. Computer vision similarly requires learning with many labeled images so as to emphasize the right categorical divisions.},
    number = {11},
    doi = {10.1371/journal.pcbi.1003915}
}


@article{freedman2007statistics,
    title = {Statistics, 4th Edition International Student Edition},
    author = {Freedman, David and Pisani, Robert and Purves, Roger},
    journal = {Pisani, R. Purves, 4th edn. WW Norton \& Company, New York},
    year = {2007}
}

@Article{freeman2013functional,
    author = {Freeman, Jeremy and Ziemba, Corey M. and Heeger, David J. and Simoncelli, Eero P. and Movshon, J. Anthony},
    title = {A functional and perceptual signature of the second visual area in primates},
    journal = {Nature Neuroscience},
    year = {2013},
    month = {Jul},
    day = {01},
    volume = {16},
    number = {7},
    pages = {974-981},
    abstract = {The authors examined neuronal responses in V1 and V2 to synthetic texture stimuli that replicate higher-order statistical dependencies found in natural images. V2, but not V1, responded differentially to these textures, in both macaque (single neurons) and human (fMRI). Human detection of naturalistic structure in the same images was predicted by V2 responses, suggesting a role for V2 in representing natural image structure.},
    issn = {1546-1726},
    doi = {10.1038/nn.3402},
    url = {https://doi.org/10.1038/nn.3402}
}


@article{logothetis1995shape,
    title = "Shape representation in the inferior temporal cortex of monkeys",
    journal = "Current Biology",
    volume = "5",
    number = "5",
    pages = "552 - 563",
    year = "1995",
    issn = "0960-9822",
    doi = "https://doi.org/10.1016/S0960-9822(95)00108-4",
    url = "http://www.sciencedirect.com/science/article/pii/S0960982295001084",
    author = "Nikos K. Logothetis and Jon Pauls and Tomaso Poggio",
    abstract = "Background: The inferior temporal cortex (IT) of the monkey has long been known to play an essential role in visual object recognition. Damage to this area results in severe deficits in perceptual learning and object recognition, without significantly affecting basic visual capacities. Consistent with these ablation studies is the discovery of IT neurons that respond to complex two-dimensional visual patterns, or objects such as faces or body parts. What is the role of these neurons in object recognition? Is such a complex configurational selectivity specific to biologically meaningful objects, or does it develop as a result of extensive exposure to any objects whose identification relies on subtle shape differences? If so, would IT neurons respond selectively to recently learned views or features of novel objects? The present study addresses this question by using combined psychophysical and electrophysiological experiments, in which monkeys learned to classify and recognize computer-generated three-dimensional objects. Results A population of IT neurons was found that responded selectively to views of previously unfamiliar objects. The cells discharged maximally to one view of an object, and their response declined gradually as the object was rotated away from this preferred view. No selective responses were ever encountered for views that the animal systematically failed to recognize. Most neurons also exhibited orientation-dependent responses during view-plane rotations. Some neurons were found to be tuned around two views of the same object, and a very small number of cells responded in a view-invariant manner. For the five different objects that were used extensively during the training of the animals, and for which behavioral performance became view-independent, multiple cells were found that were tuned around different views of the same object. A number of view-selective units showed response invariance for changes in the size of the object or the position of its image within the parafovea. Conclusion Our results suggest that IT neurons can develop a complex receptive field organization as a consequence of extensive training in the discrimination and recognition of objects. None of these objects had any prior meaning for the animal, nor did they resemble anything familiar in the monkey's environment. Simple geometric features did not appear to account for the neurons' selective responses. These findings support the idea that a population of neurons — each tuned to a different object aspect, and each showing a certain degree of invariance to image transformations — may, as an ensemble, encode at least some types of complex three-dimensional objects. In such a system, several neurons may be active for any given vantage point, with a single unit acting like a blurred template for a limited neighborhood of a single view."
}


@article{roe2012toward,
    title = "Toward a Unified Theory of Visual Area V4",
    journal = "Neuron",
    volume = "74",
    number = "1",
    pages = "12 - 29",
    year = "2012",
    issn = "0896-6273",
    doi = "https://doi.org/10.1016/j.neuron.2012.03.011",
    url = "http://www.sciencedirect.com/science/article/pii/S0896627312002747",
    author = "Anna W. Roe and Leonardo Chelazzi and Charles E. Connor and Bevil R. Conway and Ichiro Fujita and Jack L. Gallant and Haidong Lu and Wim Vanduffel",
    abstract = "Visual area V4 is a midtier cortical area in the ventral visual pathway. It is crucial for visual object recognition and has been a focus of many studies on visual attention. However, there is no unifying view of V4's role in visual processing. Neither is there an understanding of how its role in feature processing interfaces with its role in visual attention. This review captures our current knowledge of V4, largely derived from electrophysiological and imaging studies in the macaque monkey. Based on recent discovery of functionally specific domains in V4, we propose that the unifying function of V4 circuitry is to enable selective extraction of specific functional domain-based networks, whether it be by bottom-up specification of object features or by top-down attentionally driven selection."
}


@inproceedings{liu2015faceattributes,
    author = {Z. {Liu} and P. {Luo} and X. {Wang} and X. {Tang}},
    booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
    title = {Deep Learning Face Attributes in the Wild},
    year = {2015},
    pages = {3730-3738}, }


@inproceedings{imagenet_cvpr09,
    author = {J. {Deng} and W. {Dong} and R. {Socher} and L. {Li} and {Kai Li} and {Li Fei-Fei}},
    booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
    title = {ImageNet: A large-scale hierarchical image database},
    year = {2009},
    pages = {248-255}, }


@Article{ILSVRC15,
    author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
    title = {ImageNet Large Scale Visual Recognition Challenge},
    journal = {International Journal of Computer Vision},
    year = {2015},
    month = {Dec},
    day = {01},
    volume = {115},
    number = {3},
    pages = {211-252},
    abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
    issn = {1573-1405},
    doi = {10.1007/s11263-015-0816-y},
    url = {https://doi.org/10.1007/s11263-015-0816-y}
}


@book{squire2012fundamental,
    title = {Fundamental {N}euroscience},
    author = {Squire, Larry and Berg, Darwin and Bloom, Floyd E and Du Lac, Sascha and Ghosh, Anirvan and Spitzer, Nicholas C},
    year = {2012},
    publisher = {Academic Press}
}


@article{rubens1971associative,
    author = {Rubens, Alan B. and Benson, D. Frank},
    title = {Associative Visual Agnosia},
    journal = {Archives of Neurology},
    volume = {24},
    number = {4},
    pages = {305-316},
    year = {1971},
    month = {04},
    abstract = {Following an acute hypotensive episode, a patient had unusual residual visual difficulties. A right homonymous hemianopsia was the only primary neurologic deficit, but there were specific behavioral disturbances. He could not read but could write (alexia without agraphia), had difficulty in color recognition (color agnosia), facial recognition (prosopagnosia), and object recognition (visual agnosia). Despite these visual disturbances, adequate primary visual function was demonstrated by his ability to copy drawings. Clinical evidence suggests that the underlying lesion was infarction involving the left medial occipital region and the splenium of the corpus callosum. The possibility of a second lesion in the right hemisphere can be conjectured but not supported by available data. Inability to identify or recognize visually presented objects with preservation of ability to draw them is unusual and can best be termed associative visual agnosia.Visual agnosia; prosopagnosia; color agnosia; alexia without agraphia; corpus callosum.},
    issn = {0003-9942},
    doi = {10.1001/archneur.1971.00480340037003},
    url = {https://doi.org/10.1001/archneur.1971.00480340037003},
    eprint = {https://jamanetwork.com/journals/jamaneurology/articlepdf/570416/archneur\_24\_4\_003.pdf},
}




@book{Goodfellow-et-al-2016,
    title = {Deep Learning},
    author = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher = {MIT Press},
    url = {http://www.deeplearningbook.org},
    year = {2016}
}

@inproceedings{chicco2014deep,
    author = {Chicco, Davide and Sadowski, Peter and Baldi, Pierre},
    title = {Deep Autoencoder Neural Networks for Gene Ontology Annotation Predictions},
    year = {2014},
    isbn = {9781450328944},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2649387.2649442},
    doi = {10.1145/2649387.2649442},
    booktitle = {Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics},
    pages = {533–540},
    numpages = {8},
    keywords = {biomolecular annotations, matrix-completion, gene ontology, truncated singular value decomposition, neural networks, autoencoders, principal component analysis},
    location = {Newport Beach, California},
    series = {BCB ’14}
}

@article{kingma2019introduction,
    url = {http://dx.doi.org/10.1561/2200000056},
    year = {2019},
    volume = {12},
    journal = {Foundations and Trends® in Machine Learning},
    title = {An Introduction to Variational Autoencoders},
    doi = {10.1561/2200000056},
    issn = {1935-8237},
    number = {4},
    pages = {307-392},
    author = {Diederik P. Kingma and Max Welling}
}


@article{palm2012prediction,
    author = "R. B. Palm",
    title = "Prediction as a candidate for learning deep hierarchical models of data",
    year = "2012",
    school = "Technical University of Denmark, {DTU} Informatics, {E-}mail: reception@imm.dtu.dk",
    address = "Asmussens Alle. Building 305. {DK-}2800 Kgs. Lyngby. Denmark",
    note = "Supervised by Associate Professor Ole Winther, owi@imm.dtu.dk, {DTU} Informatics, and Morten M{\o}rup, mm@imm.dtu.dk, {DTU} Informatics",
    url = "http://www.imm.dtu.dk/English.aspx",
    abstract = "Recent findings [HOT06] have made possible the learning of deep layered hierarchical representations of data mimicking the brains working. It is hoped that this paradigm will unlock some of the power of the brain and lead to advances towards true {AI}. In this thesis I implement and evaluate state-of-the-art deep learning models and using these as building blocks I investigate the hypothesis that predicting the time-to-time sensory input is a good learning objective. I introduce the Predictive Encoder (PE) and show that a simple non-regularized learning rule, minimizing prediction error on natural video patches leads to receptive fields similar to those found in Macaque monkey visual area V1. I scale this model to video of natural scenes by introducing the Convolutional Predictive Encoder (CPE) and show similar results. Both models can be used in deep architectures as a deep learning module."
}


@InProceedings{walker2016uncertain,
    author = "Walker, Jacob and Doersch, Carl and Gupta, Abhinav and Hebert, Martial",
    editor = "Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max",
    title = "An Uncertain Future: Forecasting from Static Images Using Variational Autoencoders",
    booktitle = "Computer Vision -- ECCV 2016",
    year = "2016",
    publisher = "Springer International Publishing",
    address = "Cham",
    pages = "835--851",
    abstract = "In a given scene, humans can easily predict a set of immediate future events that might happen. However, pixel-level anticipation in computer vision is difficult because machine learning struggles with the ambiguity in predicting the future. In this paper, we focus on predicting the dense trajectory of pixels in a scene---what will move in the scene, where it will travel, and how it will deform over the course of one second. We propose a conditional variational autoencoder as a solution to this problem. In this framework, direct inference from the image shapes the distribution of possible trajectories while latent variables encode information that is not available in the image. We show that our method predicts events in a variety of scenes and can produce multiple different predictions for an ambiguous future. We also find that our method learns a representation that is applicable to semantic vision tasks.",
    isbn = "978-3-319-46478-7"
}


@article{cadieu2014deep,
    author = {Cadieu, Charles F. AND Hong, Ha AND Yamins, Daniel L. K. AND Pinto, Nicolas AND Ardila, Diego AND Solomon, Ethan A. AND Majaj, Najib J. AND DiCarlo, James J.},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition},
    year = {2014},
    month = {12},
    volume = {10},
    url = {https://doi.org/10.1371/journal.pcbi.1003963},
    pages = {1-18},
    abstract = {Author Summary Primates are remarkable at determining the category of a visually presented object even in brief presentations, and under changes to object exemplar, position, pose, scale, and background. To date, this behavior has been unmatched by artificial computational systems. However, the field of machine learning has made great strides in producing artificial deep neural network systems that perform highly on object recognition benchmarks. In this study, we measured the responses of neural populations in inferior temporal (IT) cortex across thousands of images and compared the performance of neural features to features derived from the latest deep neural networks. Remarkably, we found that the latest artificial deep neural networks achieve performance equal to the performance of IT cortex. Both deep neural networks and IT cortex create representational spaces in which images with objects of the same category are close, and images with objects of different categories are far apart, even in the presence of large variations in object exemplar, position, pose, scale, and background. Furthermore, we show that the top-level features in these models exceed previous models in predicting the IT neural responses themselves. This result indicates that the latest deep neural networks may provide insight into understanding primate visual processing.},
    number = {12},
    doi = {10.1371/journal.pcbi.1003963}
}

@conference{tschannen2018recent,
    author = {Tschannen, Michael and Bachem, Olivier and Lucic, Mario},
    title = {Recent advances in autoencoder-based representation learning},
    booktitle = {Third workshop on Bayesian Deep Learning (NeurIPS 2018)},
    year = 2018,
    keywords = {autoencoder, variational autoencoder, representation learning, disentanglement, rate-distortion tradeoff, unsupervised},
    url = {http://www.nari.ee.ethz.ch/pubs/p/autoenc2018}
}

@article{eickenberg2017seeing,
    title = "Seeing it all: Convolutional network layers map the function of the human visual system",
    journal = "NeuroImage",
    volume = "152",
    pages = "184 - 194",
    year = "2017",
    issn = "1053-8119",
    doi = "https://doi.org/10.1016/j.neuroimage.2016.10.001",
    url = "http://www.sciencedirect.com/science/article/pii/S1053811916305481",
    author = "Michael Eickenberg and Alexandre Gramfort and Gaël Varoquaux and Bertrand Thirion",
    abstract = "Convolutional networks used for computer vision represent candidate models for the computations performed in mammalian visual systems. We use them as a detailed model of human brain activity during the viewing of natural images by constructing predictive models based on their different layers and BOLD fMRI activations. Analyzing the predictive performance across layers yields characteristic fingerprints for each visual brain region: early visual areas are better described by lower level convolutional net layers and later visual areas by higher level net layers, exhibiting a progression across ventral and dorsal streams. Our predictive model generalizes beyond brain responses to natural images. We illustrate this on two experiments, namely retinotopy and face-place oppositions, by synthesizing brain activity and performing classical brain mapping upon it. The synthesis recovers the activations observed in the corresponding fMRI studies, showing that this deep encoding model captures representations of brain function that are universal across experimental paradigms."
}

@article{wen2018neural,
    author = {Wen, Haiguang and Shi, Junxing and Zhang, Yizhen and Lu, Kun-Han and Cao, Jiayue and Liu, Zhongming},
    title = "{Neural Encoding and Decoding with Deep Learning for Dynamic Natural Vision}",
    journal = {Cerebral Cortex},
    volume = {28},
    number = {12},
    pages = {4136-4160},
    year = {2017},
    month = {10},
    abstract = "{Convolutional neural network (CNN) driven by image recognition has been shown to be able to explain cortical responses to static pictures at ventral-stream areas. Here, we further showed that such CNN could reliably predict and decode functional magnetic resonance imaging data from humans watching natural movies, despite its lack of any mechanism to account for temporal dynamics or feedback processing. Using separate data, encoding and decoding models were developed and evaluated for describing the bi-directional relationships between the CNN and the brain. Through the encoding models, the CNN-predicted areas covered not only the ventral stream, but also the dorsal stream, albeit to a lesser degree; single-voxel response was visualized as the specific pixel pattern that drove the response, revealing the distinct representation of individual cortical location; cortical activation was synthesized from natural images with high-throughput to map category representation, contrast, and selectivity. Through the decoding models, fMRI signals were directly decoded to estimate the feature representations in both visual and semantic spaces, for direct visual reconstruction and semantic categorization, respectively. These results corroborate, generalize, and extend previous findings, and highlight the value of using deep learning, as an all-in-one model of the visual cortex, to understand and decode natural vision.}",
    issn = {1047-3211},
    doi = {10.1093/cercor/bhx268},
    url = {https://doi.org/10.1093/cercor/bhx268},
    eprint = {https://academic.oup.com/cercor/article-pdf/28/12/4136/26338870/bhx268.pdf},
}

@article{mockus1978application,
    title = {The application of Bayesian methods for seeking the extremum},
    author = {Mockus, Jonas and Tiesis, Vytautas and Zilinskas, Antanas},
    journal = {Towards global optimization},
    volume = {2},
    number = {117-129},
    pages = {2},
    year = {1978}
}

@incollection{goodfellow2014gans,
    title = {Generative Adversarial Nets},
    author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
    booktitle = {Advances in Neural Information Processing Systems 27},
    editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
    pages = {2672--2680},
    year = {2014},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf}
}


@inproceedings{brock2018large,
    title = {Large Scale {GAN} Training for High Fidelity Natural Image Synthesis},
    author = {Andrew Brock and Jeff Donahue and Karen Simonyan},
    booktitle = {International Conference on Learning Representations},
    year = {2019},
    url = {https://openreview.net/forum?id=B1xsqj09Fm},
}

@incollection{chen2016infogan,
    title = {InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets},
    author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
    booktitle = {Advances in Neural Information Processing Systems 29},
    editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
    pages = {2172--2180},
    year = {2016},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.nips.cc/paper/6399-infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets.pdf}
}


@article{bengio2013representation,
    title = {Representation learning: A review and new perspectives},
    author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
    journal = {IEEE transactions on pattern analysis and machine intelligence},
    volume = {35},
    number = {8},
    pages = {1798--1828},
    year = {2013},
    publisher = {IEEE}
}


@incollection{sonderby2016ladder,
    title = {Ladder Variational Autoencoders},
    author = {Sønderby, Casper Kaae and Raiko, Tapani and Maaløe, Lars and S\o nderby, Søren Kaae and Winther, Ole},
    booktitle = {Advances in Neural Information Processing Systems 29},
    editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
    pages = {3738--3746},
    year = {2016},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.nips.cc/paper/6275-ladder-variational-autoencoders.pdf}
}


@InProceedings{zhao2017learning,
    title = {Learning Hierarchical Features from Deep Generative Models},
    author = {Shengjia Zhao and Jiaming Song and Stefano Ermon},
    booktitle = {Proceedings of the 34th International Conference on Machine Learning},
    pages = {4091--4099},
    year = {2017},
    editor = {Doina Precup and Yee Whye Teh},
    volume = {70},
    series = {Proceedings of Machine Learning Research},
    address = {International Convention Centre, Sydney, Australia},
    month = {06--11 Aug},
    publisher = {PMLR},
    pdf = {http://proceedings.mlr.press/v70/zhao17c/zhao17c.pdf},
    url = {http://proceedings.mlr.press/v70/zhao17c.html},
    abstract = {Deep neural networks have been shown to be very successful at learning feature hierarchies in supervised learning tasks. Generative models, on the other hand, have benefited less from hierarchical models with multiple layers of latent variables. In this paper, we prove that hierarchical latent variable models do not take advantage of the hierarchical structure when trained with existing variational methods, and provide some limitations on the kind of features existing models can learn. Finally we propose an alternative architecture that do not suffer from these limitations. Our model is able to learn highly interpretable and disentangled hierarchical features on several natural image datasets with no task specific regularization or prior knowledge.}
}


@phdthesis{van2016auto,
    title = {What auto-encoders could learn from brains},
    author = {van den Broeke, Gerben},
    year = {2016},
    school = {Master’s thesis, Aalto University, Finland}
}

@article{goodale1992separate,
    title = {Separate visual pathways for perception and action},
    author = {Goodale, Melvyn A and Milner, A David and others},
    journal = {Trends in neurosciences},
    volume = {15},
    number = {1},
    pages = {20--25},
    year = {1992},
    publisher = {Elsevier}
}

@book{mack2013principles,
    title = {Principles of Neural Science, Fifth Edition},
    author = {Mack, S. and Kandel, E.R. and Jessell, T.M. and Schwartz, J.H. and Siegelbaum, S.A. and Hudspeth, A.J.},
    isbn = {9780071390118},
    lccn = {2012023071},
    series = {Principles of Neural Science},
    url = {https://books.google.de/books?id=s64z-LdAIsEC},
    year = {2013},
    publisher = {McGraw-Hill Education}
}

@article{castro2019morpho,
    title = {Morpho-Mnist: Quantitative assessment and diagnostics for representation learning},
    author = {Castro, D and Tan, Jeremy and Kainz, Bernhard and Konukoglu, Ender and Glocker, Ben},
    journal = {Journal of Machine Learning Research},
    volume = {20},
    year = {2019}
}

@ARTICLE{lecun1998gradient,
    author = {Y. {Lecun} and L. {Bottou} and Y. {Bengio} and P. {Haffner}},
    journal = {Proceedings of the IEEE},
    title = {Gradient-based learning applied to document recognition},
    year = {1998},
    volume = {86},
    number = {11},
    pages = {2278-2324}, }

@InProceedings{pidhorskyi2020adversarial,
    author = {Pidhorskyi, Stanislav and Adjeroh, Donald A. and Doretto, Gianfranco},
    title = {Adversarial Latent Autoencoders},
    booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2020}
}

@inproceedings{karras2019style,
    title = {A style-based generator architecture for generative adversarial networks},
    author = {Karras, Tero and Laine, Samuli and Aila, Timo},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
    pages = {4401--4410},
    year = {2019}
}

@InProceedings{johnson2016perceptual,
    author = "Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li",
    editor = "Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max",
    title = "Perceptual Losses for Real-Time Style Transfer and Super-Resolution",
    booktitle = "Computer Vision -- ECCV 2016",
    year = "2016",
    publisher = "Springer International Publishing",
    address = "Cham",
    pages = "694--711",
    abstract = "We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.",
    isbn = "978-3-319-46475-6"
}



@InProceedings{kim2018disentangling,
    title = {Disentangling by Factorising},
    author = {Kim, Hyunjik and Mnih, Andriy},
    booktitle = {Proceedings of the 35th International Conference on Machine Learning},
    pages = {2649--2658},
    year = {2018},
    editor = {Dy, Jennifer and Krause, Andreas},
    volume = {80},
    series = {Proceedings of Machine Learning Research},
    address = {Stockholmsmässan, Stockholm Sweden},
    month = {10--15 Jul},
    publisher = {PMLR},
    pdf = {http://proceedings.mlr.press/v80/kim18b/kim18b.pdf},
    url = {http://proceedings.mlr.press/v80/kim18b.html},
    abstract = {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon beta-VAE by providing a better trade-off between disentanglement and reconstruction quality and being more robust to the number of training iterations. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.}
}


@article{higgins2017beta,
    title = {beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework.},
    author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
    journal = {Proceedings of the 5th International Conference on Learning Representations (ICLR)},
    volume = {2},
    number = {5},
    pages = {6},
    year = {2017}
}

@InProceedings{larsen2015autoencoding,
    title = {Autoencoding beyond pixels using a learned similarity metric},
    author = {Anders Boesen Lindbo Larsen and Søren Kaae Sønderby and Hugo Larochelle and Ole Winther},
    booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
    pages = {1558--1566},
    year = {2016},
    editor = {Maria Florina Balcan and Kilian Q. Weinberger},
    volume = {48},
    series = {Proceedings of Machine Learning Research},
    address = {New York, New York, USA},
    month = {20--22 Jun},
    publisher = {PMLR},
    pdf = {http://proceedings.mlr.press/v48/larsen16.pdf},
    url = {http://proceedings.mlr.press/v48/larsen16.html},
    abstract = {We present an autoencoder that leverages learned representations to better measure similarities in data space. By combining a variational autoencoder (VAE) with a generative adversarial network (GAN) we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors to better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that it outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that the method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.}
}


@inproceedings{shao2018riemannian,
    author = {H. Shao and A. Kumar and P. Fletcher},
    booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
    title = {The Riemannian Geometry of Deep Generative Models},
    year = {2018},
    pages = {428-4288},
    keywords = {manifolds;jacobian matrices;computational modeling;measurement;geometry;generators;data models},
    doi = {10.1109/CVPRW.2018.00071},
    url = {https://doi.ieeecomputersociety.org/10.1109/CVPRW.2018.00071},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
    month = {jun}
}



@inproceedings{arvanitidis2017latent,
    title = "Latent space oddity: On the curvature of deep generative models",
    abstract = "Deep generative models provide a systematic way to learn nonlinear data distributions through a set of latent variables and a nonlinear “generator” function that maps latent points into the input space. The nonlinearity of the generator implies that the latent space gives a distorted view of the input space. Under mild conditions, we show that this distortion can be characterized by a stochastic Riemannian metric, and we demonstrate that distances and interpolants are significantly improved under this metric. This in turn improves probability distributions, sampling algorithms and clustering in the latent space. Our geometric analysis further reveals that current generators provide poor variance estimates and we propose a new generator architecture with vastly improved variance estimates. Results are demonstrated on convolutional and fully connected variational autoencoders, but the formalism easily generalizes to other deep generative models.",
    author = "Georgios Arvanitidis and Hansen, {Lars Kai} and S{\o}ren Hauberg",
    year = "2018",
    month = "jan",
    day = "1",
    language = "English",
    booktitle = "Proceedings of International Conference on Learning Representations",
    note = "6th International Conference on Learning Representations, ICLR 2018 ; Conference date: 30-04-2018 Through 03-05-2018",
}

@book{bishop2006pattern,
    title = {Pattern recognition and machine learning},
    author = {Bishop, Christopher M},
    year = {2006},
    publisher = {springer}
}

@inproceedings{
Shu2020Weakly,
    title = {Weakly Supervised Disentanglement with Guarantees},
    author = {Rui Shu and Yining Chen and Abhishek Kumar and Stefano Ermon and Ben Poole},
    booktitle = {International Conference on Learning Representations},
    year = {2020},
    url = {https://openreview.net/forum?id=HJgSwyBKvr}
}

@book{burger2009principles,
    author = {Burger, Wilhelm and Burge, Mark J.},
    title = {Principles of Digital Image Processing: Core Algorithms},
    year = {2009},
    isbn = {1848001940},
    publisher = {Springer Publishing Company, Incorporated},
    edition = {1}
}

@misc{dsprites17,
    author = {Loic Matthey and Irina Higgins and Demis Hassabis and Alexander Lerchner},
    title = {dSprites: Disentanglement testing Sprites dataset},
    url = {https://github.com/deepmind/dsprites-dataset/},
    year = "2017",
}

@misc{lexico,
    title = "Lexico",
    publisher = "Oxford University Press",
    url = "www.lexico.com",
    year = {2020},
    month = {Jul},
    day = {4} }


@Article{vanrullen2019reconstructing,
    author = {VanRullen, Rufin and Reddy, Leila},
    title = {Reconstructing faces from fMRI patterns using deep generative neural networks},
    journal = {Communications Biology},
    year = {2019},
    month = {May},
    day = {21},
    volume = {2},
    number = {1},
    pages = {193},
    abstract = {Although distinct categories are reliably decoded from fMRI brain responses, it has proved more difficult to distinguish visually similar inputs, such as different faces. Here, we apply a recently developed deep learning system to reconstruct face images from human fMRI. We trained a variational auto-encoder (VAE) neural network using a GAN (Generative Adversarial Network) unsupervised procedure over a large data set of celebrity faces. The auto-encoder latent space provides a meaningful, topologically organized 1024-dimensional description of each image. We then presented several thousand faces to human subjects, and learned a simple linear mapping between the multi-voxel fMRI activation patterns and the 1024 latent dimensions. Finally, we applied this mapping to novel test images, translating fMRI patterns into VAE latent codes, and codes into face reconstructions. The system not only performed robust pairwise decoding (>95 {\%} correct), but also accurate gender classification, and even decoded which face was imagined, rather than seen.},
    issn = {2399-3642},
    doi = {10.1038/s42003-019-0438-y},
    url = {https://doi.org/10.1038/s42003-019-0438-y}
}


@Article{yoshida2020natural,
    author = {Yoshida, Takashi and Ohki, Kenichi},
    title = {Natural images are reliably represented by sparse and variable populations of neurons in visual cortex},
    journal = {Nature Communications},
    year = {2020},
    month = {Feb},
    day = {13},
    volume = {11},
    number = {1},
    pages = {872},
    abstract = {Natural scenes sparsely activate neurons in the primary visual cortex (V1). However, how sparsely active neurons reliably represent complex natural images and how the information is optimally decoded from these representations have not been revealed. Using two-photon calcium imaging, we recorded visual responses to natural images from several hundred V1 neurons and reconstructed the images from neural activity in anesthetized and awake mice. A single natural image is linearly decodable from a surprisingly small number of highly responsive neurons, and the remaining neurons even degrade the decoding. Furthermore, these neurons reliably represent the image across trials, regardless of trial-to-trial response variability. Based on our results, diverse, partially overlapping receptive fields ensure sparse and reliable representation. We suggest that information is reliably represented while the corresponding neuronal patterns change across trials and collecting only the activity of highly responsive neurons is an optimal decoding strategy for the downstream neurons.},
    issn = {2041-1723},
    doi = {10.1038/s41467-020-14645-x},
    url = {https://doi.org/10.1038/s41467-020-14645-x}
}

@article{jones1987evaluation,
    author = {Jones, J. P. and Palmer, L. A.},
    title = {An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex},
    journal = {Journal of Neurophysiology},
    volume = {58},
    number = {6},
    pages = {1233-1258},
    year = {1987},
    doi = {10.1152/jn.1987.58.6.1233},
    note = {PMID: 3437332},
    URL = {https://doi.org/10.1152/jn.1987.58.6.1233},
    eprint = {https://doi.org/10.1152/jn.1987.58.6.1233},
    abstract = { 1. Using the two-dimensional (2D) spatial and spectral response profiles described in the previous two reports, we test Daugman's generalization of Marcelja's hypothesis that simple receptive fields belong to a class of linear spatial filters analogous to those described by Gabor and referred to here as 2D Gabor filters. 2. In the space domain, we found 2D Gabor filters that fit the 2D spatial response profile of each simple cell in the least-squared error sense (with a simplex algorithm), and we show that the residual error is devoid of spatial structure and statistically indistinguishable from random error. 3. Although a rigorous statistical approach was not possible with our spectral data, we also found a Gabor function that fit the 2D spectral response profile of each simple cell and observed that the residual errors are everywhere small and unstructured. 4. As an assay of spatial linearity in two dimensions, on which the applicability of Gabor theory is dependent, we compare the filter parameters estimated from the independent 2D spatial and spectral measurements described above. Estimates of most parameters from the two domains are highly correlated, indicating that assumptions about spatial linearity are valid. 5. Finally, we show that the functional form of the 2D Gabor filter provides a concise mathematical expression, which incorporates the important spatial characteristics of simple receptive fields demonstrated in the previous two reports. Prominent here are 1) Cartesian separable spatial response profiles, 2) spatial receptive fields with staggered subregion placement, 3) Cartesian separable spectral response profiles, 4) spectral response profiles with axes of symmetry not including the origin, and 5) the uniform distribution of spatial phase angles. 6. We conclude that the Gabor function provides a useful and reasonably accurate description of most spatial aspects of simple receptive fields. Thus it seems that an optimal strategy has evolved for sampling images simultaneously in the 2D spatial and spatial frequency domains. }
}

@Article{hubel1962receptive,
    author = {Hubel, D. H. and Wiesel, T. N.},
    title = {Receptive fields, binocular interaction and functional architecture in the cat's visual cortex},
    journal = {The Journal of physiology},
    year = {1962},
    month = {Jan},
    volume = {160},
    number = {1},
    pages = {106-154},
    keywords = {*CEREBRAL CORTEX/physiology; Animals; Cats; Cerebral Cortex/*physiology; *Visual Cortex},
    note = {14449617[pmid]},
    note = {PMC1359523[pmcid]},
    issn = {0022-3751},
    doi = {10.1113/jphysiol.1962.sp006837},
    url = {https://pubmed.ncbi.nlm.nih.gov/14449617},
    url = {https://doi.org/10.1113/jphysiol.1962.sp006837},
    language = {eng}
}

@article{lindsay2020convolutional,
    author = {Lindsay, Grace},
    title = {Convolutional Neural Networks as a Model of the Visual System: Past, Present, and Future},
    journal = {Journal of Cognitive Neuroscience},
    volume = {0},
    number = {0},
    pages = {1-15},
    year = {0},
    doi = {10.1162/jocn\_a\_01544},
    note = {PMID: 32027584},
    URL = {https://doi.org/10.1162/jocn_a_01544},
    eprint = {https://doi.org/10.1162/jocn_a_01544},
    abstract = { Convolutional neural networks (CNNs) were inspired by early findings in the study of biological vision. They have since become successful tools in computer vision and state-of-the-art models of both neural activity and behavior on visual tasks. This review highlights what, in the context of CNNs, it means to be a good model in computational neuroscience and the various ways models can provide insight. Specifically, it covers the origins of CNNs and the methods by which we validate them as models of biological vision. It then goes on to elaborate on what we can learn about biological vision by understanding and experimenting on CNNs and discusses emerging opportunities for the use of CNNs in vision research beyond basic object recognition. }
}
s
@InProceedings{fukushima1980neocognitron,
    author = "Fukushima, Kunihiko and Miyake, Sei",
    editor = "Amari, Shun-ichi and Arbib, Michael A.",
    title = "Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Visual Pattern Recognition",
    booktitle = "Competition and Cooperation in Neural Nets",
    year = "1982",
    publisher = "Springer Berlin Heidelberg",
    address = "Berlin, Heidelberg",
    pages = "267--285",
    abstract = "A neural network model, called a ``neocognitron'', is proposed for a mechanism of visual pattern recognition. It is demonstrated by computer simulation that the neocognitron has characteristics similar to those of visual systems of vertebrates.",
    isbn = "978-3-642-46466-9"
}



@inproceedings{liu2015hmax,
    title = {HMAX model: A survey},
    author = {Liu, Chang and Sun, Fuchun},
    booktitle = {2015 International Joint Conference on Neural Networks (IJCNN)},
    pages = {1--7},
    year = {2015},
    organization = {IEEE}
}

@Article{riesenhuber1999hierarchical,
    author = {Riesenhuber, Maximilian and Poggio, Tomaso},
    title = {Hierarchical models of object recognition in cortex},
    journal = {Nature Neuroscience},
    year = {1999},
    month = {Nov},
    day = {01},
    volume = {2},
    number = {11},
    pages = {1019-1025},
    abstract = {Visual processing in cortex is classically modeled as a hierarchy of increasingly sophisticated representations, naturally extending the model of simple to complex cells of Hubel and Wiesel. Surprisingly, little quantitative modeling has been done to explore the biological feasibility of this class of models to explain aspects of higher-level visual processing such as object recognition. We describe a new hierarchical model consistent with physiological data from inferotemporal cortex that accounts for this complex visual task and makes testable predictions. The model is based on a MAX-like operation applied to inputs to certain cortical neurons that may have a general role in cortical function.},
    issn = {1546-1726},
    doi = {10.1038/14819},
    url = {https://doi.org/10.1038/14819}
}

@article{lecun1989backpropagation,
    author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
    title = {Backpropagation Applied to Handwritten Zip Code Recognition},
    journal = {Neural Computation},
    volume = {1},
    number = {4},
    pages = {541-551},
    year = {1989},
    doi = {10.1162/neco.1989.1.4.541},
    URL = {https://doi.org/10.1162/neco.1989.1.4.541},
    eprint = {https://doi.org/10.1162/neco.1989.1.4.541},
    abstract = { The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification. }
}

@inproceedings{frankle2018lottery,
    title = {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
    author = {Jonathan Frankle and Michael Carbin},
    booktitle = {International Conference on Learning Representations},
    year = {2019},
    url = {https://openreview.net/forum?id=rJl-b3RcF7},
}

@article{lucas2019understanding,
    title = {Understanding posterior collapse in generative latent variable models},
    author = {Lucas, James and Tucker, George and Grosse, Roger and Norouzi, Mohammad},
    journal = {International Conference on Learning Representations, Workshop Paper},
    year = {2019}
}

@article{zhao2017towards,
    title = {Towards deeper understanding of variational autoencoding models},
    author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
    journal = {arXiv preprint arXiv:1702.08658},
    year = {2017}
}

@ARTICLE{rodriguez2015hierarchical,
    AUTHOR={Rodríguez-Sánchez, Antonio J. and Fallah, Mazyar and Leonardis, Aleš},
    TITLE={Editorial: Hierarchical Object Representations in the Visual Cortex and Computer Vision},
    JOURNAL={Frontiers in Computational Neuroscience},
    VOLUME={9},
    PAGES={142},
    YEAR={2015},
    URL={https://www.frontiersin.org/article/10.3389/fncom.2015.00142},
    DOI={10.3389/fncom.2015.00142},
    ISSN={1662-5188}
}

@inproceedings{mikolov2013efficient,
    author = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
    editor = {Yoshua Bengio and Yann LeCun},
    title = {Efficient Estimation of Word Representations in Vector Space},
    booktitle = {1st International Conference on Learning Representations, {ICLR} 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings},
    year = {2013},
    url = {http://arxiv.org/abs/1301.3781},
    timestamp = {Thu, 25 Jul 2019 14:25:36 +0200},
    biburl = {https://dblp.org/rec/journals/corr/abs-1301-3781.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{berkes2005slow,
    author = {Berkes, Pietro and Wiskott, Laurenz},
    title = "{Slow feature analysis yields a rich repertoire of complex cell properties}",
    journal = {Journal of Vision},
    volume = {5},
    number = {6},
    pages = {9-9},
    year = {2005},
    month = {07},
    abstract = "{ In this study we investigate temporal slowness as a learning principle for receptive fields using slow feature analysis, a new algorithm to determine functions that extract slowly varying signals from the input data. We find a good qualitative and quantitative match between the set of learned functions trained on image sequences and the population of complex cells in the primary visual cortex (V1). The functions show many properties found also experimentally in complex cells, such as direction selectivity, non-orthogonal inhibition, end-inhibition, and side-inhibition. Our results demonstrate that a single unsupervised learning principle can account for such a rich repertoire of receptive field properties.}",
    issn = {1534-7362},
    doi = {10.1167/5.6.9},
    url = {https://doi.org/10.1167/5.6.9},
    eprint = {https://arvojournals.org/arvo/content\_public/journal/jov/933512/jov-5-6-9.pdf},
}


@article{burgess2018understanding,
    title = {Understanding disentangling in $\beta$-VAE},
    author = {Burgess, Christopher P and Higgins, Irina and Pal, Arka and Matthey, Loic and Watters, Nick and Desjardins, Guillaume and Lerchner, Alexander},
    journal = {Learning Disentangled Representations: From Perception to Control Workshop},
    year = {2017}
}

@inproceedings{ruan2016exploring,
    title = "Exploring Semantic Representation in Brain Activity Using Word Embeddings",
    author = "Ruan, Yu-Ping  and Ling, Zhen-Hua  and Hu, Yu",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1064",
    doi = "10.18653/v1/D16-1064",
    pages = "669--679",
}

@inproceedings{anderson2013words,
    title = "Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts",
    author = "Anderson, Andrew J.  and Bruni, Elia  and Bordignon, Ulisse  and Poesio, Massimo  and Baroni, Marco",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D13-1202",
    pages = "1960--1970",
}

@article{fairhall2013brain,
    author = {Fairhall, Scott L. and Caramazza, Alfonso},
    title = {Brain Regions That Represent Amodal Conceptual Knowledge},
    volume = {33},
    number = {25},
    pages = {10552--10558},
    year = {2013},
    doi = {10.1523/JNEUROSCI.0051-13.2013},
    publisher = {Society for Neuroscience},
    abstract = {To what extent do the brain regions implicated in semantic processing contribute to the representation of amodal conceptual content rather than modality-specific mechanisms or mechanisms of semantic access and manipulation? Here, we propose that a brain region can be considered to represent amodal conceptual object knowledge if it is supramodal and plays a role in distinguishing among the conceptual representations of different objects. In an fMRI study, human participants made category typicality judgments about pictured objects or their names drawn from five different categories. Crossmodal multivariate pattern analysis revealed a network of six left-lateralized regions largely outside of category-selective visual cortex that showed a supramodal representation of object categories. These were located in the posterior middle/inferior temporal gyrus (pMTG/ITG), angular gyrus, ventral temporal cortex, posterior cingulate/precuneus (PC), and lateral and dorsomedial prefrontal cortex. Representational similarity analysis within these regions determined that the similarity between category-specific patterns of neural activity in the pMTG/ITG and the PC was consistent with the semantic similarity between these categories. This finding supports the PC and pMTG/ITG as candidate regions for the amodal representation of the conceptual properties of objects.},
    issn = {0270-6474},
    URL = {https://www.jneurosci.org/content/33/25/10552},
    eprint = {https://www.jneurosci.org/content/33/25/10552.full.pdf},
    journal = {Journal of Neuroscience}
}


@article{che2016mode,
    title = {Mode regularized generative adversarial networks},
    author = {Che, Tong and Li, Yanran and Jacob, Athul Paul and Bengio, Yoshua and Li, Wenjie},
    journal = {Proceedings of the International Conference on Learning Representations (ICLR)},
    year = {2017}
}

@article{ham2020unbalanced,
    title = {Unbalanced GANs: Pre-training the Generator of Generative Adversarial Network using Variational Autoencoder},
    author = {Ham, Hyungrok and Jun, Tae Joon and Kim, Daeyoung},
    journal = {arXiv preprint arXiv:2002.02112},
    year = {2020}
}

@inproceedings{hou2017deep,
    title = {Deep feature consistent variational autoencoder},
    author = {Hou, Xianxu and Shen, Linlin and Sun, Ke and Qiu, Guoping},
    booktitle = {2017 IEEE Winter Conference on Applications of Computer Vision (WACV)},
    pages = {1133--1141},
    year = {2017},
    organization = {IEEE}
}

@article{desimone1995neural,
    author = {Desimone, Robert and Duncan, John},
    title = {Neural Mechanisms of Selective Visual Attention},
    journal = {Annual Review of Neuroscience},
    volume = {18},
    number = {1},
    pages = {193-222},
    year = {1995},
    doi = {10.1146/annurev.ne.18.030195.001205},
    note = {PMID: 7605061},
    URL = {https://doi.org/10.1146/annurev.ne.18.030195.001205},
    eprint = {https://doi.org/10.1146/annurev.ne.18.030195.001205}
}

@book{sutton2018reinforcement,
    title = {Reinforcement learning: An introduction},
    author = {Sutton, Richard S and Barto, Andrew G},
    year = {2018},
    publisher = {MIT press}
}

@book{trappenberg2010fundamentals,
    title = {Fundamentals of Computational Neuroscience},
    author = {Trappenberg, T.},
    isbn = {9780199568413},
    lccn = {2009043549},
    url = {https://books.google.de/books?id=1xSLktiRAX4C},
    year = {2010},
    publisher = {OUP Oxford}
}

@inproceedings{radford2016deep,
    author = {Alec Radford and
 Luke Metz and
 Soumith Chintala},
    editor = {Yoshua Bengio and
 Yann LeCun},
    title = {Unsupervised Representation Learning with Deep Convolutional Generative
 Adversarial Networks},
    booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
 San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
    year = {2016},
    url = {http://arxiv.org/abs/1511.06434},
    timestamp = {Thu, 25 Jul 2019 14:25:38 +0200},
    biburl = {https://dblp.org/rec/journals/corr/RadfordMC15.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{chen2018isolating,
    title = {Isolating Sources of Disentanglement in Variational Autoencoders},
    author = {Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger B and Duvenaud, David K},
    booktitle = {Advances in Neural Information Processing Systems 31},
    editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
    pages = {2610--2620},
    year = {2018},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders.pdf}
}

@InProceedings{springenberg2015striving,
    author = "J.T. Springenberg and A. Dosovitskiy and T. Brox and M. Riedmiller",
    title = "Striving for Simplicity: The All Convolutional Net",
    booktitle = "ICLR (workshop track)",
    year = "2015",
    url = "http://lmb.informatik.uni-freiburg.de/Publications/2015/DB15a"
}

@article{dumoulin2016guide,
    title = {A guide to convolution arithmetic for deep learning},
    author = {Dumoulin, Vincent and Visin, Francesco},
    journal = {arXiv preprint arXiv:1603.07285},
    year = {2016}
}

@INPROCEEDINGS{georganas2018anatomy,
    author = {E. {Georganas} and S. {Avancha} and K. {Banerjee} and D. {Kalamkar} and G. {Henry} and H. {Pabst} and A. {Heinecke}},
    booktitle = {SC18: International Conference for High Performance Computing, Networking, Storage and Analysis},
    title = {Anatomy of High-Performance Deep Learning Convolutions on SIMD Architectures},
    year = {2018},
    pages = {830-841}, }

@article{higgins2018towards,
    title = {Towards a definition of disentangled representations},
    author = {Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
    journal = {arXiv preprint arXiv:1812.02230},
    year = {2018}
}

@article{foldiak2003sparse,
    title = {Sparse coding in the primate cortex},
    author = {Foldiak, Peter},
    journal = {The handbook of brain theory and neural networks},
    year = {2003},
    publisher = {MIT Press}
}

@InProceedings{Szegedy_2015_CVPR,
    author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
    title = {Going Deeper With Convolutions},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2015}
}

@Article{Olshausen1996,
    author = {Olshausen, Bruno A. and Field, David J.},
    title = {Emergence of simple-cell receptive field properties by learning a sparse code for natural images},
    journal = {Nature},
    year = {1996},
    month = {Jun},
    day = {01},
    volume = {381},
    number = {6583},
    pages = {607-609},
    abstract = {THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1--4 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7--12. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13--18, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs.},
    issn = {1476-4687},
    doi = {10.1038/381607a0},
    url = {https://doi.org/10.1038/381607a0}
}

@inproceedings{kingma2014autoencoding,
    author = {Diederik P. Kingma and
 Max Welling},
    editor = {Yoshua Bengio and
 Yann LeCun},
    title = {Auto-Encoding Variational Bayes},
    booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
 Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
    year = {2014},
    url = {http://arxiv.org/abs/1312.6114},
    timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
    biburl = {https://dblp.org/rec/journals/corr/KingmaW13.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}
