@article{han2019variational,
    title = "Variational autoencoder: An unsupervised model for encoding and decoding fMRI activity in visual cortex",
    journal = "NeuroImage",
    volume = "198",
    pages = "125 - 136",
    year = "2019",
    issn = "1053-8119",
    doi = "https://doi.org/10.1016/j.neuroimage.2019.05.039",
    url = "http://www.sciencedirect.com/science/article/pii/S1053811919304318",
    author = "Kuan Han and Haiguang Wen and Junxing Shi and Kun-Han Lu and Yizhen Zhang and Di Fu and Zhongming Liu",
    keywords = "Neural coding, Variational autoencoder, Bayesian brain, Visual reconstruction",
    abstract = "Goal-driven and feedforward-only convolutional neural networks (CNN) have been shown to be able to predict and decode cortical responses to natural images or videos. Here, we explored an alternative deep neural network, variational auto-encoder (VAE), as a computational model of the visual cortex. We trained a VAE with a five-layer encoder and a five-layer decoder to learn visual representations from a diverse set of unlabeled images. Using the trained VAE, we predicted and decoded cortical activity observed with functional magnetic resonance imaging (fMRI) from three human subjects passively watching natural videos. Compared to CNN, VAE could predict the video-evoked cortical responses with comparable accuracy in early visual areas, but relatively lower accuracy in higher-order visual areas. The distinction between CNN and VAE in terms of encoding performance was primarily attributed to their different learning objectives, rather than their different model architecture or number of parameters. Despite lower encoding accuracies, VAE offered a more convenient strategy for decoding the fMRI activity to reconstruct the video input, by first converting the fMRI activity to the VAE's latent variables, and then converting the latent variables to the reconstructed video frames through the VAE's decoder. This strategy was more advantageous than alternative decoding methods, e.g. partial least squares regression, for being able to reconstruct both the spatial structure and color of the visual input. Such findings highlight VAE as an unsupervised model for learning visual representation, as well as its potential and limitations for explaining cortical responses and reconstructing naturalistic and diverse visual experiences."
}

@incollection{krizhevsky2012imagenet,
    title = {ImageNet Classification with Deep Convolutional Neural Networks},
    author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
    booktitle = {Advances in Neural Information Processing Systems 25},
    editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
    pages = {1097--1105},
    year = {2012},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@article{khaligh2014deep,
    author = {Khaligh-Razavi, Seyed-Mahdi AND Kriegeskorte, Nikolaus},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation},
    year = {2014},
    month = {11},
    volume = {10},
    url = {https://doi.org/10.1371/journal.pcbi.1003915},
    pages = {1-29},
    abstract = {Author Summary Computers cannot yet recognize objects as well as humans can. Computer vision might learn from biological vision. However, neuroscience has yet to explain how brains recognize objects and must draw from computer vision for initial computational models. To make progress with this chicken-and-egg problem, we compared 37 computational model representations to representations in biological brains. The more similar a model representation was to the high-level visual brain representation, the better the model performed at object categorization. Most models did not come close to explaining the brain representation, because they missed categorical distinctions between animates and inanimates and between faces and other objects, which are prominent in primate brains. A deep neural network model that was trained by supervision with over a million category-labeled images and represents the state of the art in computer vision came closest to explaining the brain representation. Our brains appear to impose upon the visual input certain categorical divisions that are important for successful behavior. Brains might learn these divisions through evolution and individual experience. Computer vision similarly requires learning with many labeled images so as to emphasize the right categorical divisions.},
    number = {11},
    doi = {10.1371/journal.pcbi.1003915}
}


@article{freedman2007statistics,
    title = {Statistics, 4th Edition International Student Edition},
    author = {Freedman, David and Pisani, Robert and Purves, Roger},
    journal = {Pisani, R. Purves, 4th edn. WW Norton \& Company, New York},
    year = {2007}
}

@Article{freeman2013functional,
    author = {Freeman, Jeremy and Ziemba, Corey M. and Heeger, David J. and Simoncelli, Eero P. and Movshon, J. Anthony},
    title = {A functional and perceptual signature of the second visual area in primates},
    journal = {Nature Neuroscience},
    year = {2013},
    month = {Jul},
    day = {01},
    volume = {16},
    number = {7},
    pages = {974-981},
    abstract = {The authors examined neuronal responses in V1 and V2 to synthetic texture stimuli that replicate higher-order statistical dependencies found in natural images. V2, but not V1, responded differentially to these textures, in both macaque (single neurons) and human (fMRI). Human detection of naturalistic structure in the same images was predicted by V2 responses, suggesting a role for V2 in representing natural image structure.},
    issn = {1546-1726},
    doi = {10.1038/nn.3402},
    url = {https://doi.org/10.1038/nn.3402}
}


@article{logothetis1995shape,
    title = "Shape representation in the inferior temporal cortex of monkeys",
    journal = "Current Biology",
    volume = "5",
    number = "5",
    pages = "552 - 563",
    year = "1995",
    issn = "0960-9822",
    doi = "https://doi.org/10.1016/S0960-9822(95)00108-4",
    url = "http://www.sciencedirect.com/science/article/pii/S0960982295001084",
    author = "Nikos K. Logothetis and Jon Pauls and Tomaso Poggio",
    abstract = "Background: The inferior temporal cortex (IT) of the monkey has long been known to play an essential role in visual object recognition. Damage to this area results in severe deficits in perceptual learning and object recognition, without significantly affecting basic visual capacities. Consistent with these ablation studies is the discovery of IT neurons that respond to complex two-dimensional visual patterns, or objects such as faces or body parts. What is the role of these neurons in object recognition? Is such a complex configurational selectivity specific to biologically meaningful objects, or does it develop as a result of extensive exposure to any objects whose identification relies on subtle shape differences? If so, would IT neurons respond selectively to recently learned views or features of novel objects? The present study addresses this question by using combined psychophysical and electrophysiological experiments, in which monkeys learned to classify and recognize computer-generated three-dimensional objects. Results A population of IT neurons was found that responded selectively to views of previously unfamiliar objects. The cells discharged maximally to one view of an object, and their response declined gradually as the object was rotated away from this preferred view. No selective responses were ever encountered for views that the animal systematically failed to recognize. Most neurons also exhibited orientation-dependent responses during view-plane rotations. Some neurons were found to be tuned around two views of the same object, and a very small number of cells responded in a view-invariant manner. For the five different objects that were used extensively during the training of the animals, and for which behavioral performance became view-independent, multiple cells were found that were tuned around different views of the same object. A number of view-selective units showed response invariance for changes in the size of the object or the position of its image within the parafovea. Conclusion Our results suggest that IT neurons can develop a complex receptive field organization as a consequence of extensive training in the discrimination and recognition of objects. None of these objects had any prior meaning for the animal, nor did they resemble anything familiar in the monkey's environment. Simple geometric features did not appear to account for the neurons' selective responses. These findings support the idea that a population of neurons — each tuned to a different object aspect, and each showing a certain degree of invariance to image transformations — may, as an ensemble, encode at least some types of complex three-dimensional objects. In such a system, several neurons may be active for any given vantage point, with a single unit acting like a blurred template for a limited neighborhood of a single view."
}


@article{roe2012toward,
    title = "Toward a Unified Theory of Visual Area V4",
    journal = "Neuron",
    volume = "74",
    number = "1",
    pages = "12 - 29",
    year = "2012",
    issn = "0896-6273",
    doi = "https://doi.org/10.1016/j.neuron.2012.03.011",
    url = "http://www.sciencedirect.com/science/article/pii/S0896627312002747",
    author = "Anna W. Roe and Leonardo Chelazzi and Charles E. Connor and Bevil R. Conway and Ichiro Fujita and Jack L. Gallant and Haidong Lu and Wim Vanduffel",
    abstract = "Visual area V4 is a midtier cortical area in the ventral visual pathway. It is crucial for visual object recognition and has been a focus of many studies on visual attention. However, there is no unifying view of V4's role in visual processing. Neither is there an understanding of how its role in feature processing interfaces with its role in visual attention. This review captures our current knowledge of V4, largely derived from electrophysiological and imaging studies in the macaque monkey. Based on recent discovery of functionally specific domains in V4, we propose that the unifying function of V4 circuitry is to enable selective extraction of specific functional domain-based networks, whether it be by bottom-up specification of object features or by top-down attentionally driven selection."
}


@inproceedings{liu2015faceattributes,
    author = {Z. {Liu} and P. {Luo} and X. {Wang} and X. {Tang}},
    booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
    title = {Deep Learning Face Attributes in the Wild},
    year = {2015},
    pages = {3730-3738}, }


@inproceedings{imagenet_cvpr09,
    author = {J. {Deng} and W. {Dong} and R. {Socher} and L. {Li} and {Kai Li} and {Li Fei-Fei}},
    booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
    title = {ImageNet: A large-scale hierarchical image database},
    year = {2009},
    pages = {248-255}, }


@Article{ILSVRC15,
    author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
    title = {ImageNet Large Scale Visual Recognition Challenge},
    journal = {International Journal of Computer Vision},
    year = {2015},
    month = {Dec},
    day = {01},
    volume = {115},
    number = {3},
    pages = {211-252},
    abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
    issn = {1573-1405},
    doi = {10.1007/s11263-015-0816-y},
    url = {https://doi.org/10.1007/s11263-015-0816-y}
}


% @formatter:off
@book{squire2012fundamental,
    title = {Fundamental {N}euroscience},
    author = {Squire, Larry and Berg, Darwin and Bloom, Floyd E and Du Lac, Sascha and Ghosh, Anirvan and Spitzer, Nicholas C},
    year = {2012},
    publisher = {Academic Press}
}
% @formatter:on


@article{rubens1971associative,
    author = {Rubens, Alan B. and Benson, D. Frank},
    title = {Associative Visual Agnosia},
    journal = {Archives of Neurology},
    volume = {24},
    number = {4},
    pages = {305-316},
    year = {1971},
    month = {04},
    abstract = {Following an acute hypotensive episode, a patient had unusual residual visual difficulties. A right homonymous hemianopsia was the only primary neurologic deficit, but there were specific behavioral disturbances. He could not read but could write (alexia without agraphia), had difficulty in color recognition (color agnosia), facial recognition (prosopagnosia), and object recognition (visual agnosia). Despite these visual disturbances, adequate primary visual function was demonstrated by his ability to copy drawings. Clinical evidence suggests that the underlying lesion was infarction involving the left medial occipital region and the splenium of the corpus callosum. The possibility of a second lesion in the right hemisphere can be conjectured but not supported by available data. Inability to identify or recognize visually presented objects with preservation of ability to draw them is unusual and can best be termed associative visual agnosia.Visual agnosia; prosopagnosia; color agnosia; alexia without agraphia; corpus callosum.},
    issn = {0003-9942},
    doi = {10.1001/archneur.1971.00480340037003},
    url = {https://doi.org/10.1001/archneur.1971.00480340037003},
    eprint = {https://jamanetwork.com/journals/jamaneurology/articlepdf/570416/archneur\_24\_4\_003.pdf},
}




@book{Goodfellow-et-al-2016,
    title = {Deep Learning},
    author = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher = {MIT Press},
    url = {http://www.deeplearningbook.org},
    year = {2016}
}

@inproceedings{chicco2014deep,
    author = {Chicco, Davide and Sadowski, Peter and Baldi, Pierre},
    title = {Deep Autoencoder Neural Networks for Gene Ontology Annotation Predictions},
    year = {2014},
    isbn = {9781450328944},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2649387.2649442},
    doi = {10.1145/2649387.2649442},
    booktitle = {Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics},
    pages = {533–540},
    numpages = {8},
    keywords = {biomolecular annotations, matrix-completion, gene ontology, truncated singular value decomposition, neural networks, autoencoders, principal component analysis},
    location = {Newport Beach, California},
    series = {BCB ’14}
}

@article{kingma2019introduction,
    url = {http://dx.doi.org/10.1561/2200000056},
    year = {2019},
    volume = {12},
    journal = {Foundations and Trends® in Machine Learning},
    title = {An Introduction to Variational Autoencoders},
    doi = {10.1561/2200000056},
    issn = {1935-8237},
    number = {4},
    pages = {307-392},
    author = {Diederik P. Kingma and Max Welling}
}


@article{palm2012prediction,
    author = "R. B. Palm",
    title = "Prediction as a candidate for learning deep hierarchical models of data",
    year = "2012",
    school = "Technical University of Denmark, {DTU} Informatics, {E-}mail: reception@imm.dtu.dk",
    address = "Asmussens Alle, Building 305, {DK-}2800 Kgs. Lyngby, Denmark",
    note = "Supervised by Associate Professor Ole Winther, owi@imm.dtu.dk, {DTU} Informatics, and Morten M{\o}rup, mm@imm.dtu.dk, {DTU} Informatics",
    url = "http://www.imm.dtu.dk/English.aspx",
    abstract = "Recent findings [HOT06] have made possible the learning of deep layered hierarchical representations of data mimicking the brains working. It is hoped that this paradigm will unlock some of the power of the brain and lead to advances towards true {AI}.
In this thesis I implement and evaluate state-of-the-art deep learning models and using these as building blocks I investigate the hypothesis that predicting the time-to-time sensory input is a good learning objective. I introduce the Predictive Encoder (PE) and show that a simple non-regularized learning rule, minimizing prediction error on natural video patches leads to receptive fields similar to those found in Macaque monkey visual area V1. I scale this model to video of natural scenes by introducing the Convolutional Predictive Encoder (CPE) and show similar results. Both models can be used in deep architectures as a deep learning module."
}




@InProceedings{walker2016uncertain,
    author = "Walker, Jacob and Doersch, Carl and Gupta, Abhinav and Hebert, Martial",
    editor = "Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max",
    title = "An Uncertain Future: Forecasting from Static Images Using Variational Autoencoders",
    booktitle = "Computer Vision -- ECCV 2016",
    year = "2016",
    publisher = "Springer International Publishing",
    address = "Cham",
    pages = "835--851",
    abstract = "In a given scene, humans can easily predict a set of immediate future events that might happen. However, pixel-level anticipation in computer vision is difficult because machine learning struggles with the ambiguity in predicting the future. In this paper, we focus on predicting the dense trajectory of pixels in a scene---what will move in the scene, where it will travel, and how it will deform over the course of one second. We propose a conditional variational autoencoder as a solution to this problem. In this framework, direct inference from the image shapes the distribution of possible trajectories while latent variables encode information that is not available in the image. We show that our method predicts events in a variety of scenes and can produce multiple different predictions for an ambiguous future. We also find that our method learns a representation that is applicable to semantic vision tasks.",
    isbn = "978-3-319-46478-7"
}


@article{cadieu2014deep,
    author = {Cadieu, Charles F. AND Hong, Ha AND Yamins, Daniel L. K. AND Pinto, Nicolas AND Ardila, Diego AND Solomon, Ethan A. AND Majaj, Najib J. AND DiCarlo, James J.},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition},
    year = {2014},
    month = {12},
    volume = {10},
    url = {https://doi.org/10.1371/journal.pcbi.1003963},
    pages = {1-18},
    abstract = {Author Summary Primates are remarkable at determining the category of a visually presented object even in brief presentations, and under changes to object exemplar, position, pose, scale, and background. To date, this behavior has been unmatched by artificial computational systems. However, the field of machine learning has made great strides in producing artificial deep neural network systems that perform highly on object recognition benchmarks. In this study, we measured the responses of neural populations in inferior temporal (IT) cortex across thousands of images and compared the performance of neural features to features derived from the latest deep neural networks. Remarkably, we found that the latest artificial deep neural networks achieve performance equal to the performance of IT cortex. Both deep neural networks and IT cortex create representational spaces in which images with objects of the same category are close, and images with objects of different categories are far apart, even in the presence of large variations in object exemplar, position, pose, scale, and background. Furthermore, we show that the top-level features in these models exceed previous models in predicting the IT neural responses themselves. This result indicates that the latest deep neural networks may provide insight into understanding primate visual processing.},
    number = {12},
    doi = {10.1371/journal.pcbi.1003963}
}

@conference{tschannen2018recent,
    author = {Tschannen, Michael and Bachem, Olivier and Lucic, Mario},
    title = {Recent advances in autoencoder-based representation learning},
    booktitle = {Third workshop on Bayesian Deep Learning (NeurIPS 2018)},
    year = 2018,
    keywords = {autoencoder, variational autoencoder, representation learning, disentanglement, rate-distortion tradeoff, unsupervised},
    url = {http://www.nari.ee.ethz.ch/pubs/p/autoenc2018}
}

% CONTINUE HERE

@article{eickenberg2017seeing,
    title = {Seeing it all: Convolutional network layers map the function of the human visual system},
    author = {Eickenberg, Michael and Gramfort, Alexandre and Varoquaux, Ga {\"e} l and Thirion, Bertrand},
    journal = {NeuroImage},
    volume = {152},
    pages = {184--194},
    year = {2017},
    publisher = {Elsevier}
}

@article{wen2018neural,
    title = {Neural encoding and decoding with deep learning for dynamic natural vision},
    author = {Wen, Haiguang and Shi, Junxing and Zhang, Yizhen and Lu, Kun-Han and Cao, Jiayue and Liu, Zhongming},
    journal = {Cerebral Cortex},
    volume = {28},
    number = {12},
    pages = {4136--4160},
    year = {2018},
    publisher = {Oxford University Press}
}

@article{mockus1978application,
    title = {The application of Bayesian methods for seeking the extremum},
    author = {Mockus, Jonas and Tiesis, Vytautas and Zilinskas, Antanas},
    journal = {Towards global optimization},
    volume = {2},
    number = {117-129},
    pages = {2},
    year = {1978}
}

@incollection{goodfellow2014gans,
    title = {Generative Adversarial Nets},
    author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
    booktitle = {Advances in Neural Information Processing Systems 27},
    editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
    pages = {2672--2680},
    year = {2014},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf}
}

@article{brock2018large,
    title = {Large scale gan training for high fidelity natural image synthesis},
    author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
    journal = {arXiv preprint arXiv:1809.11096},
    year = {2018}
}

@inproceedings{chen2016infogan,
    title = {Infogan: Interpretable representation learning by information maximizing generative adversarial nets},
    author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
    booktitle = {Advances in neural information processing systems},
    pages = {2172--2180},
    year = {2016}
}

@article{bengio2013representation,
    title = {Representation learning: A review and new perspectives},
    author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
    journal = {IEEE transactions on pattern analysis and machine intelligence},
    volume = {35},
    number = {8},
    pages = {1798--1828},
    year = {2013},
    publisher = {IEEE}
}

% @formatter:off
@inproceedings{sonderby2016ladder,
    title = {Ladder variational autoencoders},
    author = {Sønderby, Casper Kaae and Raiko, Tapani and Maaløe, Lars and Sønderby, Søren Kaae and Winther, Ole},
    booktitle = {Advances in neural information processing systems},
    pages = {3738--3746},
    year = {2016}
}
% @formatter:on

@inproceedings{zhao2017learning,
    title = {Learning hierarchical features from deep generative models},
    author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
    booktitle = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
    pages = {4091--4099},
    year = {2017},
    organization = {JMLR. org}
}

@phdthesis{van2016auto,
    title = {What auto-encoders could learn from brains},
    author = {van den Broeke, Gerben},
    year = {2016},
    school = {Master’s thesis, Aalto University, Finland}
}

@article{goodale1992separate,
    title = {Separate visual pathways for perception and action},
    author = {Goodale, Melvyn A and Milner, A David and others},
    year = {1992}
}

@book{mack2013principles,
    title = {Principles of Neural Science, Fifth Edition},
    author = {Mack, S. and Kandel, E.R. and Jessell, T.M. and Schwartz, J.H. and Siegelbaum, S.A. and Hudspeth, A.J.},
    isbn = {9780071390118},
    lccn = {2012023071},
    series = {Principles of Neural Science},
    url = {https://books.google.de/books?id=s64z-LdAIsEC},
    year = {2013},
    publisher = {McGraw-Hill Education}
}

@article{castro2019morpho,
    title = {Morpho-Mnist: Quantitative assessment and diagnostics for representation learning},
    author = {Castro, D and Tan, Jeremy and Kainz, Bernhard and Konukoglu, Ender and Glocker, Ben},
    journal = {Journal of Machine Learning Research},
    volume = {20},
    year = {2019}
}

@article{lecun1998gradient,
    title = {Gradient-based learning applied to document recognition},
    author = {LeCun, Yann and Bottou, L {\'e} on and Bengio, Yoshua and Haffner, Patrick},
    journal = {Proceedings of the IEEE},
    volume = {86},
    number = {11},
    pages = {2278--2324},
    year = {1998},
    publisher = {Ieee}
}

@article{pidhorskyi2020adversarial,
    title = {Adversarial Latent Autoencoders},
    author = {Pidhorskyi, Stanislav and Adjeroh, Donald and Doretto, Gianfranco},
    journal = {arXiv preprint arXiv:2004.04467},
    year = {2020}
}

@inproceedings{karras2019style,
    title = {A style-based generator architecture for generative adversarial networks},
    author = {Karras, Tero and Laine, Samuli and Aila, Timo},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
    pages = {4401--4410},
    year = {2019}
}

@inproceedings{johnson2016perceptual,
    title = {Perceptual losses for real-time style transfer and super-resolution},
    author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
    booktitle = {European conference on computer vision},
    pages = {694--711},
    year = {2016},
    organization = {Springer}
}

@article{kim2018disentangling,
    title = {Disentangling by factorising},
    author = {Kim, Hyunjik and Mnih, Andriy},
    journal = {arXiv preprint arXiv:1802.05983},
    year = {2018}
}

@article{higgins2017beta,
    title = {beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework.},
    author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
    journal = {Iclr},
    volume = {2},
    number = {5},
    pages = {6},
    year = {2017}
}

@article{larsen2015autoencoding,
    title = {Autoencoding beyond pixels using a learned similarity metric},
    author = {Larsen, Anders Boesen Lindbo and S {\o} nderby, S {\o} ren Kaae and Larochelle, Hugo and Winther, Ole},
    journal = {arXiv preprint arXiv:1512.09300},
    year = {2015}
}

@inproceedings{shao2018riemannian,
    title = {The riemannian geometry of deep generative models},
    author = {Shao, Hang and Kumar, Abhishek and Thomas Fletcher, P},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
    pages = {315--323},
    year = {2018}
}

@article{arvanitidis2017latent,
    title = {Latent space oddity: on the curvature of deep generative models},
    author = {Arvanitidis, Georgios and Hansen, Lars Kai and Hauberg, S {\o} ren},
    journal = {arXiv preprint arXiv:1710.11379},
    year = {2017}
}

@book{bishop2006pattern,
    title = {Pattern recognition and machine learning},
    author = {Bishop, Christopher M},
    year = {2006},
    publisher = {springer}
}

@inproceedings{
Shu2020Weakly,
    title = {Weakly Supervised Disentanglement with Guarantees},
    author = {Rui Shu and Yining Chen and Abhishek Kumar and Stefano Ermon and Ben Poole},
    booktitle = {International Conference on Learning Representations},
    year = {2020},
    url = {https://openreview.net/forum?id=HJgSwyBKvr}
}

@book{burger2009principles,
    author = {Burger, Wilhelm and Burge, Mark J.},
    title = {Principles of Digital Image Processing: Core Algorithms},
    year = {2009},
    isbn = {1848001940},
    publisher = {Springer Publishing Company, Incorporated},
    edition = {1}
}

@misc{dsprites17,
    author = {Loic Matthey and Irina Higgins and Demis Hassabis and Alexander Lerchner},
    title = {dSprites: Disentanglement testing Sprites dataset},
    howpublished = {https://github.com/deepmind/dsprites-dataset/},
    year = "2017",
}

@article{vanrullen2019reconstructing,
    title = {Reconstructing faces from fMRI patterns using deep generative neural networks},
    author = {VanRullen, Rufin and Reddy, Leila},
    journal = {Communications biology},
    volume = {2},
    number = {1},
    pages = {1--10},
    year = {2019},
    publisher = {Nature Publishing Group}
}
@article{yoshida2020natural,
    title = {Natural images are reliably represented by sparse and variable populations of neurons in visual cortex},
    author = {Yoshida, Takashi and Ohki, Kenichi},
    journal = {Nature communications},
    volume = {11},
    number = {1},
    pages = {1--19},
    year = {2020},
    publisher = {Nature Publishing Group}
}

@article{jones1987evaluation,
    title = {An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex},
    author = {Jones, Judson P and Palmer, Larry A},
    journal = {Journal of neurophysiology},
    volume = {58},
    number = {6},
    pages = {1233--1258},
    year = {1987}
}

@article{hubel1962receptive,
    title = {Receptive fields, binocular interaction and functional architecture in the cat's visual cortex},
    author = {Hubel, David H and Wiesel, Torsten N},
    journal = {The Journal of physiology},
    volume = {160},
    number = {1},
    pages = {106--154},
    year = {1962},
    publisher = {Wiley Online Library}
}

@article{lindsay2020convolutional,
    title = {Convolutional neural networks as a model of the visual system: past, present, and future},
    author = {Lindsay, Grace},
    journal = {Journal of Cognitive Neuroscience},
    pages = {1--15},
    year = {2020},
    publisher = {MIT Press}
}

@article{fukushima1980neocognitron,
    title = {Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
    author = {Fukushima, Kunihiko},
    journal = {Biological cybernetics},
    volume = {36},
    number = {4},
    pages = {193--202},
    year = {1980},
    publisher = {Springer}
}

@inproceedings{liu2015hmax,
    title = {HMAX model: A survey},
    author = {Liu, Chang and Sun, Fuchun},
    booktitle = {2015 International Joint Conference on Neural Networks (IJCNN)},
    pages = {1--7},
    year = {2015},
    organization = {IEEE}
}

@article{riesenhuber1999hierarchical,
    title = {Hierarchical models of object recognition in cortex},
    author = {Riesenhuber, Maximilian and Poggio, Tomaso},
    journal = {Nature neuroscience},
    volume = {2},
    number = {11},
    pages = {1019--1025},
    year = {1999},
    publisher = {Nature Publishing Group}
}

@article{lecun1989backpropagation,
    title = {Backpropagation applied to handwritten zip code recognition},
    author = {LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
    journal = {Neural computation},
    volume = {1},
    number = {4},
    pages = {541--551},
    year = {1989},
    publisher = {MIT Press}
}

@article{frankle2018lottery,
    title = {The lottery ticket hypothesis: Finding sparse, trainable neural networks},
    author = {Frankle, Jonathan and Carbin, Michael},
    journal = {arXiv preprint arXiv:1803.03635},
    year = {2018}
}

@article{lucas2019understanding,
    title = {Understanding posterior collapse in generative latent variable models},
    author = {Lucas, James and Tucker, George and Grosse, Roger and Norouzi, Mohammad},
    year = {2019}
}

@article{zhao2017towards,
    title = {Towards deeper understanding of variational autoencoding models},
    author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
    journal = {arXiv preprint arXiv:1702.08658},
    year = {2017}
}

% @formatter:off
@article{rodriguez2015hierarchical,
    title = {Hierarchical object representations in the visual cortex and computer vision},
    author = {Rodr{\'\i}guez-S{\'a}nchez, Antonio J and Fallah, Mazyar and Leonardis, Ale {\v {s}}},
    journal = {Frontiers in computational neuroscience},
    volume = {9},
    pages = {142},
    year = {2015},
    publisher = {Frontiers}
}
% @formatter:on

@article{mikolov2013efficient,
    title = {Efficient estimation of word representations in vector space},
    author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
    journal = {arXiv preprint arXiv:1301.3781},
    year = {2013}
}

@article{berkes2005slow,
    title = {Slow feature analysis yields a rich repertoire of complex cell properties},
    author = {Berkes, Pietro and Wiskott, Laurenz},
    journal = {Journal of vision},
    volume = {5},
    number = {6},
    pages = {9--9},
    year = {2005},
    publisher = {The Association for Research in Vision and Ophthalmology}
}

@article{burgess2018understanding,
    title = {Understanding disentangling in $\beta$-VAE},
    author = {Burgess, Christopher P and Higgins, Irina and Pal, Arka and Matthey, Loic and Watters, Nick and Desjardins, Guillaume and Lerchner, Alexander},
    journal = {arXiv preprint arXiv:1804.03599},
    year = {2018}
}

@inproceedings{ruan2016exploring,
    title = {Exploring semantic representation in brain activity using word embeddings},
    author = {Ruan, Yu-Ping and Ling, Zhen-Hua and Hu, Yu},
    booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
    pages = {669--679},
    year = {2016}
}

@inproceedings{anderson2013words,
    title = {Of words, eyes and brains: Correlating image-based distributional semantic models with neural representations of concepts},
    author = {Anderson, Andrew J and Bruni, Elia and Bordignon, Ulisse and Poesio, Massimo and Baroni, Marco},
    booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
    pages = {1960--1970},
    year = {2013}
}

@article{fairhall2013brain,
    title = {Brain regions that represent amodal conceptual knowledge},
    author = {Fairhall, Scott L and Caramazza, Alfonso},
    journal = {Journal of Neuroscience},
    volume = {33},
    number = {25},
    pages = {10552--10558},
    year = {2013},
    publisher = {Soc Neuroscience}
}

@article{che2016mode,
    title = {Mode regularized generative adversarial networks},
    author = {Che, Tong and Li, Yanran and Jacob, Athul Paul and Bengio, Yoshua and Li, Wenjie},
    journal = {arXiv preprint arXiv:1612.02136},
    year = {2016}
}

@article{ham2020unbalanced,
    title = {Unbalanced GANs: Pre-training the Generator of Generative Adversarial Network using Variational Autoencoder},
    author = {Ham, Hyungrok and Jun, Tae Joon and Kim, Daeyoung},
    journal = {arXiv preprint arXiv:2002.02112},
    year = {2020}
}

@inproceedings{hou2017deep,
    title = {Deep feature consistent variational autoencoder},
    author = {Hou, Xianxu and Shen, Linlin and Sun, Ke and Qiu, Guoping},
    booktitle = {2017 IEEE Winter Conference on Applications of Computer Vision (WACV)},
    pages = {1133--1141},
    year = {2017},
    organization = {IEEE}
}