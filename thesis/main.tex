\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{parskip}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{acronym}
\usepackage[natbib=true]{biblatex}
\usepackage{amsmath}
\usepackage{dirtytalk}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{pgffor}
\usepackage{algorithm}
\usepackage{algpseudocode}
% bold math symbols
\usepackage{bm}
\usepackage{amssymb}
\addbibresource{literature.bib}
\renewcommand{\baselinestretch}{1.5}
\newcounter{savepage}

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
    #1\;\delimsize\|\;#2%
}
\newcommand{\kldiv}{D_{KL}\infdivx}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\makeatletter
\newenvironment{breakablealgorithm}
{% \begin{breakablealgorithm}
    \begin{center}
        \refstepcounter{algorithm}% New algorithm
        \vspace{1em}
        \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
        \renewcommand{\caption}[2][\relax]{% Make a new \caption
            {\raggedright\textbf{\ALG@name~\thealgorithm} ##2\par}%
            \ifx\relax##1\relax % #1 is \relax
            \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
            \else % #1 is not \relax
            \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
            \fi
            \kern2pt\hrule\kern4pt
        }
}{% \end{breakablealgorithm}
    \kern2pt\hrule\relax% \@fs@post for \@fs@ruled
    \vspace{1em}
\end{center}
}
\makeatother

\begin{document}

\begin{titlepage}
\centering
\includegraphics[width=0.25\textwidth]{rublogo.png}\par
{\scshape\huge\bfseries Semantic Representations in Variational Autoencoders as a Model of the Visual System \par}
{\scshape\large Schriftliche Pr체fungsarbeit f체r die Master-Pr체fung des Studiengangs Angewandte Informatik an der Ruhr-Universit채t Bochum\par}
\vspace{1em}
vorgelegt von\par
\vspace{2em}
Leonard Papenmeier\par 108017257755\par
\vspace{2em}
01.01.1980\par

\vfill
Prof. Dr. Laurenz Wiskott\par
M.Sc. Zahra Fayyaz


\end{titlepage}
\pagenumbering{Roman}
\tableofcontents
\newpage
    \setcounter{savepage}{\arabic{page}}
    \pagenumbering{arabic}


    \section{Introduction}\label{sec:introduction}


    \section{Theoretical Background}\label{sec:theoretical-background}
    \include{background}


    \section{Methods}\label{sec:methods}
\input{methods}


\section{Results}\label{sec:results}
\input{results}

\section{Discussion}\label{sec:discussion}

\subsection{Feedback Connections of the \acl{LGN}}\label{subsec:feedback-connections-of-the-lateral-geniculate-nucleus}

    As discussed in Section~\ref{subsubsec:visual-cortex}, the \ac{LGN} receives strong feedback connections from the \ac{V1}.
    Consider the \ac{LVAE}-model in Section~\ref{subsubsec:representation_learning} and the discussion concerning the top-down pass.
    It is stated that, compared to the Vanilla VAE, the \ac{LVAE} model is more biologically plausible because top-down connections resemble the feedback connections in the ventral and primary visual pathway.
    \citet{sonderby2016ladder} claim that the top-down pass helps improving the low-level feature representation because it enables the model to incorporate the higher-level context.

    Maybe the feedback connections fulfil this very function, i.e.\ allowing visual regions operating on a lower semantic level to incorporate the high-level context to disambiguate the lower-level representations.

    \subsection{Criticism on \citet{zhao2017learning}}

    \citet{zhao2017learning} compared their model to a HVAE and showed the difference on a plot.
    However, the plots are not really comparable.
    \textbf{CONTINUE}

    \subsection{Visual Features in Variational Autoencoders}\label{subsec:discussion_visual_features_in_variational_autoencoders}
    \begin{itemize}
        \item Reconstructions of AlexNet-VAE don't look natural at all, this might be because ImageNet, unlike CelebA, is not properly aligned and contains images from a large variety of classes.
        This might be another hint towards the assumption that \acp{VAE} are no suitable model for semantic representations
        \item How does it come that supervised models seem to be a good representation of \ac{IT} cortical representation~\citep{khaligh2014deep}? Maybe the brain learns in a \say{supervised} manner in the sense that if you have never seen a horse, you need someone to tell you that that is a horse in order to make sense out of it.
        However, you might, at least, be able to recognize that it is an animal.
        Also, if someone tells you \say{This is a horse.}, you'll be able to recognize horses from different orientations and in different body positions without further training.
This is quite different from how \acp{CNN} capture animals and is a strong hint to that supervised \acp{CNN} alone a no sufficient model for cortical IT representations.
\end{itemize}


\section{Conclusion}\label{sec:conclusion}

\newpage
\printbibliography

\newpage
\appendix
\section{Additional Plots for Section~\ref{subsec:independence-of-vlae-embeddings}}\label{sec:additional-plots-for-section_independence}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/appendix_plots/notprop_1_3.png}
\caption{Proportionality of pixel intensities when fixing $\bm{z}_1 = \bm{z}_3=\varphi$. Created accordingly to Figure~\ref{fig:notprop}.}
\label{fig:notprop_1_3}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/appendix_plots/notprop_2_3.png}
\caption{Proportionality of pixel intensities when fixing $\bm{z}_2 = \bm{z}_3=\varphi$. Created accordingly to Figure~\ref{fig:notprop}.}
\label{fig:notprop_2_3}
\end{figure}


\newpage
\pagenumbering{Roman}
\setcounter{page}{\thesavepage}
\section*{Acronyms}
\begin{acronym}[TDMA]
\acro{VAE}{Variational Autoencoder}
\acro{CNN}{Convolutional Neural Network}
\acro{IT}{Inferior Temporal Cortex}
\acro{ReLU}{Rectified Linear Unit}
\acro{LeakyReLU}{Leaky Rectified Linear Unit}
\acro{ILSVRC2017}{Large Scale Visual Recognition Challenge 2017}
\acro{MSE}{Mean Squared Error}
\acro{KL-divergence}{Kullback-Leibler divergence}
        \acro{ELBO}{Evidence Lower Bound}
        \acro{PDF}{Probability Density Function}
        \acro{GAN}{Generative Adversarial Network}
        \acro{VLAE}{Variational Ladder Autoencoder}
        \acro{LVAE}{Ladder Variational Autoencoder}
        \acro{CNS}{Central Nervous System}
        \acro{V1}{Primary Visual Cortex}
        \acro{LGN}{Lateral Geniculate Nucleus}
        \acro{TEO}{Temporo-Occipital Area}
    \end{acronym}
    \newpage
    \listoffigures
    \newpage
    \listoftables
    \newpage
    \include{erklaerung}


\end{document}
