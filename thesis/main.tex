\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{parskip}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{acronym}
\usepackage[natbib=true]{biblatex}
\usepackage{amsmath}
\usepackage{dirtytalk}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{pgffor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{lscape}
\usetikzlibrary{shapes,positioning,decorations.pathreplacing}
\addbibresource{literature.bib}
\renewcommand{\baselinestretch}{1.5}
\let\oldsection\section
\renewcommand


\section{\clearpage\oldsection}
\newcounter{savepage}

\lstset{lineskip=-0.7ex,frame=single,breaklines=true}

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
#1\;\delimsize\|\;#2%
}
\newcommand{\kldiv}{D_{KL}\infdivx}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\makeatletter
\newenvironment{breakablealgorithm}
{% \begin{breakablealgorithm}
\begin{center}
\refstepcounter{algorithm}% New algorithm
    \vspace{1em}
    \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
    \renewcommand{\caption}[2][\relax]{% Make a new \caption
    {\raggedright\textbf{\ALG@name~\thealgorithm} ##2\par}%
    \ifx\relax##1\relax % #1 is \relax
    \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
    \else % #1 is not \relax
    \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
    \fi
    \kern2pt\hrule\kern4pt
    }
}{% \end{breakablealgorithm}
\kern2pt\hrule\relax% \@fs@post for \@fs@ruled
\vspace{1em}
\end{center}
}
\makeatother

\begin{document}

\begin{titlepage}
\centering
\includegraphics[width=0.25\textwidth]{rublogo.png}\par
{\scshape\huge\bfseries Semantic Representations in Variational Autoencoders as a Model of the Visual System \par}
{\scshape\large Schriftliche Prüfungsarbeit für die Master-Prüfung des Studiengangs Angewandte Informatik an der Ruhr-Universität Bochum\par}
\vspace{1em}
vorgelegt von\par
\vspace{2em}
Leonard Papenmeier\par 108017257755\par
\vspace{2em}
01.01.1980\par

\vfill
Prof. Dr. Laurenz Wiskott\par
M.Sc. Zahra Fayyaz


\end{titlepage}
\pagenumbering{Roman}
\tableofcontents
\newpage
\setcounter{savepage}{\arabic{page}}
\pagenumbering{arabic}


\section{Introduction}\label{sec:introduction}

Research in artificial neural networks has risen in recent years due to their success in a large variety of different tasks.
This increase in research has led to progressively better network architectures, achieving better and better results.
The improvement of network architectures, however, is mainly driven by the question whether a new improvement will lead to better results \citep{lindsay2020convolutional}.

Biological plausibility, an important consideration in the field of computational neuroscience, often is of no interest to researchers aiming at solving a complex problem.
Even though progress in neural network research has become more and more disconnected from the biological example, they are inspired by biology.
Modern neural networks operating on images (mainly \acp{CNN}) indeed share many features with the visual system.

Even though neural networks are trained in a manner largely detached from the way the brain learns and builds memories, it has been shown that image classification networks are related to the visual system, not only areas where this relatedness has been built into the model.

Both, the biological foundation of \acp{CNN} and the more recent insights on their more indirect relatedness to the visual system qualify them as a potentially good model of the visual system.

Unfortunately, the neural networks where this kind of relatedness has been discovered so far are trained in a supervised manner, requiring lots of labeled data.
This kind of learning is disconnected from human perception where often one single example is sufficient when it comes to grouping objects into classes.

The \ac{VAE} is one important representative of a class of models \textit{generative models}.
This class of models allows a different training procedure, still requiring many samples but no labels.
Furthermore, \acp{VAE} build generalizable latent representations of inputs which might be similar to the abstract representation of the brain when perceiving the world.

\acp{VAE}, therefore, could be an important step towards a more realistic model of the visual system.

This thesis aims at answering the question whether \acp{VAE} prove to be a successful model of the visual system.

\section{Theoretical Background}\label{sec:theoretical-background}
\input{background}


\section{Methods}\label{sec:methods}
\input{methods}


\section{Results and Discussion}\label{sec:results}
\input{results}

\section{Discussion and Future Work}\label{sec:discussion}

\subsection{Possible applications of VLAE-GAN model}
\citet{vanrullen2019reconstructing} learn a mapping between test subject's fMRI responses when shown images from the CelebA dataset and the latent space of a \ac{VAE}.
They discuss which brain regions have most influence in the mapping, showing that the occipital lobe contributes most and the temporal and frontoparietal Cortex less.

The \ac{VLAE}(-\ac{GAN}) used in the course of this thesis uses three latent spaces compared to one latent space for a regular \ac{VAE}(-\ac{GAN}).
Furthermore it is designed such that the latent spaces encode features of different granularities.

\textbf{DRAFT FROM HERE:}
This model allows to learn three mappings from different lobes to different latent spaces of the VLAE-GAN decoder.
Maybe this is a better model and allows deeper analyses compared to the normal VAE decoder.


\subsection{Disentangled Representations of VLAE}
\textbf{Discuss the result from section \ref{subsec:morpho-mnist-on-vlae}. For example, digit identity is encoded in} $\bm{z}_2$ and $\bm{z}_3$ \textbf{. That is another indicator to why VLAE does not learn fully independent representation (Ref \ref{subsec:independence-of-vlae-embeddings})}

\subsection{Feedback Connections of the \acl{LGN}}\label{subsec:feedback-connections-of-the-lateral-geniculate-nucleus}
\textbf{DRAFT!}
As discussed in Section~\ref{subsubsec:visual-cortex}, the \ac{LGN} receives strong feedback connections from the \ac{V1}.
Consider the \ac{LVAE}-model in Section~\ref{subsubsec:representation_learning} and the discussion concerning the top-down pass.
It is stated that, compared to the Vanilla VAE, the \ac{LVAE} model is more biologically plausible because top-down connections resemble the feedback connections in the ventral and primary visual pathway.
\citet{sonderby2016ladder} claim that the top-down pass helps improving the low-level feature representation because it enables the model to incorporate the higher-level context.

Maybe the feedback connections fulfil this very function, i.e.\ allowing visual regions operating on a lower semantic level to incorporate the high-level context to disambiguate the lower-level representations.

\subsection{Criticism on \citet{zhao2017learning}}\label{subseq:criticism_vlae}
\textbf{DRAFT!}
\citet{zhao2017learning} compared their model to a HVAE and showed the difference on a plot.
However, the plots are not really comparable.


\subsection{Visual Features in Variational Autoencoders}\label{subsec:discussion_visual_features_in_variational_autoencoders}
\textbf{DRAFT!}
\begin{itemize}
\item Reconstructions of AlexNet-VAE don't look natural at all, this might be because ImageNet, unlike CelebA, is not properly aligned and contains images from a large variety of classes.
This might be another hint towards the assumption that \acp{VAE} are no suitable model for semantic representations
\item How does it come that supervised models seem to be a good representation of \ac{IT} cortical representation~\citep{khaligh2014deep}? Maybe the brain learns in a \say{supervised} manner in the sense that if you have never seen a horse, you need someone to tell you that that is a horse in order to make sense out of it.
However, you might, at least, be able to recognize that it is an animal.
Also, if someone tells you \say{This is a horse.}, you'll be able to recognize horses from different orientations and in different body positions without further training.
This is quite different from how \acp{CNN} capture animals and is a strong hint to that supervised \acp{CNN} alone a no sufficient model for cortical IT representations.
\end{itemize}

\subsection{MNIST vs. VAE and VLAE Embeddings}
\textbf{DRAFT!}
\textbf{Discuss in how far the models are comparable because they have different architectures.}

\subsection{Comparison to \ac{IT}}
\textbf{DRAFT!}
Talk about how to compare network activites to IT activities.

\subsection{Pixelwise vs. Generative Loss}
This thesis employed two different methods to train the decoder towards generating natural looking images, namely the pixel-wise or the generative loss.
The encoder of the generative models furthermore was trained such that the hidden-layer activity of the discriminator is similar for generated and real images (in addition to the KL-term).

Advantages, disadvantages, and use-cases for both loss functions are discussed in the following.

First of all, both loss functions do not seem to be biologically plausible as they are not founded in Hebbian learning.
For the encoder loss, it could be argued that it is trained such that it elicits a \say{neural response} in the discriminator, similar to the one for real images.
This \say{activity matching} seems to be at least somewhat related to Hebbian learning as it explicitly considers the amount of activity on the level on single neurons.
However, discriminator and encoder itself are trained using backpropagation.

\citet{larsen2015autoencoding} state that the pixel-wise loss can lead to very high values even for small translation (see Section~\ref{subsubsec:representation_learning}).
Intuitively seeming like a disadvantage, this is only true for images with a high frequency and a large variance of pixel values.
A black-and-white image consisting of alternating black and white rows and the same image shifted by one pixel in $y$-direction in fact would result in a maximum pixel-wise loss as the two images are orthogonal to one another.
Natural images, however, usually are less susceptible to a large pixel-wise loss for small translation because there are less regions of high contrast.
In the context of natural images, the pixel-wise loss function for example enables the model to detect if an object is placed in another corner of the image as the loss would incrase for such high translations.
If, however, the aim is to generate more realistic images, the generative loss should be used since the pixel-wise loss leads to blurry reconstructions.
Also, \acp{GAN} empirically are not agnostic of an objects position (\textbf{REF: GAN-VLAE on dsprites}).

Another consideration is training stability.
Due to the adversarial loss, training \acp{GAN} has many difficulties (mode collapse, error oscillation\textbf{REF}), often resulting in failure modes.
Training regular \acp{VAE}, in contrast, is far more stable.

The choice of the loss function also might play an important role when studying a model’s representation with human \ac{IT} representations.
Compared to the pixel-wise loss, the generative loss allows for a more holistic image assessment.
This might play an important role in the emergence of a models’s latent representation, i.e. might lead to a more realistic latent representation in terms of human IT activations.
This aspect should be investigated in future work.

\subsection{Failure on ImageNet}

\subsection{Model Limitations}
All networks investigated in the course of this thesis are \acp{CNN}.
A network design has to be chosen for each network type (see Section~\ref{subsec:models}).
Some of these hyperparameters are context dependent.
A network operating on \textsc{Mnist}, for example, receives inputs of size $28\times 28$ pixels.
Such a networks must not be as deep as a network operating on ImageNet, as fewer layers are sufficient for the network to capture the image in it's entirety.
Other hyperparameters are chosen based on previous research.
Some configurations are known to be better for a certain task than others (see Section~\ref{subsec:models}).

Another consideration in the context of computational neuroscience is the biological plausibility.
This thesis aims at anwering in how far \acp{VAE} are reasonable models of the human visual cortex.
In order to do this, the model structure must allow comparisons with empirical data for the parts under investigation.

Specifically, lower model layers are compared with earlier, higher model layers with later regions in the visual cortex.
Furthermore, the models are chosen such that similar inputs translate to similar encodings.
The biological plausibility of the encoding should be further investigated in future research (\textbf{REF TO RELATED SECTION}).

However, due to practial reasons such as the availability of training data or computational resoureces, the model disregards actualities of the human body, a more refined model should incorporate.

Other than in the models used in this thesis, the human eye receives a stream of visual stimuli.
Perceiving movements of animate objects could play an important role in distinguishing between animate and inanimate objects.
The dissimilarity between semantic representations of animate and inanimate objects has been shown in \textbf{REF}.
Recurrent models such as LSTMs could allow to extend the models developed in the course of this thesis to sequences of images.

This thesis only considered image data to form semantic representations even though the human mind perceives the world through more senses.
Future research could aim at investigating if and by how far semantic representations can be improved by using additional sources of information, for example sound.

\section{Conclusion}\label{sec:conclusion}

\newpage
\printbibliography

\newpage
\appendix
\section{Additional Plots for Section~\ref{subsec:independence-of-vlae-embeddings}}\label{sec:additional-plots-for-section_independence}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/appendix_plots/notprop_1_3.png}
\caption{Proportionality of pixel intensities when fixing $\bm{z}_1 = \bm{z}_3=\varphi$. Created accordingly to Figure~\ref{fig:notprop}.}
\label{fig:notprop_1_3}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/appendix_plots/notprop_2_3.png}
\caption{Proportionality of pixel intensities when fixing $\bm{z}_2 = \bm{z}_3=\varphi$. Created accordingly to Figure~\ref{fig:notprop}.}
\label{fig:notprop_2_3}
\end{figure}

\section{Additional Plots for Section~\ref{subsec:morpho-mnist-on-vlae}}\label{sec:additional-plots-for-section_morpho_mnist}
\foreach \i in {1,...,3}{
\begin{figure}[H]
\centering
\foreach \j in {0,...,6}{
\begin{subfigure}{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{images/vlae_embeddings/embeddings_mu_\i_\j.png}
\label{subfig:vlae_morpho_z\i_\j}
\end{subfigure}
\hfill
}
\caption{Embeddings on VLAE layer $\bm{z}_{\i}$ colored by means of MorphoMNIST}
\label{fig:vlae_morpho_z\i}
\end{figure}
}
\pagebreak
\begin{landscape}
\section{\ac{VLAE} Networks used in Experiments on Sparsity (Section~\label{subsec:effective-network-capacity})}\label{sec:listings_sparsity_networks}
\input{listings/results_sparsity/fm1-encoder.tex}
\pagebreak
\input{listings/results_sparsity/fm1-decoder.tex}
\pagebreak
\input{listings/results_sparsity/fm2-encoder.tex}
\pagebreak
\input{listings/results_sparsity/fm2-decoder.tex}
\pagebreak
\input{listings/results_sparsity/fm3-encoder.tex}
\pagebreak
\input{listings/results_sparsity/fm3-decoder.tex}
\end{landscape}

\pagebreak

\section{Discriminator Network used in Section \ref{subsec:vae-generated-samples-vs-true-samples}}\label{sec:listing_discriminator_network}
\begin{lstlisting}[caption={The discriminator network used to distinguish generated \ac{VAE}/\ac{VLAE} samples from true \textsc{MNIST} images.},captionpos=b]
_________________________________________________________________
Layer (type)                 Output Shape Param #
=================================================================
input_2 (InputLayer)         (None, 28, 28, 1)         0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 24, 24, 20)        520
_________________________________________________________________
leaky_re_lu_4 (LeakyReLU)    (None, 24, 24, 20)        0
_________________________________________________________________
batch_normalization_23 (Batc (None, 24, 24, 20)        80
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 22, 22, 20)        3620
_________________________________________________________________
leaky_re_lu_5 (LeakyReLU)    (None, 22, 22, 20)        0
_________________________________________________________________
batch_normalization_24 (Batc (None, 22, 22, 20)        80
_________________________________________________________________
flatten_5 (Flatten)          (None, 9680)              0
_________________________________________________________________
dense_15 (Dense)             (None, 100)               968100
_________________________________________________________________
leaky_re_lu_6 (LeakyReLU)    (None, 100)               0
_________________________________________________________________
batch_normalization_25 (Batc (None, 100)               400
_________________________________________________________________
dense_16 (Dense)             (None, 1)                 101
=================================================================
Total params: 972,901
Trainable params: 972,621
Non-trainable params: 280
_________________________________________________________________
\end{lstlisting}


\newpage
\pagenumbering{Roman}
\setcounter{page}{\thesavepage}
\section*{Acronyms}
\begin{acronym}[TDMA]
\acro{AdaIN}{Adaptive Instance Normalization}
\acro{ALAE}{Adversarial Latent Autoencoder}
\acro{VAE}{Variational Autoencoder}
\acro{CNN}{Convolutional Neural Network}
\acro{IT}{Inferior Temporal Cortex}
\acro{ReLU}{Rectified Linear Unit}
\acro{LeakyReLU}{Leaky Rectified Linear Unit}
\acro{ILSVRC2017}{Large Scale Visual Recognition Challenge 2017}
\acro{MSE}{Mean Squared Error}
\acro{RDM}{Representational Dissimilarity Matrix}
\acro{KL-divergence}{Kullback-Leibler divergence}
\acro{ELBO}{Evidence Lower Bound}
\acro{PDF}{Probability Density Function}
\acro{GAN}{Generative Adversarial Network}
\acro{VLAE}{Variational Ladder Autoencoder}
\acro{LVAE}{Ladder Variational Autoencoder}
\acro{CNS}{Central Nervous System}
\acro{V1}{Primary Visual Cortex}
\acro{LGN}{Lateral Geniculate Nucleus}
\acro{TEO}{Temporo-Occipital Area}
\end{acronym}
\newpage
\listoffigures
\newpage
\listoftables
\newpage
\input{erklaerung}


\end{document}
