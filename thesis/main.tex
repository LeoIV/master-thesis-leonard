\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[margin=1.0in]{geometry}
\usepackage{parskip}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{acronym}
\usepackage[natbib=true]{biblatex}
\usepackage{amsmath}
\usepackage{dirtytalk}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{pgffor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{soul}
\usepackage{lscape}
\usetikzlibrary{shapes,positioning,decorations.pathreplacing}
\addbibresource{literature.bib}
\renewcommand{\baselinestretch}{1.0}
% @formatter:off
\let\oldsection\section
\renewcommand\section{\clearpage\oldsection}
% @formatter:on
\newcounter{savepage}

\lstset{lineskip=-0.7ex,frame=single,breaklines=true}

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
#1\;\delimsize\|\;#2%
}
\newcommand{\kldiv}{D_{KL}\infdivx}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\makeatletter
\newenvironment{breakablealgorithm}
{% \begin{breakablealgorithm}
\begin{center}
\refstepcounter{algorithm}% New algorithm
    \vspace{1em}
    \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
    \renewcommand{\caption}[2][\relax]{% Make a new \caption
    {\raggedright\textbf{\ALG@name~\thealgorithm} ##2\par}%
    \ifx\relax##1\relax % #1 is \relax
    \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
    \else % #1 is not \relax
    \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
    \fi
    \kern2pt\hrule\kern4pt
    }
}{% \end{breakablealgorithm}
\kern2pt\hrule\relax% \@fs@post for \@fs@ruled
\vspace{1em}
\end{center}
}
\makeatother

\begin{document}

\begin{titlepage}
\centering
\includegraphics[width=0.25\textwidth]{rublogo.png}\par
{\scshape\huge\bfseries Semantic Representations in Variational Autoencoders as a Model of the Visual System \par}
{\scshape\large Schriftliche Pr체fungsarbeit f체r die Master-Pr체fung des Studiengangs Angewandte Informatik an der Ruhr-Universit채t Bochum\par}
\vspace{1em}
vorgelegt von\par
\vspace{2em}
Leonard Papenmeier\par 108017257755\par
\vspace{2em}
01.01.1980\par

\vfill
Prof. Dr. Laurenz Wiskott\par
M.Sc. Zahra Fayyaz


\end{titlepage}
\pagenumbering{Roman}

\section*{\centering Abstract}

bla bla bla

\tableofcontents
\newpage
\setcounter{savepage}{\arabic{page}}
\pagenumbering{arabic}


\section{Introduction}\label{sec:introduction}

Research in artificial neural networks has risen in recent years due to their success in a large variety of different tasks.
This increase in research has led to progressively better network architectures, achieving better and better results.
However, the improvement of network architectures is mainly driven by the question of whether a new improvement will lead to better results \citep{lindsay2020convolutional}.

Biological plausibility, an essential consideration in computational neuroscience, is often of no interest to researchers aiming at solving a complex problem.
Even though progress in neural network research has become more disconnected from the biological example, they are inspired by biology.
Modern neural networks operating on images (mainly \acp{CNN}) indeed share many features with the visual system.

Even though neural networks are trained in a manner different from the way the brain learns and builds memories, it has been shown that image classification networks are related to the visual system, not only areas where this relatedness has been built into the model.

Both the biological foundation of \acp{CNN} and the more recent insights on their more indirect relatedness to the visual system qualify them as a potentially useful visual system model.

Unfortunately, the neural networks where this kind of relatedness has been discovered are trained in a supervised manner, requiring lots of labeled data.
This kind of learning is disconnected from human perception, where often one single example is sufficient when it comes to grouping objects into classes.

The \ac{VAE} is one important representative of a class of models \textit{generative models}.
This class of models allows a different training procedure, still requiring many samples but no labels.
FFurthermore, \acp{VAE} build generalizable latent representations of inputs, which might be similar to the abstract representation of the brain when perceiving the world.

\acp{VAE}, therefore, could be an important step towards a more realistic model of the visual system.

This thesis aims to answer whether \acp{VAE} prove to be a successful model of the visual system.

\section{Theoretical Background}\label{sec:theoretical-background}
\input{background}


\section{Methods}\label{sec:methods}
\input{methods}


\section{Results and Discussion}\label{sec:results}
\input{results}

\section{Conclusion}\label{sec:conclusion}

This thesis discussed if \acfp{VAE} are candidate models of the visual cortex, potentially allowing to obtain semantic representations of the input.
Only little evidence has been found that \acp{VAE} and other related models have this property, suggesting that these models at least need to be modified to be good models of the visual system.
A more pessimistic conclusion is that unsupervised models per se are unfit models of the visual system.

This, however, leads to the question why supervised explain \ac{IT} activity better than unsupervised models.

One assumption is that the brain also mainly learns in a supervised manner and that supervision is always required to build realistic models of the brain.
This assumption seems to be too naive for a variety of reasons.

First, the models discussed in this thesis are too unrelated to the brain to allow such a conclusion.
The role of top-down connections as well as the role of sequential data has not been investigated thoroughly.
Furthermore, there are unsupervised models leading to Gabor wavelets~\citep{berkes2005slow}.
This is another hint that unsupervised models in general might be suitable models but need more refinement.

This thesis ruled out a family of models (\acp{VAE} and \acp{VLAE} in particular) as good models of the visual system.
But also, this thesis discussed ways to extend these models to better model the visual cortex and the emergence of semantic representations, suggesting multiple approaches for future research.

\newpage
\printbibliography

\newpage
\pagenumbering{Roman}
\setcounter{page}{\thesavepage}
\appendix
\section{Additional Plots for Section~\ref{subsec:independence-of-vlae-embeddings}}\label{sec:additional-plots-for-section_independence}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/notprop/mnist/vlae/ccs_0_2_vlae.png}
\caption{Proportionality of pixel intensities when fixing $\bm{z}_1 = \bm{z}_3=\varphi$ for \ac{VLAE} on \textsc{Mnist}. Created accordingly to Figure~\ref{fig:notprop}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/notprop/mnist/vlae/ccs_1_2_vlae.png}
\caption{Proportionality of pixel intensities when fixing $\bm{z}_2 = \bm{z}_3=\varphi$ for \ac{VLAE} on \textsc{Mnist}. Created accordingly to Figure~\ref{fig:notprop}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/notprop/mnist/vlae_gan/ccs_0_1_vlae_gan.png}
\caption{Proportionality of pixel intensities when fixing $\bm{z}_1 = \bm{z}_2=\varphi$ for \ac{VLAE}-\ac{GAN} on \textsc{Mnist}. Created accordingly to Figure~\ref{fig:notprop}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/notprop/mnist/vlae_gan/ccs_0_2_vlae_gan.png}
\caption{Proportionality of pixel intensities when fixing $\bm{z}_1 = \bm{z}_3=\varphi$ for \ac{VLAE}-\ac{GAN} on \textsc{Mnist}. Created accordingly to Figure~\ref{fig:notprop}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/notprop/mnist/vlae_gan/ccs_1_2_vlae_gan.png}
\caption{Proportionality of pixel intensities when fixing $\bm{z}_2 = \bm{z}_3=\varphi$ for \ac{VLAE}-\ac{GAN} on \textsc{Mnist}. Created accordingly to Figure~\ref{fig:notprop}.}
\end{figure}


\pagebreak
\section{Additional Plots For Section~\ref{subsec:latent-space-entanglement-and-categorical-factors-of-variation}}\label{sec:additional_plots_latent_space_entanglement}
\begin{figure}[H]
\centering
\begin{subfigure}{\textwidth}
\centering
\includegraphics[width=\textwidth]{images/latent_space_entanglement/vae_7500_traverse_square_ellipse.png}
\caption{Latent space traversal from \textit{Square} $\rightarrow$ \textit{Ellipse}}
\end{subfigure}
\begin{subfigure}{\textwidth}
\centering
\includegraphics[width=\textwidth]{images/latent_space_entanglement/vae_7500_traverse_ellipse_heart.png}
\caption{Latent space traversal from \textit{Ellipse} $\rightarrow$ \textit{Heart}}
\end{subfigure}
\begin{subfigure}{\textwidth}
\centering
\includegraphics[width=\textwidth]{images/latent_space_entanglement/vae_7500_traverse_heart_square.png}
\caption{Latent space traversal from \textit{Heart} $\rightarrow$ \textit{Square}}
\label{subfig:10000_vae_latent_space_traversal_heart_to_square}
\end{subfigure}
\caption{Latent space traversal between latent space representations of images with certain shapes for 7,500-\ac{VAE}. Color-values were inverted for this plot.}
\label{fig:7500_vae_latent_space_traversal_shape_to_shape}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}{\textwidth}
\centering
\includegraphics[width=\textwidth]{images/latent_space_entanglement/vae_5000_traverse_square_ellipse.png}
\caption{Latent space traversal from \textit{Square} $\rightarrow$ \textit{Ellipse}}
\end{subfigure}
\begin{subfigure}{\textwidth}
\centering
\includegraphics[width=\textwidth]{images/latent_space_entanglement/vae_5000_traverse_ellipse_heart.png}
\caption{Latent space traversal from \textit{Ellipse} $\rightarrow$ \textit{Heart}}
\end{subfigure}
\begin{subfigure}{\textwidth}
\centering
\includegraphics[width=\textwidth]{images/latent_space_entanglement/vae_5000_traverse_heart_square.png}
\caption{Latent space traversal from \textit{Heart} $\rightarrow$ \textit{Square}}
\label{subfig:10000_vae_latent_space_traversal_heart_to_square}
\end{subfigure}
\caption{Latent space traversal between latent space representations of images with certain shapes for 5,000-\ac{VAE}. Color-values were inverted for this plot.}
\label{fig:5000_vae_latent_space_traversal_shape_to_shape}
\end{figure}


\pagebreak
\section{Additional Latent Space Embeddings (Section~\ref{subsubsec:latent_space_embeddings})}

\subsection{\textsc{Mnist}}

\begin{figure}[H]
\centering
\begin{subfigure}{.32\textwidth}
\includegraphics[width=\textwidth]{images/latent_spaces/mnist/vae_gan/embeddings_mu_0.png}
\caption{Latent space colored by digit slant}
\label{subfig:vae_mnist_latent_space_slant}
\end{subfigure}
\hfill
\begin{subfigure}{.32\textwidth}
\includegraphics[width=\textwidth]{images/latent_spaces/mnist/vae_gan/embeddings_mu_1.png}
\caption{Latent space colored by digit thickness}
\label{subfig:vae_mnist_latent_space_thickness}
\end{subfigure}
\hfill
\begin{subfigure}{.32\textwidth}
\includegraphics[width=\textwidth]{images/latent_spaces/mnist/vae_gan/embeddings_mu_2.png}
\caption{Latent space colored by digit area}
\label{subfig:vae_mnist_latent_space_area}
\end{subfigure}
\hfill
\begin{subfigure}{.24\textwidth}
\includegraphics[width=\textwidth]{images/latent_spaces/mnist/vae_gan/embeddings_mu_3.png}
\caption{Latent space colored by digit length}
\label{subfig:vae_mnist_latent_space_length}
\end{subfigure}
\hfill
\begin{subfigure}{.24\textwidth}
\includegraphics[width=\textwidth]{images/latent_spaces/mnist/vae_gan/embeddings_mu_4.png}
\caption{Latent space colored by digit width}
\label{subfig:vae_mnist_latent_space_width}
\end{subfigure}
\hfill
\begin{subfigure}{.24\textwidth}
\includegraphics[width=\textwidth]{images/latent_spaces/mnist/vae_gan/embeddings_mu_5.png}
\caption{Latent space colored by digit height}
\label{subfig:vae_mnist_latent_space_height}
\end{subfigure}
\hfill
\begin{subfigure}{.24\textwidth}
\includegraphics[width=\textwidth]{images/latent_spaces/mnist/vae_gan/embeddings_mu_6.png}
\caption{Latent space colored by digit identity}
\label{subfig:vae_mnist_latent_space_identity}
\end{subfigure}
\caption{Latent space colored by different means of \ac{VAE}-\ac{GAN} with $dim_z=2$ trained on \textsc{Mnist} dataset}
\label{fig:vae_latent_space_mnist}
\end{figure}

\begin{landscape}
\begin{figure}[H]
\centering
\foreach \i in {1,2,3}{
\begin{subfigure}{.16\textwidth}
\includegraphics[width=\textwidth]{images/latent_spaces/mnist/vlae_gan/embeddings_mu_\i_0.png}
\caption{$z_{\i}$: digit slant}
\label{subfig:vlae_mnist_latent_space_z_\i_slant}
\end{subfigure}
\hfill
\begin{subfigure}{.16\textwidth}
\includegraphics[width=\textwidth]{images/latent_spaces/mnist/vlae_gan/embeddings_mu_\i_1.png}
\caption{$z_{\i}$: digit thickness}
\label{subfig:vlae_mnist_latent_space_z_\i_thickness}
\end{subfigure}
\hfill
\begin{subfigure}{.16\textwidth}
\includegraphics[width=\textwidth]{images/latent_spaces/mnist/vlae_gan/embeddings_mu_\i_2.png}
\caption{$z_{\i}$: digit area}
\label{subfig:vlae_mnist_latent_space_z_\i_area}
\end{subfigure}
\hfill
\begin{subfigure}{.16\textwidth}
\includegraphics[width=\textwidth]{images/latent_spaces/mnist/vlae_gan/embeddings_mu_\i_3.png}
\caption{$z_{\i}$: digit length}
\label{subfig:vlae_mnist_latent_space_z_\i_length}
\end{subfigure}
\hfill
\begin{subfigure}{.16\textwidth}
\includegraphics[width=\textwidth]{images/latent_spaces/mnist/vlae_gan/embeddings_mu_\i_4.png}
\caption{$z_{\i}$: digit width}
\label{subfig:vlae_mnist_latent_space_z_\i_width}
\end{subfigure}
\hfill
\begin{subfigure}{.16\textwidth}
\includegraphics[width=\textwidth]{images/latent_spaces/mnist/vlae_gan/embeddings_mu_\i_5.png}
\caption{$z_{\i}$: digit height}
\label{subfig:vlae_mnist_latent_space_z_\i_height}
\end{subfigure}
\hfill
\begin{subfigure}{.16\textwidth}
\includegraphics[width=\textwidth]{images/latent_spaces/mnist/vlae_gan/embeddings_mu_\i_6.png}
\caption{$z_{\i}$: digit identity}
\label{subfig:vlae_mnist_latent_space_z_\i_identity}
\end{subfigure}}
\caption{Latent space colored by different means of \ac{VLAE}-\ac{GAN} with $dim_z=2$ trained on \textsc{Mnist} dataset}
\label{fig:vlae_latent_space_mnist}
\end{figure}
\end{landscape}

\subsection{dsprites}
\begin{figure}[H]
\centering
\begin{subfigure}{.19\textwidth}
\includegraphics[width=\textwidth]{images/latent_spaces/dsprites/vae_gan/embeddings_mu_0.png}
\caption{Latent space colored by object shape}
\end{subfigure}
\hfill
\begin{subfigure}{.19\textwidth}
\includegraphics[width=\textwidth]{images/latent_spaces/dsprites/vae_gan/embeddings_mu_1.png}
\caption{Latent space colored by object scale}
\label{subfig:vae_embedding_dsprites_scale}
\end{subfigure}
\hfill
\begin{subfigure}{.19\textwidth}
\includegraphics[width=\textwidth]{images/latent_spaces/dsprites/vae_gan/embeddings_mu_2.png}
\caption{Latent space colored by object orientation}
\label{subfig:vae_embedding_dsprites_orientation}
\end{subfigure}
\hfill
\begin{subfigure}{.19\textwidth}
\includegraphics[width=\textwidth]{images/latent_spaces/dsprites/vae_gan/embeddings_mu_3.png}
\caption{Latent space colored by object $x$-position}
\end{subfigure}
\hfill
\begin{subfigure}{.19\textwidth}
\includegraphics[width=\textwidth]{images/latent_spaces/dsprites/vae_gan/embeddings_mu_4.png}
\caption{Latent space colored by object $y$-position}
\end{subfigure}
\caption{t-SNE latent space embeddings colored by different means of \ac{VAE}-\ac{GAN} with $dim_z=6$ trained on dsprites dataset}
\label{fig:vae_gan_latent_space_dsprites}
\end{figure}

\textbf{TODO: VLAE-GAN}


\pagebreak
\begin{landscape}
\section{\ac{VLAE} Networks used in Experiments on Sparsity (Section~\label{subsec:effective-network-capacity})}\label{sec:listings_sparsity_networks}
\input{listings/results_sparsity/fm1-encoder.tex}
\pagebreak
\input{listings/results_sparsity/fm1-decoder.tex}
\pagebreak
\input{listings/results_sparsity/fm2-encoder.tex}
\pagebreak
\input{listings/results_sparsity/fm2-decoder.tex}
\pagebreak
\input{listings/results_sparsity/fm3-encoder.tex}
\pagebreak
\input{listings/results_sparsity/fm3-decoder.tex}
\end{landscape}

\pagebreak
\section{Additional Plots for Section~\ref{subsubsec:pixel_wise_statistics}}\label{sec:appendix_pixel_wise_statistics}

\begin{figure}[H]
\centering
\includegraphics[width=.8\textwidth]{images/generated_vs_true/vlae_kde.png}
\caption{Histogram of mean $z$-values for different layers and dimensions of \ac{VLAE} (blue), the result of the \ac{KDE} (green), and a standard normal distribution (orange).}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.8\textwidth]{images/generated_vs_true/vae_kde.png}
\caption{Histogram of mean $z$-values for different layers and dimensions of \ac{VAE} (blue), the result of the \ac{KDE} (green), and a standard normal distribution (orange).}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=.8\textwidth]{images/generated_vs_true/vae_gan_kde.png}
\caption{Histogram of mean $z$-values for different layers and dimensions of \ac{VAE}-\ac{GAN} (blue), the result of the \ac{KDE} (green), and a standard normal distribution (orange).}
\end{figure}

\pagebreak
\section{Feature Extraction Network in Section~\ref{subsec:latent-space-entanglement-and-categorical-factors-of-variation}}\label{sec:appendix_feature_extraction_network_ppl_dsprites}
\begin{lstlisting}[caption={Feature extraction network for \ac{PPL} computation.},captionpos=b]
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
discriminator_input (InputLa (None, 64, 64, 1)         0
_________________________________________________________________
conv2d_91 (Conv2D)           (None, 32, 32, 64)        1664
_________________________________________________________________
batch_normalization_114 (Bat (None, 32, 32, 64)        256
_________________________________________________________________
leaky_re_lu_55 (LeakyReLU)   (None, 32, 32, 64)        0
_________________________________________________________________
conv2d_92 (Conv2D)           (None, 16, 16, 128)       204928
_________________________________________________________________
batch_normalization_115 (Bat (None, 16, 16, 128)       512
_________________________________________________________________
leaky_re_lu_56 (LeakyReLU)   (None, 16, 16, 128)       0
_________________________________________________________________
conv2d_93 (Conv2D)           (None, 8, 8, 256)         295168
_________________________________________________________________
batch_normalization_116 (Bat (None, 8, 8, 256)         1024
_________________________________________________________________
leaky_re_lu_57 (LeakyReLU)   (None, 8, 8, 256)         0
_________________________________________________________________
conv2d_94 (Conv2D)           (None, 8, 8, 512)         1180160
_________________________________________________________________
batch_normalization_117 (Bat (None, 8, 8, 512)         2048
_________________________________________________________________
leaky_re_lu_58 (LeakyReLU)   (None, 8, 8, 512)         0
_________________________________________________________________
conv2d_95 (Conv2D)           (None, 3, 3, 3)           1539
_________________________________________________________________
flatten_31 (Flatten)         (None, 27)                0
_________________________________________________________________
dense_31 (Dense)             (None, 3)                 84
_________________________________________________________________
activation_31 (Activation)   (None, 3)                 0
=================================================================
Total params: 1,687,383
Trainable params: 1,685,463
Non-trainable params: 1,920
_________________________________________________________________
\end{lstlisting}

\pagebreak

\section{Discriminator Network used in Section \ref{subsec:vae-generated-samples-vs-true-samples}}\label{sec:listing_discriminator_network}
\begin{lstlisting}[caption={The discriminator network used to distinguish generated \ac{VAE}/\ac{VLAE} samples from true \textsc{MNIST} images.},captionpos=b]
_________________________________________________________________
Layer (type)                 Output Shape Param #
=================================================================
input_2 (InputLayer)         (None, 28, 28, 1)         0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 24, 24, 20)        520
_________________________________________________________________
leaky_re_lu_4 (LeakyReLU)    (None, 24, 24, 20)        0
_________________________________________________________________
batch_normalization_23 (Batc (None, 24, 24, 20)        80
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 22, 22, 20)        3620
_________________________________________________________________
leaky_re_lu_5 (LeakyReLU)    (None, 22, 22, 20)        0
_________________________________________________________________
batch_normalization_24 (Batc (None, 22, 22, 20)        80
_________________________________________________________________
flatten_5 (Flatten)          (None, 9680)              0
_________________________________________________________________
dense_15 (Dense)             (None, 100)               968100
_________________________________________________________________
leaky_re_lu_6 (LeakyReLU)    (None, 100)               0
_________________________________________________________________
batch_normalization_25 (Batc (None, 100)               400
_________________________________________________________________
dense_16 (Dense)             (None, 1)                 101
=================================================================
Total params: 972,901
Trainable params: 972,621
Non-trainable params: 280
_________________________________________________________________
\end{lstlisting}


\newpage
\section*{Acronyms}
\begin{acronym}[TDMA]
\acro{AdaIN}{Adaptive Instance Normalization}
\acro{ALAE}{Adversarial Latent Autoencoder}
\acro{VAE}{Variational Autoencoder}
\acro{CNN}{Convolutional Neural Network}
\acro{IT}{Inferior Temporal Cortex}
\acro{ReLU}{Rectified Linear Unit}
\acro{LeakyReLU}{Leaky Rectified Linear Unit}
\acro{ILSVRC2017}{Large Scale Visual Recognition Challenge 2017}
\acro{MSE}{Mean Squared Error}
\acro{RDM}{Representational Dissimilarity Matrix}
\acro{KL-divergence}{Kullback-Leibler divergence}
\acro{KL}{Kullback-Leibler}
\acro{ELBO}{Evidence Lower Bound}
\acro{PDF}{Probability Density Function}
\acro{NLP}{Natural Language Processing}
\acro{GAN}{Generative Adversarial Network}
\acro{VLAE}{Variational Ladder Autoencoder}
\acro{LVAE}{Ladder Variational Autoencoder}
\acro{CNS}{Central Nervous System}
\acro{V1}{Primary Visual Cortex}
\acro{V2}{Secondary Visual Cortex}
\acro{V4}{Quaternary Visual Cortex}
\acro{LGN}{Lateral Geniculate Nucleus}
\acro{TEO}{Temporo-Occipital Area}
\acro{PPL}{Perceptual Path Length}
\acro{PDF}{Probability Density Function}
\acro{KDE}{Kernel Density Estimation}
\end{acronym}
\newpage
\listoffigures
\newpage
\listoftables
\newpage
\input{erklaerung}


\end{document}
