\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{parskip}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{acronym}
\usepackage[natbib=true]{biblatex}
\usepackage{amsmath}
\usepackage{dirtytalk}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage{pgffor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{lscape}
\usetikzlibrary{shapes,positioning,decorations.pathreplacing}
\addbibresource{literature.bib}
\renewcommand{\baselinestretch}{1.5}
\let\oldsection\section
\renewcommand


\section{\clearpage\oldsection}
\newcounter{savepage}

\lstset{lineskip=-0.7ex,frame=single,breaklines=true}

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
#1\;\delimsize\|\;#2%
}
\newcommand{\kldiv}{D_{KL}\infdivx}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\makeatletter
\newenvironment{breakablealgorithm}
{% \begin{breakablealgorithm}
\begin{center}
\refstepcounter{algorithm}% New algorithm
    \vspace{1em}
    \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
    \renewcommand{\caption}[2][\relax]{% Make a new \caption
    {\raggedright\textbf{\ALG@name~\thealgorithm} ##2\par}%
    \ifx\relax##1\relax % #1 is \relax
    \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
    \else % #1 is not \relax
    \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
    \fi
    \kern2pt\hrule\kern4pt
    }
}{% \end{breakablealgorithm}
\kern2pt\hrule\relax% \@fs@post for \@fs@ruled
\vspace{1em}
\end{center}
}
\makeatother

\begin{document}

\begin{titlepage}
\centering
\includegraphics[width=0.25\textwidth]{rublogo.png}\par
{\scshape\huge\bfseries Semantic Representations in Variational Autoencoders as a Model of the Visual System \par}
{\scshape\large Schriftliche Pr체fungsarbeit f체r die Master-Pr체fung des Studiengangs Angewandte Informatik an der Ruhr-Universit채t Bochum\par}
\vspace{1em}
vorgelegt von\par
\vspace{2em}
Leonard Papenmeier\par 108017257755\par
\vspace{2em}
01.01.1980\par

\vfill
Prof. Dr. Laurenz Wiskott\par
M.Sc. Zahra Fayyaz


\end{titlepage}
\pagenumbering{Roman}

\section*{\centering Abstract}

bla bla bla

\tableofcontents
\newpage
\setcounter{savepage}{\arabic{page}}
\pagenumbering{arabic}


\section{Introduction}\label{sec:introduction}

Research in artificial neural networks has risen in recent years due to their success in a large variety of different tasks.
This increase in research has led to progressively better network architectures, achieving better and better results.
The improvement of network architectures, however, is mainly driven by the question whether a new improvement will lead to better results \citep{lindsay2020convolutional}.

Biological plausibility, an important consideration in the field of computational neuroscience, often is of no interest to researchers aiming at solving a complex problem.
Even though progress in neural network research has become more and more disconnected from the biological example, they are inspired by biology.
Modern neural networks operating on images (mainly \acp{CNN}) indeed share many features with the visual system.

Even though neural networks are trained in a manner largely detached from the way the brain learns and builds memories, it has been shown that image classification networks are related to the visual system, not only areas where this relatedness has been built into the model.

Both, the biological foundation of \acp{CNN} and the more recent insights on their more indirect relatedness to the visual system qualify them as a potentially good model of the visual system.

Unfortunately, the neural networks where this kind of relatedness has been discovered so far are trained in a supervised manner, requiring lots of labeled data.
This kind of learning is disconnected from human perception where often one single example is sufficient when it comes to grouping objects into classes.

The \ac{VAE} is one important representative of a class of models \textit{generative models}.
This class of models allows a different training procedure, still requiring many samples but no labels.
Furthermore, \acp{VAE} build generalizable latent representations of inputs which might be similar to the abstract representation of the brain when perceiving the world.

\acp{VAE}, therefore, could be an important step towards a more realistic model of the visual system.

This thesis aims at answering the question whether \acp{VAE} prove to be a successful model of the visual system.

\section{Theoretical Background}\label{sec:theoretical-background}
\input{background}


\section{Methods}\label{sec:methods}
\input{methods}


\section{Results and Discussion}\label{sec:results}
\input{results}

\section{Conclusion}\label{sec:conclusion}

This thesis discussed if \acfp{VAE} are candidate models of the visual cortex, potentially allowing to obtain semantic representations of the input.
Only little evidence has been found that \acp{VAE} and other related models have this property, suggesting that these models at least need to be modified to be good models of the visual system.
A more pessimistic conclusion is that unsupervised models per se are unfit models of the visual system.

This, however, leads to the question why supervised explain \ac{IT} activity better than unsupervised models.

One assumption is that the brain also mainly learns in a supervised manner and that supervision is always required to build realistic models of the brain.
This assumption seems to be too naive for a variety of reasons.

First, the models discussed in this thesis are too unrelated to the brain to allow such a conclusion.
The role of top-down connections as well as the role of sequential data has not been investigated thoroughly.
Furthermore, there are unsupervised models leading to Gabor wavelets~\citep{berkes2005slow}.
This is another hint that unsupervised models in general might be suitable models but need more refinement.

This thesis ruled out a family of models (\acp{VAE} and \acp{VLAE} in particular) as good models of the visual system.
But also, this thesis discussed ways to extend these models to better model the visual cortex and the emergence of semantic representations, suggesting multiple approaches for future research.

\newpage
\printbibliography

\newpage
\pagenumbering{Roman}
\setcounter{page}{\thesavepage}
\appendix
\section{Additional Plots for Section~\ref{subsec:independence-of-vlae-embeddings}}\label{sec:additional-plots-for-section_independence}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/notprop/mnist/vlae/ccs_0_2_vlae.png}
\caption{Proportionality of pixel intensities when fixing $\bm{z}_1 = \bm{z}_3=\varphi$ for \ac{VLAE} on \textsc{Mnist}. Created accordingly to Figure~\ref{fig:notprop}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/notprop/mnist/vlae/ccs_1_2_vlae.png}
\caption{Proportionality of pixel intensities when fixing $\bm{z}_2 = \bm{z}_3=\varphi$ for \ac{VLAE} on \textsc{Mnist}. Created accordingly to Figure~\ref{fig:notprop}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/notprop/mnist/vlae_gan/ccs_0_1_vlae_gan.png}
\caption{Proportionality of pixel intensities when fixing $\bm{z}_1 = \bm{z}_2=\varphi$ for \ac{VLAE}-\ac{GAN} on \textsc{Mnist}. Created accordingly to Figure~\ref{fig:notprop}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/notprop/mnist/vlae_gan/ccs_0_2_vlae_gan.png}
\caption{Proportionality of pixel intensities when fixing $\bm{z}_1 = \bm{z}_3=\varphi$ for \ac{VLAE}-\ac{GAN} on \textsc{Mnist}. Created accordingly to Figure~\ref{fig:notprop}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{images/notprop/mnist/vlae_gan/ccs_1_2_vlae_gan.png}
\caption{Proportionality of pixel intensities when fixing $\bm{z}_2 = \bm{z}_3=\varphi$ for \ac{VLAE}-\ac{GAN} on \textsc{Mnist}. Created accordingly to Figure~\ref{fig:notprop}.}
\end{figure}

\pagebreak
\begin{landscape}
\section{\ac{VLAE} Networks used in Experiments on Sparsity (Section~\label{subsec:effective-network-capacity})}\label{sec:listings_sparsity_networks}
\input{listings/results_sparsity/fm1-encoder.tex}
\pagebreak
\input{listings/results_sparsity/fm1-decoder.tex}
\pagebreak
\input{listings/results_sparsity/fm2-encoder.tex}
\pagebreak
\input{listings/results_sparsity/fm2-decoder.tex}
\pagebreak
\input{listings/results_sparsity/fm3-encoder.tex}
\pagebreak
\input{listings/results_sparsity/fm3-decoder.tex}
\end{landscape}

\pagebreak

\section{Discriminator Network used in Section \ref{subsec:vae-generated-samples-vs-true-samples}}\label{sec:listing_discriminator_network}
\begin{lstlisting}[caption={The discriminator network used to distinguish generated \ac{VAE}/\ac{VLAE} samples from true \textsc{MNIST} images.},captionpos=b]
_________________________________________________________________
Layer (type)                 Output Shape Param #
=================================================================
input_2 (InputLayer)         (None, 28, 28, 1)         0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 24, 24, 20)        520
_________________________________________________________________
leaky_re_lu_4 (LeakyReLU)    (None, 24, 24, 20)        0
_________________________________________________________________
batch_normalization_23 (Batc (None, 24, 24, 20)        80
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 22, 22, 20)        3620
_________________________________________________________________
leaky_re_lu_5 (LeakyReLU)    (None, 22, 22, 20)        0
_________________________________________________________________
batch_normalization_24 (Batc (None, 22, 22, 20)        80
_________________________________________________________________
flatten_5 (Flatten)          (None, 9680)              0
_________________________________________________________________
dense_15 (Dense)             (None, 100)               968100
_________________________________________________________________
leaky_re_lu_6 (LeakyReLU)    (None, 100)               0
_________________________________________________________________
batch_normalization_25 (Batc (None, 100)               400
_________________________________________________________________
dense_16 (Dense)             (None, 1)                 101
=================================================================
Total params: 972,901
Trainable params: 972,621
Non-trainable params: 280
_________________________________________________________________
\end{lstlisting}


\newpage
\section*{Acronyms}
\begin{acronym}[TDMA]
\acro{AdaIN}{Adaptive Instance Normalization}
\acro{ALAE}{Adversarial Latent Autoencoder}
\acro{VAE}{Variational Autoencoder}
\acro{CNN}{Convolutional Neural Network}
\acro{IT}{Inferior Temporal Cortex}
\acro{ReLU}{Rectified Linear Unit}
\acro{LeakyReLU}{Leaky Rectified Linear Unit}
\acro{ILSVRC2017}{Large Scale Visual Recognition Challenge 2017}
\acro{MSE}{Mean Squared Error}
\acro{RDM}{Representational Dissimilarity Matrix}
\acro{KL-divergence}{Kullback-Leibler divergence}
\acro{KL}{Kullback-Leibler}
\acro{ELBO}{Evidence Lower Bound}
\acro{PDF}{Probability Density Function}
\acro{NLP}{Natural Language Processing}
\acro{GAN}{Generative Adversarial Network}
\acro{VLAE}{Variational Ladder Autoencoder}
\acro{LVAE}{Ladder Variational Autoencoder}
\acro{CNS}{Central Nervous System}
\acro{V1}{Primary Visual Cortex}
\acro{V2}{Secondary Visual Cortex}
\acro{V4}{Quaternary Visual Cortex}
\acro{LGN}{Lateral Geniculate Nucleus}
\acro{TEO}{Temporo-Occipital Area}
\end{acronym}
\newpage
\listoffigures
\newpage
\listoftables
\newpage
\input{erklaerung}


\end{document}
