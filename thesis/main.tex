\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[margin=1.0in]{geometry}
\usepackage{parskip}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{acronym}
\usepackage[natbib=true,backend=bibtex]{biblatex}
\usepackage{amsmath}
\usepackage{dirtytalk}
\usepackage{bookmark}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{csquotes}
\usepackage[T1]{fontenc}
\usepackage{pgffor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{bm}
\usepackage{adjustbox}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{soul}
\usepackage{lscape}
\usepackage{tabularx}
\usetikzlibrary{shapes,positioning,decorations.pathreplacing}
\addbibresource{literature.bib}
\renewcommand{\baselinestretch}{1.0}
% @formatter:off
\let\oldsection\section
\renewcommand\section{\clearpage\oldsection}
% @formatter:on
\newcommand{\round}[2]{\num[round-mode=places,round-precision=#1]{#2}}
\newcounter{savepage}

\lstset{lineskip=-0.7ex,frame=single,breaklines=true}

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
#1\;\delimsize\|\;#2%
}
\newcommand{\kldiv}{D_{KL}\infdivx}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\makeatletter
\newenvironment{breakablealgorithm}
{% \begin{breakablealgorithm}
\begin{center}
\refstepcounter{algorithm}% New algorithm
    \vspace{1em}
    \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
    \renewcommand{\caption}[2][\relax]{% Make a new \caption
    {\raggedright\textbf{\ALG@name~\thealgorithm} ##2\par}%
    \ifx\relax##1\relax % #1 is \relax
    \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
    \else % #1 is not \relax
    \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
    \fi
    \kern2pt\hrule\kern4pt
    }
}{% \end{breakablealgorithm}
\kern2pt\hrule\relax% \@fs@post for \@fs@ruled
\vspace{1em}
\end{center}
}
\makeatother

\begin{document}

\begin{titlepage}
\centering
\includegraphics[width=0.25\textwidth]{rublogo.png}\par
{\scshape\huge\bfseries Semantic Representations in Variational Autoencoders as a Model of the Visual System \par}
{\scshape\large Schriftliche Pr체fungsarbeit f체r die Master-Pr체fung des Studiengangs Angewandte Informatik an der Ruhr-Universit채t Bochum\par}
\vspace{1em}
vorgelegt von\par
\vspace{2em}
Leonard Papenmeier\par 108017257755\par
\vspace{2em}
21.07.2020\par

\vfill
Prof. Dr. Laurenz Wiskott\par
M.Sc. Zahra Fayyaz


\end{titlepage}
\pagenumbering{Roman}


\newgeometry{margin=2.5in}
\section*{\centering Abstract}
Unsupervised convolutional neural networks have been successfully applied to various tasks, but their biological plausibility is underexplored.
Previous research has developed both biologically plausible networks and networks able to generate natural images, but the gap between these network types remains.
This thesis aims at answering two questions:
First, whether \acp{VAE} (a specific type of convolutional neural networks) are a biologically plausible model of the visual cortex and, second, whether the latent representations of these networks are related to semantic representations in the brain.
Regarding the first question, it is studied whether they learn similar features as the \acl{V1} and whether they employ sparseness similarly to the brain.
For the second question, it is investigated how the model maps categorical and continuous attributes in the latent space.

For this purpose, six different network types are developed and evaluated on different established datasets.
It is found that hierarchical and non-hierarchical \acp{VAE} do not learn Gabor wavelets;
And even though they employ sparseness, this sparseness seems dissimilar to sparseness the in the brain.
It is shown that feed-forward \ac{VAE}-models trained on static images are probably an inapt model of the visual cortex.
Furthermore, it is argued that the latent space represents many attributes of datasets in a highly non-linear manner.
This asks to revise common assumptions of the latent space structure.
\restoregeometry
\clearpage

\tableofcontents
\newpage
\listoffigures
\newpage
\listoftables
\newpage
\setcounter{savepage}{\arabic{page}}
\pagenumbering{arabic}


\acresetall
\section{Introduction}\label{sec:introduction}
\input{introduction}

\acresetall
\section{Theoretical Background}\label{sec:theoretical-background}
\input{background}

\acresetall
\section{Methods}\label{sec:methods}
\input{methods}

\acresetall
\section{Results and Discussion}\label{sec:results}
\input{results}

\acresetall
\section{Conclusion}\label{sec:conclusion}
\input{conclusion}



\newpage
\printbibliography

\newpage
\pagenumbering{Roman}
\setcounter{page}{\thesavepage}

\appendix


% \begin{landscape}
\section{Network Architectures}\label{sec:appendix_network_architectures}
\input{appendices/network_architectures}

\pagebreak
\section{Additional Plots For Section~\ref{subsec:latent-space-entanglement-and-categorical-factors-of-variation}}\label{sec:additional_plots_latent_space_entanglement}
\input{appendices/latent_space_entanglement_categorical}

\pagebreak
\section{Feature Extraction Network - Section~\ref{subsec:latent-space-entanglement-and-categorical-factors-of-variation}}\label{sec:appendix_feature_extraction_network_ppl_dsprites}
\input{appendices/feature_extraction_network}

\pagebreak
\section{Additional Plots for Section~\ref{subsubsec:latent_space_embeddings}}\label{sec:appendix_latent_space_embeddings}
\input{appendices/latent_space_embeddings}

\pagebreak
\section{Additional Plots for Section \ref{subsubsec:latent_space_traversals}}\label{sec:appendix_plots_latent_space_traversals}
\input{appendices/latent_space_traversals}

\pagebreak
\section{Additional Plots for Section~\ref{subsec:independence-of-vlae-embeddings}}\label{sec:additional-plots-for-section_independence}
\input{appendices/pixelwise_correlations}

\pagebreak
\section{Additional Plots for Section~\ref{subsubsec:pixel_wise_statistics}}\label{sec:appendix_pixel_wise_statistics}
\input{appendices/pixelwise_stats}

\pagebreak
\section{Discriminator Network - Section \ref{subsubsec:pixel_wise_distribution_mnist}}\label{sec:listing_discriminator_network}
\input{appendices/discriminator_network_pixelwise_dists}

\newpage
\section*{Acronyms}
\begin{acronym}[TDMA]
\acro{AdaIN}{adaptive instance normalization}
\acro{ALAE}{adversarial latent autoencoder}
\acro{CNN}{convolutional neural network}
\acro{CNS}{central nervous system}
\acro{ELBO}{evidence lower bound}
\acro{GAN}{generative adversarial network}
\acro{IT}{inferior temporal cortex}
\acro{ILSVRC2017}{Large Scale Visual Recognition Challenge 2017}
\acro{LeakyReLU}{leaky rectified linear unit}
\acro{LGN}{lateral geniculate nucleus}
\acro{KDE}{kernel density estimation}
\acro{KL-divergence}{Kullback-Leibler divergence}
\acro{KL}{Kullback-Leibler}
\acro{MSE}{mean squared error}
\acro{NLP}{natural language processing}
\acro{PCA}{principal component analysis}
\acrodefplural{PCA}{principal component analyses}
\acro{PDF}{probability density function}
\acro{PPL}{perceptual path length}
\acro{ReLU}{rectified linear unit}
\acro{RDM}{representational dissimilarity matrix}
\acrodefplural{RDM}{representational dissimilarity matrices}
\acro{SVM}{support vector machine}
\acro{TEO}{temporo-occipital area}
\acro{t-SNE}{t-distributed stochastic neighbor embedding}
\acro{VAE}{Variational Autoencoder}
\acro{VLAE}{Variational Ladder Autoencoder}
\acro{LVAE}{Ladder Variational Autoencoder}
\acro{V1}{primary visual cortex}
\acro{V2}{secondary visual cortex}
\acro{V4}{quaternary visual cortex}
\end{acronym}
\newpage
\input{erklaerung}


\end{document}
